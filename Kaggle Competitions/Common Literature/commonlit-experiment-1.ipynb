{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install \"/kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\"\n!pip install \"/kaggle/input/autocorrect/autocorrect-2.6.1.tar\"","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:42:29.322315Z","iopub.execute_input":"2023-10-03T04:42:29.322767Z","iopub.status.idle":"2023-10-03T04:43:38.841797Z","shell.execute_reply.started":"2023-10-03T04:42:29.322742Z","shell.execute_reply":"2023-10-03T04:43:38.840717Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Processing /kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\nInstalling collected packages: pyspellchecker\nSuccessfully installed pyspellchecker-0.7.2\nProcessing /kaggle/input/autocorrect/autocorrect-2.6.1.tar\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: autocorrect\n  Building wheel for autocorrect (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622363 sha256=ed4af99cfe413db17f43469239acbf159cc771b0a7968c5ff4faea24d40a3212\n  Stored in directory: /root/.cache/pip/wheels/db/69/42/0fb0421d2fe70d195a04665edc760cfe5fd341d7bb8d8e0aaa\nSuccessfully built autocorrect\nInstalling collected packages: autocorrect\nSuccessfully installed autocorrect-2.6.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport torch \nfrom torch import nn\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom torch.utils.data import Dataset\nfrom autocorrect import Speller","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:43:38.844270Z","iopub.execute_input":"2023-10-03T04:43:38.845237Z","iopub.status.idle":"2023-10-03T04:43:43.090038Z","shell.execute_reply.started":"2023-10-03T04:43:38.845204Z","shell.execute_reply":"2023-10-03T04:43:43.089083Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"summaries = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_train.csv')\nprompts = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/prompts_train.csv')\n\nsummaries.shape, prompts.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:43:43.091197Z","iopub.execute_input":"2023-10-03T04:43:43.091940Z","iopub.status.idle":"2023-10-03T04:43:43.213659Z","shell.execute_reply.started":"2023-10-03T04:43:43.091908Z","shell.execute_reply":"2023-10-03T04:43:43.212764Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"((7165, 5), (4, 4))"},"metadata":{}}]},{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"# parameters for training\n\nEPOCHS = 5\nmodel_name = '/kaggle/input/bert-base-uncased'\nnum_labels = 2\nlearning_rate = 0.001\nBATCH_SIZE = 20\nMAX_LENGTH = 512\n\n# If there are GPUs available, use the first one \ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:43:43.215062Z","iopub.execute_input":"2023-10-03T04:43:43.215607Z","iopub.status.idle":"2023-10-03T04:43:43.244043Z","shell.execute_reply.started":"2023-10-03T04:43:43.215567Z","shell.execute_reply":"2023-10-03T04:43:43.243171Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"STOP_WORDS = set(stopwords.words('english'))","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:43:43.247422Z","iopub.execute_input":"2023-10-03T04:43:43.247782Z","iopub.status.idle":"2023-10-03T04:43:43.262172Z","shell.execute_reply.started":"2023-10-03T04:43:43.247753Z","shell.execute_reply":"2023-10-03T04:43:43.261287Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"### Utilty Functions","metadata":{}},{"cell_type":"code","source":"def preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n    \n    return text\n\ndef is_valid_float(x):\n    return isinstance(x, float) and x == x  # This checks that x is not NaN since NaN != NaN in Python.","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:43:43.263710Z","iopub.execute_input":"2023-10-03T04:43:43.264475Z","iopub.status.idle":"2023-10-03T04:43:43.269673Z","shell.execute_reply.started":"2023-10-03T04:43:43.264445Z","shell.execute_reply":"2023-10-03T04:43:43.268844Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from spellchecker import SpellChecker\nspellchecker = SpellChecker()\n\ndef get_misspelled_count(text):\n    tokens = nltk.word_tokenize(text)\n    misspelled = [token for token in spellchecker.unknown(tokens) if token.isalpha()]\n    \n    return len(misspelled)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:43:43.270966Z","iopub.execute_input":"2023-10-03T04:43:43.271623Z","iopub.status.idle":"2023-10-03T04:43:43.343871Z","shell.execute_reply.started":"2023-10-03T04:43:43.271594Z","shell.execute_reply":"2023-10-03T04:43:43.342953Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def quotes_count(row):\n    summary = row['text']\n    text = row['prompt_text']\n    quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n    \n    if len(quotes_from_summary)>0:\n        return [quote in text for quote in quotes_from_summary].count(True)\n    else:\n        return 0","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:43:43.344986Z","iopub.execute_input":"2023-10-03T04:43:43.345295Z","iopub.status.idle":"2023-10-03T04:43:43.350566Z","shell.execute_reply.started":"2023-10-03T04:43:43.345268Z","shell.execute_reply":"2023-10-03T04:43:43.349696Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\n\ndef ngrams(token, n):\n    ngrams = zip(*[token[i:] for i in range(n)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\ndef ngrams_co_occurence(row, n):\n    original_tokens = row['prompt_tokens']\n    summary_tokens = row['summary_tokens']\n    \n    original_ngrams = set(ngrams(original_tokens, n))\n    summary_ngrams = set(ngrams(summary_tokens, n))\n    \n    common_ngrams = original_ngrams.intersection(summary_ngrams)\n    return len(common_ngrams)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:43:43.351837Z","iopub.execute_input":"2023-10-03T04:43:43.352867Z","iopub.status.idle":"2023-10-03T04:43:43.360749Z","shell.execute_reply.started":"2023-10-03T04:43:43.352837Z","shell.execute_reply":"2023-10-03T04:43:43.359852Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def check_is_stop_word(word):\n    return word in STOP_WORDS","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:43:43.361765Z","iopub.execute_input":"2023-10-03T04:43:43.362071Z","iopub.status.idle":"2023-10-03T04:43:43.373035Z","shell.execute_reply.started":"2023-10-03T04:43:43.362040Z","shell.execute_reply":"2023-10-03T04:43:43.372159Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def word_overlap_count(row):\n    \"\"\" intersection(prompt_text, text) \"\"\"    \n    prompt_words = row['prompt_tokens']\n    summary_words = row['summary_tokens']\n    if STOP_WORDS:\n        prompt_words = list(filter(check_is_stop_word, prompt_words))\n        summary_words = list(filter(check_is_stop_word, summary_words))\n    return len(set(prompt_words).intersection(set(summary_words)))","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:43:43.374503Z","iopub.execute_input":"2023-10-03T04:43:43.374848Z","iopub.status.idle":"2023-10-03T04:43:43.383079Z","shell.execute_reply.started":"2023-10-03T04:43:43.374815Z","shell.execute_reply":"2023-10-03T04:43:43.382111Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### For the train dataset","metadata":{}},{"cell_type":"code","source":"train = summaries.merge(prompts, on = 'prompt_id', how = 'inner')\ntrain.drop(['student_id', 'prompt_id'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:43:43.384295Z","iopub.execute_input":"2023-10-03T04:43:43.385293Z","iopub.status.idle":"2023-10-03T04:43:43.415252Z","shell.execute_reply.started":"2023-10-03T04:43:43.385264Z","shell.execute_reply":"2023-10-03T04:43:43.414323Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train['text_len'] = train['text'].apply(lambda x: len(x.split()))\ntrain['prompt_len'] = train['prompt_text'].apply(lambda x: len(x.split()))\ntrain['length_ratio'] = train['text_len'] / train['prompt_len']","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:43:43.416515Z","iopub.execute_input":"2023-10-03T04:43:43.417097Z","iopub.status.idle":"2023-10-03T04:43:43.689759Z","shell.execute_reply.started":"2023-10-03T04:43:43.417068Z","shell.execute_reply":"2023-10-03T04:43:43.688835Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train['text'] = train['text'].apply(lambda x: preprocess_text(x))\ntrain['prompt_question'] = train['prompt_question'].apply(lambda x: preprocess_text(x))\ntrain['prompt_text'] = train['prompt_text'].apply(lambda x: preprocess_text(x))\ntrain['prompt_title'] = train['prompt_title'].apply(lambda x: preprocess_text(x))\n\ntrain['misspelled'] = train['text'].apply(lambda x: get_misspelled_count(x))","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:43:43.694857Z","iopub.execute_input":"2023-10-03T04:43:43.695118Z","iopub.status.idle":"2023-10-03T04:43:48.572249Z","shell.execute_reply.started":"2023-10-03T04:43:43.695096Z","shell.execute_reply":"2023-10-03T04:43:48.571327Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train['prompt_tokens'] = train['prompt_text'].apply(lambda x: word_tokenize(x))\ntrain['summary_tokens'] = train['text'].apply(lambda x: word_tokenize(x))\n\ntrain['word_overlap_count'] = train.apply(word_overlap_count, axis = 1)\ntrain['bigram_overlap_count'] = train.apply(ngrams_co_occurence, args=(2,), axis = 1)\ntrain['trigram_overlap_count'] = train.apply(ngrams_co_occurence, args=(3,), axis = 1)\n\ntrain['bigram_overlap_ratio'] = train['bigram_overlap_count'] / (train['text_len'] - 1)\ntrain['trigram_overlap_ratio'] = train['trigram_overlap_count'] / (train['text_len'] - 2)\n\ntrain.drop(['prompt_tokens', 'summary_tokens'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:43:48.573385Z","iopub.execute_input":"2023-10-03T04:43:48.573978Z","iopub.status.idle":"2023-10-03T04:44:10.909791Z","shell.execute_reply.started":"2023-10-03T04:43:48.573947Z","shell.execute_reply":"2023-10-03T04:44:10.908843Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### For the test dataset","metadata":{}},{"cell_type":"code","source":"test_summaries = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_test.csv')\ntest_prompt = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/prompts_test.csv')\n\nstudent_ids = test_summaries['student_id'].values.tolist()","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:44:10.911389Z","iopub.execute_input":"2023-10-03T04:44:10.911711Z","iopub.status.idle":"2023-10-03T04:44:10.926965Z","shell.execute_reply.started":"2023-10-03T04:44:10.911681Z","shell.execute_reply":"2023-10-03T04:44:10.926193Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"test = test_summaries.merge(test_prompt, on = 'prompt_id', how = 'inner')\ntest.drop(['student_id', 'prompt_id'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:44:10.928063Z","iopub.execute_input":"2023-10-03T04:44:10.928622Z","iopub.status.idle":"2023-10-03T04:44:10.936837Z","shell.execute_reply.started":"2023-10-03T04:44:10.928580Z","shell.execute_reply":"2023-10-03T04:44:10.935751Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"test['text_len'] = test['text'].apply(lambda x: len(x.split()))\ntest['prompt_len'] = test['prompt_text'].apply(lambda x: len(x.split()))\ntest['length_ratio'] = test['text_len'] / train['prompt_len']","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:44:10.939734Z","iopub.execute_input":"2023-10-03T04:44:10.940088Z","iopub.status.idle":"2023-10-03T04:44:10.952475Z","shell.execute_reply.started":"2023-10-03T04:44:10.940059Z","shell.execute_reply":"2023-10-03T04:44:10.951565Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"test['text'] = test['text'].apply(lambda x: preprocess_text(x))\ntest['prompt_question'] = test['prompt_question'].apply(lambda x: preprocess_text(x))\ntest['prompt_text'] = test['prompt_text'].apply(lambda x: preprocess_text(x))\ntest['prompt_title'] = test['prompt_title'].apply(lambda x: preprocess_text(x))\n\ntest['misspelled'] = test['text'].apply(lambda x: get_misspelled_count(x))","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:44:10.953835Z","iopub.execute_input":"2023-10-03T04:44:10.954241Z","iopub.status.idle":"2023-10-03T04:44:10.965654Z","shell.execute_reply.started":"2023-10-03T04:44:10.954209Z","shell.execute_reply":"2023-10-03T04:44:10.964553Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"test['prompt_tokens'] = test['prompt_text'].apply(lambda x: word_tokenize(x))\ntest['summary_tokens'] = test['text'].apply(lambda x: word_tokenize(x))\n\ntest['word_overlap_count'] = test.apply(word_overlap_count, axis = 1)\ntest['bigram_overlap_count'] = test.apply(ngrams_co_occurence, args=(2,), axis = 1)\ntest['trigram_overlap_count'] = test.apply(ngrams_co_occurence, args=(3,), axis = 1)\n\ntest['bigram_overlap_ratio'] = test['bigram_overlap_count'] / (test['text_len'] - 1)\ntest['trigram_overlap_ratio'] = test['trigram_overlap_count'] / (test['text_len'] - 2)\n\ntest.drop(['prompt_tokens', 'summary_tokens'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:44:10.966767Z","iopub.execute_input":"2023-10-03T04:44:10.967548Z","iopub.status.idle":"2023-10-03T04:44:10.982887Z","shell.execute_reply.started":"2023-10-03T04:44:10.967474Z","shell.execute_reply":"2023-10-03T04:44:10.981817Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### Tokenization","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:44:10.984213Z","iopub.execute_input":"2023-10-03T04:44:10.985304Z","iopub.status.idle":"2023-10-03T04:44:12.711537Z","shell.execute_reply.started":"2023-10-03T04:44:10.985273Z","shell.execute_reply":"2023-10-03T04:44:12.710544Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"sep = tokenizer.sep_token","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:44:12.712971Z","iopub.execute_input":"2023-10-03T04:44:12.713776Z","iopub.status.idle":"2023-10-03T04:44:12.719869Z","shell.execute_reply.started":"2023-10-03T04:44:12.713737Z","shell.execute_reply":"2023-10-03T04:44:12.717935Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"train['inputs'] = train['prompt_question'] + sep + train['text']\ntest['inputs'] = test['prompt_question'] + sep + test['text']","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:44:12.721160Z","iopub.execute_input":"2023-10-03T04:44:12.722192Z","iopub.status.idle":"2023-10-03T04:44:12.737010Z","shell.execute_reply.started":"2023-10-03T04:44:12.722157Z","shell.execute_reply":"2023-10-03T04:44:12.735825Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# sanity check \n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:44:12.738512Z","iopub.execute_input":"2023-10-03T04:44:12.739680Z","iopub.status.idle":"2023-10-03T04:44:12.764640Z","shell.execute_reply.started":"2023-10-03T04:44:12.739644Z","shell.execute_reply":"2023-10-03T04:44:12.763526Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"                                                text   content   wording  \\\n0  the third wave was an experimentto see how peo...  0.205683  0.380538   \n1  the third wave developed  rapidly because the ...  3.272894  3.219757   \n2  the third wave only started as an experiment w...  0.205683  0.380538   \n3  the experimen was orginally about how even whe...  0.567975  0.969062   \n4  the third wave developed so quickly due to the... -0.910596 -0.081769   \n\n                                     prompt_question    prompt_title  \\\n0  summarize how the third wave developed over su...  the third wave   \n1  summarize how the third wave developed over su...  the third wave   \n2  summarize how the third wave developed over su...  the third wave   \n3  summarize how the third wave developed over su...  the third wave   \n4  summarize how the third wave developed over su...  the third wave   \n\n                                         prompt_text  text_len  prompt_len  \\\n0  background   the third wave experiment took pl...        61         596   \n1  background   the third wave experiment took pl...       203         596   \n2  background   the third wave experiment took pl...        60         596   \n3  background   the third wave experiment took pl...        76         596   \n4  background   the third wave experiment took pl...        27         596   \n\n   length_ratio  misspelled  word_overlap_count  bigram_overlap_count  \\\n0      0.102349           2                  15                     7   \n1      0.340604          13                  27                    27   \n2      0.100671           3                  15                    16   \n3      0.127517           4                  18                    21   \n4      0.045302           2                   9                     6   \n\n   trigram_overlap_count  bigram_overlap_ratio  trigram_overlap_ratio  \\\n0                      2              0.116667               0.033898   \n1                      8              0.133663               0.039801   \n2                      7              0.271186               0.120690   \n3                      9              0.280000               0.121622   \n4                      1              0.230769               0.040000   \n\n                                              inputs  \n0  summarize how the third wave developed over su...  \n1  summarize how the third wave developed over su...  \n2  summarize how the third wave developed over su...  \n3  summarize how the third wave developed over su...  \n4  summarize how the third wave developed over su...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>content</th>\n      <th>wording</th>\n      <th>prompt_question</th>\n      <th>prompt_title</th>\n      <th>prompt_text</th>\n      <th>text_len</th>\n      <th>prompt_len</th>\n      <th>length_ratio</th>\n      <th>misspelled</th>\n      <th>word_overlap_count</th>\n      <th>bigram_overlap_count</th>\n      <th>trigram_overlap_count</th>\n      <th>bigram_overlap_ratio</th>\n      <th>trigram_overlap_ratio</th>\n      <th>inputs</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>the third wave was an experimentto see how peo...</td>\n      <td>0.205683</td>\n      <td>0.380538</td>\n      <td>summarize how the third wave developed over su...</td>\n      <td>the third wave</td>\n      <td>background   the third wave experiment took pl...</td>\n      <td>61</td>\n      <td>596</td>\n      <td>0.102349</td>\n      <td>2</td>\n      <td>15</td>\n      <td>7</td>\n      <td>2</td>\n      <td>0.116667</td>\n      <td>0.033898</td>\n      <td>summarize how the third wave developed over su...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>the third wave developed  rapidly because the ...</td>\n      <td>3.272894</td>\n      <td>3.219757</td>\n      <td>summarize how the third wave developed over su...</td>\n      <td>the third wave</td>\n      <td>background   the third wave experiment took pl...</td>\n      <td>203</td>\n      <td>596</td>\n      <td>0.340604</td>\n      <td>13</td>\n      <td>27</td>\n      <td>27</td>\n      <td>8</td>\n      <td>0.133663</td>\n      <td>0.039801</td>\n      <td>summarize how the third wave developed over su...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>the third wave only started as an experiment w...</td>\n      <td>0.205683</td>\n      <td>0.380538</td>\n      <td>summarize how the third wave developed over su...</td>\n      <td>the third wave</td>\n      <td>background   the third wave experiment took pl...</td>\n      <td>60</td>\n      <td>596</td>\n      <td>0.100671</td>\n      <td>3</td>\n      <td>15</td>\n      <td>16</td>\n      <td>7</td>\n      <td>0.271186</td>\n      <td>0.120690</td>\n      <td>summarize how the third wave developed over su...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>the experimen was orginally about how even whe...</td>\n      <td>0.567975</td>\n      <td>0.969062</td>\n      <td>summarize how the third wave developed over su...</td>\n      <td>the third wave</td>\n      <td>background   the third wave experiment took pl...</td>\n      <td>76</td>\n      <td>596</td>\n      <td>0.127517</td>\n      <td>4</td>\n      <td>18</td>\n      <td>21</td>\n      <td>9</td>\n      <td>0.280000</td>\n      <td>0.121622</td>\n      <td>summarize how the third wave developed over su...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>the third wave developed so quickly due to the...</td>\n      <td>-0.910596</td>\n      <td>-0.081769</td>\n      <td>summarize how the third wave developed over su...</td>\n      <td>the third wave</td>\n      <td>background   the third wave experiment took pl...</td>\n      <td>27</td>\n      <td>596</td>\n      <td>0.045302</td>\n      <td>2</td>\n      <td>9</td>\n      <td>6</td>\n      <td>1</td>\n      <td>0.230769</td>\n      <td>0.040000</td>\n      <td>summarize how the third wave developed over su...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"class TextDataset(Dataset):\n    def __init__(self, texts, feature_cols, tokenizer, max_length, targets = None, is_train = False):\n        self.texts = texts\n        self.feature_cols = feature_cols\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.is_train = is_train\n        \n        if self.is_train:\n            self.targets = targets\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        \n        encoding = self.tokenizer.encode_plus(text,\n                                             add_special_tokens = True,\n                                             max_length = self.max_length,\n                                             return_token_type_ids = False,\n                                             padding = 'max_length',\n                                             truncation = True,\n                                             return_attention_mask = True,\n                                             return_tensors = 'pt')\n        if self.is_train:\n            item = {'input_ids': encoding['input_ids'].flatten(),\n                    'attention_mask': encoding['attention_mask'].flatten(),\n                   'feature_cols': torch.tensor(self.feature_cols[idx], dtype=torch.float), \n                   'target': torch.tensor(self.targets[idx], dtype=torch.float)}\n        else:\n            item = {'input_ids': encoding['input_ids'].flatten(),\n                   'attention_mask': encoding['attention_mask'].flatten(),\n                   'feature_cols': torch.tensor(self.feature_cols[idx], dtype=torch.float)}\n        \n        return item","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:44:12.765993Z","iopub.execute_input":"2023-10-03T04:44:12.766645Z","iopub.status.idle":"2023-10-03T04:44:12.777563Z","shell.execute_reply.started":"2023-10-03T04:44:12.766609Z","shell.execute_reply":"2023-10-03T04:44:12.776348Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"feature_cols = ['text_len', 'prompt_len', 'length_ratio', 'misspelled',\n                'word_overlap_count', 'bigram_overlap_count', 'trigram_overlap_count',\n               'bigram_overlap_ratio', 'trigram_overlap_ratio']\ntargets = ['content', 'wording']\nfeature_col_size = len(feature_cols)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:44:12.779338Z","iopub.execute_input":"2023-10-03T04:44:12.780418Z","iopub.status.idle":"2023-10-03T04:44:12.791123Z","shell.execute_reply.started":"2023-10-03T04:44:12.780380Z","shell.execute_reply":"2023-10-03T04:44:12.789959Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"The `MAX_LENGTH` feature is guided by my work in the [EDA notebook](https://github.com/Akorex/Natural-Language-Processing/blob/main/Kaggle%20Competitions/commonlit-comp-starter.ipynb).","metadata":{}},{"cell_type":"code","source":"train_dataset = TextDataset(texts = train['inputs'], \n                            feature_cols = train[feature_cols].values, \n                            tokenizer = tokenizer, \n                            max_length = MAX_LENGTH,\n                            targets = train[targets].values,\n                           is_train = True)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:44:12.792811Z","iopub.execute_input":"2023-10-03T04:44:12.793818Z","iopub.status.idle":"2023-10-03T04:44:12.805224Z","shell.execute_reply.started":"2023-10-03T04:44:12.793781Z","shell.execute_reply":"2023-10-03T04:44:12.804103Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"test_dataset = TextDataset(texts = test['inputs'],\n                          feature_cols = test[feature_cols].values,\n                          tokenizer = tokenizer,\n                          max_length = MAX_LENGTH,\n                          is_train = False)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:44:12.807304Z","iopub.execute_input":"2023-10-03T04:44:12.808317Z","iopub.status.idle":"2023-10-03T04:44:12.819787Z","shell.execute_reply.started":"2023-10-03T04:44:12.808278Z","shell.execute_reply":"2023-10-03T04:44:12.818757Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# split the dataset to train and validation dataset\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_dataset, val_dataset = train_test_split(train_dataset, test_size = 0.1, shuffle = True, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:44:12.821504Z","iopub.execute_input":"2023-10-03T04:44:12.822205Z","iopub.status.idle":"2023-10-03T04:44:18.739428Z","shell.execute_reply.started":"2023-10-03T04:44:12.822173Z","shell.execute_reply":"2023-10-03T04:44:18.738472Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# load the dataset using pytorch's dataloader tool\n\nfrom torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE)\nval_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE)\ntest_loader = DataLoader(test_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:44:18.740630Z","iopub.execute_input":"2023-10-03T04:44:18.740929Z","iopub.status.idle":"2023-10-03T04:44:18.746153Z","shell.execute_reply.started":"2023-10-03T04:44:18.740902Z","shell.execute_reply":"2023-10-03T04:44:18.745196Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"## Modelling","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:44:18.747522Z","iopub.execute_input":"2023-10-03T04:44:18.748168Z","iopub.status.idle":"2023-10-03T04:44:18.775712Z","shell.execute_reply.started":"2023-10-03T04:44:18.748088Z","shell.execute_reply":"2023-10-03T04:44:18.774964Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"class CustomModel(nn.Module):\n    def __init__(self, model_name, num_labels, feature_col_size):\n        \"\"\"Instantiate a model that can fit on the dataset\"\"\"\n        \n        super().__init__()\n        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = num_labels)\n        self.numerics = nn.Linear(feature_col_size, 16)\n        self.final_layer = nn.Linear(16 + num_labels, num_labels)\n        \n    def forward(self, input_ids, attention_mask, feature_cols):\n        text_output = self.model(input_ids = input_ids, attention_mask = attention_mask)\n        numerics = self.numerics(feature_cols)\n        concat_features = torch.cat([text_output.logits, numerics], dim = 1)\n        \n        final_output = self.final_layer(concat_features)\n        \n        return final_output","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:44:18.776916Z","iopub.execute_input":"2023-10-03T04:44:18.777218Z","iopub.status.idle":"2023-10-03T04:44:18.783129Z","shell.execute_reply.started":"2023-10-03T04:44:18.777191Z","shell.execute_reply":"2023-10-03T04:44:18.782310Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"model = CustomModel(model_name, num_labels, feature_col_size)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:44:18.784193Z","iopub.execute_input":"2023-10-03T04:44:18.785000Z","iopub.status.idle":"2023-10-03T04:44:34.842811Z","shell.execute_reply.started":"2023-10-03T04:44:18.784964Z","shell.execute_reply":"2023-10-03T04:44:34.841881Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"CustomModel(\n  (model): BertForSequenceClassification(\n    (bert): BertModel(\n      (embeddings): BertEmbeddings(\n        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n        (position_embeddings): Embedding(512, 768)\n        (token_type_embeddings): Embedding(2, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): BertEncoder(\n        (layer): ModuleList(\n          (0-11): 12 x BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (pooler): BertPooler(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (activation): Tanh()\n      )\n    )\n    (dropout): Dropout(p=0.1, inplace=False)\n    (classifier): Linear(in_features=768, out_features=2, bias=True)\n  )\n  (numerics): Linear(in_features=9, out_features=16, bias=True)\n  (final_layer): Linear(in_features=18, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# loss and optimizer\n\nfrom torch.optim import Adam\nfrom torch.nn import MSELoss\n\n\noptimizer = Adam(model.parameters(), lr = learning_rate)\nloss_function = MSELoss()","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:44:34.844023Z","iopub.execute_input":"2023-10-03T04:44:34.844355Z","iopub.status.idle":"2023-10-03T04:44:34.850653Z","shell.execute_reply.started":"2023-10-03T04:44:34.844324Z","shell.execute_reply":"2023-10-03T04:44:34.849662Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"def train_step(train_loader):\n    \"\"\"The training loop for the dataset\"\"\"\n    \n    for step, batch in enumerate(train_loader):\n        epochal_loss = 0\n        steps = 0\n        \n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        feature_cols = batch['feature_cols'].to(device)\n        targets = batch['target'].to(device)\n        \n        optimizer.zero_grad()\n        \n        outputs = model(input_ids, attention_mask, feature_cols)\n        loss = loss_function(outputs, targets)\n        epochal_loss += loss\n        steps += 1\n        loss.backward()\n        \n        optimizer.step()\n        \n        if step % 50 == 0:\n            print(f\"Epoch {epoch + 1} Step {step} Loss {loss.item()}\")\n            \n    print(f\"Epoch {epoch + 1} Train Loss: {epochal_loss/steps}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:44:34.851953Z","iopub.execute_input":"2023-10-03T04:44:34.852729Z","iopub.status.idle":"2023-10-03T04:44:34.863023Z","shell.execute_reply.started":"2023-10-03T04:44:34.852700Z","shell.execute_reply":"2023-10-03T04:44:34.862132Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"def val_step(val_loader):\n    \"\"\"The validation loop\"\"\"\n    \n    with torch.no_grad():\n        for step, batch in enumerate(val_loader):\n            epochal_loss = 0\n            steps = 0\n            \n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            feature_cols = batch['feature_cols'].to(device)\n            targets = batch['target'].to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask, feature_cols)\n            loss = loss_function(outputs, targets)\n            epochal_loss += loss\n            steps += 1\n        \n        print(f\"Epoch {epoch + 1} Validation Loss: {epochal_loss/steps}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:44:34.864611Z","iopub.execute_input":"2023-10-03T04:44:34.864941Z","iopub.status.idle":"2023-10-03T04:44:34.874935Z","shell.execute_reply.started":"2023-10-03T04:44:34.864913Z","shell.execute_reply":"2023-10-03T04:44:34.873885Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"import time\n\nfor epoch in range(EPOCHS):\n    start = time.time()\n    \n    train_step(train_loader)\n    val_step(val_loader)\n    print(f\"Total time for training epoch {epoch + 1}: {time.time() - start}s\")\n    print('\\n')","metadata":{"execution":{"iopub.status.busy":"2023-10-03T04:44:34.876123Z","iopub.execute_input":"2023-10-03T04:44:34.877064Z","iopub.status.idle":"2023-10-03T05:12:07.060594Z","shell.execute_reply.started":"2023-10-03T04:44:34.877035Z","shell.execute_reply":"2023-10-03T05:12:07.059558Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Epoch 1 Step 0 Loss 1965.4530029296875\nEpoch 1 Step 50 Loss 131.23826599121094\nEpoch 1 Step 100 Loss 29.7383975982666\nEpoch 1 Step 150 Loss 25.536453247070312\nEpoch 1 Step 200 Loss 12.18255615234375\nEpoch 1 Step 250 Loss 4.197801113128662\nEpoch 1 Step 300 Loss 1.2913610935211182\nEpoch 1 Train Loss: 1.9974167346954346\nEpoch 1 Validation Loss: 2.6117513179779053\nTotal time for training epoch 1: 331.2814335823059s\n\n\nEpoch 2 Step 0 Loss 1.9019527435302734\nEpoch 2 Step 50 Loss 2.2115108966827393\nEpoch 2 Step 100 Loss 3.3882617950439453\nEpoch 2 Step 150 Loss 1.191575050354004\nEpoch 2 Step 200 Loss 1.1557422876358032\nEpoch 2 Step 250 Loss 0.6739975810050964\nEpoch 2 Step 300 Loss 0.5298209190368652\nEpoch 2 Train Loss: 0.5751464366912842\nEpoch 2 Validation Loss: 0.6558099389076233\nTotal time for training epoch 2: 329.3616201877594s\n\n\nEpoch 3 Step 0 Loss 0.7243967056274414\nEpoch 3 Step 50 Loss 0.9273635745048523\nEpoch 3 Step 100 Loss 0.9164897203445435\nEpoch 3 Step 150 Loss 0.6905603408813477\nEpoch 3 Step 200 Loss 0.6173905730247498\nEpoch 3 Step 250 Loss 0.4190193712711334\nEpoch 3 Step 300 Loss 0.4236547648906708\nEpoch 3 Train Loss: 0.5251274108886719\nEpoch 3 Validation Loss: 0.4681771695613861\nTotal time for training epoch 3: 329.5579650402069s\n\n\nEpoch 4 Step 0 Loss 0.5993326902389526\nEpoch 4 Step 50 Loss 0.6646066308021545\nEpoch 4 Step 100 Loss 0.5612307786941528\nEpoch 4 Step 150 Loss 0.5090894103050232\nEpoch 4 Step 200 Loss 0.4816500246524811\nEpoch 4 Step 250 Loss 0.28672000765800476\nEpoch 4 Step 300 Loss 0.2989138066768646\nEpoch 4 Train Loss: 0.547524094581604\nEpoch 4 Validation Loss: 0.44134634733200073\nTotal time for training epoch 4: 330.8124623298645s\n\n\nEpoch 5 Step 0 Loss 0.5504519939422607\nEpoch 5 Step 50 Loss 0.4961129128932953\nEpoch 5 Step 100 Loss 0.3862178921699524\nEpoch 5 Step 150 Loss 0.4772689938545227\nEpoch 5 Step 200 Loss 0.4159155786037445\nEpoch 5 Step 250 Loss 0.2873380780220032\nEpoch 5 Step 300 Loss 0.26089581847190857\nEpoch 5 Train Loss: 0.5635006427764893\nEpoch 5 Validation Loss: 0.4998725354671478\nTotal time for training epoch 5: 331.16181683540344s\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"preds = []\n\nfor batch in test_loader:\n    with torch.no_grad():\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        feature_cols = batch['feature_cols'].to(device)\n        \n        outputs = model(input_ids, attention_mask, feature_cols)\n        preds.extend(outputs.cpu().numpy())","metadata":{"execution":{"iopub.status.busy":"2023-10-03T05:12:07.062084Z","iopub.execute_input":"2023-10-03T05:12:07.062749Z","iopub.status.idle":"2023-10-03T05:12:07.153212Z","shell.execute_reply.started":"2023-10-03T05:12:07.062714Z","shell.execute_reply":"2023-10-03T05:12:07.152365Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame({\n    'student_id': student_ids,\n    'content': [pred[0] for pred in preds],\n    'wording': [pred[1] for pred in preds]\n})\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T05:12:07.154433Z","iopub.execute_input":"2023-10-03T05:12:07.154747Z","iopub.status.idle":"2023-10-03T05:12:07.159898Z","shell.execute_reply.started":"2023-10-03T05:12:07.154719Z","shell.execute_reply":"2023-10-03T05:12:07.159005Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"cols_to_check = ['wording', 'content']\nsubmission[cols_to_check] = submission[cols_to_check].applymap(lambda x: x if is_valid_float(x) else 0.0)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T05:12:07.161119Z","iopub.execute_input":"2023-10-03T05:12:07.162217Z","iopub.status.idle":"2023-10-03T05:12:07.172238Z","shell.execute_reply.started":"2023-10-03T05:12:07.162188Z","shell.execute_reply":"2023-10-03T05:12:07.171152Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2023-10-03T05:12:07.178503Z","iopub.execute_input":"2023-10-03T05:12:07.179364Z","iopub.status.idle":"2023-10-03T05:12:07.189252Z","shell.execute_reply.started":"2023-10-03T05:12:07.179333Z","shell.execute_reply":"2023-10-03T05:12:07.188182Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"     student_id   content   wording\n0  000000ffffff -1.366251 -1.537591\n1  111111eeeeee -1.366251 -1.537591\n2  222222cccccc -1.366251 -1.537591\n3  333333dddddd -1.366251 -1.537591","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>student_id</th>\n      <th>content</th>\n      <th>wording</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000000ffffff</td>\n      <td>-1.366251</td>\n      <td>-1.537591</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>111111eeeeee</td>\n      <td>-1.366251</td>\n      <td>-1.537591</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>222222cccccc</td>\n      <td>-1.366251</td>\n      <td>-1.537591</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>333333dddddd</td>\n      <td>-1.366251</td>\n      <td>-1.537591</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T05:12:07.190421Z","iopub.execute_input":"2023-10-03T05:12:07.191248Z","iopub.status.idle":"2023-10-03T05:12:07.200079Z","shell.execute_reply.started":"2023-10-03T05:12:07.191216Z","shell.execute_reply":"2023-10-03T05:12:07.198972Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
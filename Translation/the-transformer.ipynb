{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"The aim of this notebook is to serve as a repository of some kind of any and important information I can find on the concept of Transformers in the fild of ML/AI.","metadata":{"id":"RyFvwJe0jaY7"}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport logging\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow_datasets as tfds\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"id":"aDwccFvLjaY-","execution":{"iopub.status.busy":"2022-07-05T07:39:45.900112Z","iopub.execute_input":"2022-07-05T07:39:45.901304Z","iopub.status.idle":"2022-07-05T07:39:53.206316Z","shell.execute_reply.started":"2022-07-05T07:39:45.901160Z","shell.execute_reply":"2022-07-05T07:39:53.205004Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# check if GPU is used\ndevice_name = tf.test.gpu_device_name()\nif device_name != '/device:GPU:0':\n    raise SystemError('GPU device not found')\nprint(f\"Found GPU device at {device_name}\")","metadata":{"id":"_lIdhL5JjaZA","outputId":"1d0b28a0-dadf-4e49-f649-6cc00ec374de","execution":{"iopub.status.busy":"2022-07-05T07:39:53.212854Z","iopub.execute_input":"2022-07-05T07:39:53.216930Z","iopub.status.idle":"2022-07-05T07:39:56.057171Z","shell.execute_reply.started":"2022-07-05T07:39:53.216885Z","shell.execute_reply":"2022-07-05T07:39:56.053673Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Found GPU device at /device:GPU:0\n","output_type":"stream"},{"name":"stderr","text":"2022-07-05 07:39:53.223752: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-07-05 07:39:53.305724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-05 07:39:53.431730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-05 07:39:53.433210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-05 07:39:56.037926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-05 07:39:56.038924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-05 07:39:56.039835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-05 07:39:56.040839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## What is the Transformer Architecture\n\n\n![The Transformer](https://miro.medium.com/max/700/1*BHzGVskWGS_3jEcYYi6miQ.png)\n\n\nAfter the revolution of the research paper from scientists at Google, [Attention is all you need](https://arxiv.org/abs/1706.03762), the field of Natural Language Processing exploded with the new architecture proposed, The Transformer. The Transformer is special because it dealt away with recurrence in its architecture and focused solely on attention mechanisms. \n\nRNNs, recurrent neural networks, were the start of the art ways of creating ML models in the field of Natural Language Processing. RNNs work well with tasks like sentiment analysis or topic modelling. In order to use RNNs for sequence-sequence tasks like translation, summarization or text generation, the model had to be designed in such a way that one section would be for the encoder (the goal of which is to create a rich representation of the text input) and the other section would be for the decoder (which would receive outputs from the encoder and use it to output tokens for the task at hand). The encoders and decoders are usually similarly designed with the decoder usually having some sort of mask so that future tokens are not seen (otherwise its predictions would be trivial). \n\nThe problem with RNNs is that they're quite slow and can't really handle long sequences. Improvements to the RNNs is the use of attention mechanisms. With the paper from the Google scientists, the Transformer dealt away with recurrence altogether and used only attention mechanisms for its architecture. It uses self-attention to detect subtle ways even distant data elememts in a sequence influence and depend on each other. \n\nIn other to understand how the transformer works, we need to carefully explain its different layers with its basic implentation as is done below.","metadata":{}},{"cell_type":"markdown","source":"Heavy inspiration - \n\n* https://www.tensorflow.org/text/tutorials/transformer\n\n* https://colab.research.google.com/github/dlmacedo/starter-academic/blob/master/content/courses/deeplearning/notebooks/tensorflow/transformer.ipynb#scrollTo=s_qNSzzyaCbD\n\n* https://colab.research.google.com/drive/1CBe2VlogbyXzmIyRQGH5xzuvLwGrvjcf?usp=sharing#scrollTo=y7TFAAnXklib\n\n* https://sungwookyoo.github.io/study/Transformer/\n","metadata":{}},{"cell_type":"markdown","source":"## Data Input","metadata":{"id":"ibyb1NdSjaZB"}},{"cell_type":"markdown","source":"We use the steps in the tutorial from the [official TensorFlow tutorial](https://www.tensorflow.org/text/tutorials/transformer) load the Portuguese-English translation dataset, tokenize and make it in batches","metadata":{"id":"7moC-r_KjaZB"}},{"cell_type":"code","source":"examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n                               as_supervised=True)\n\ntrain_examples, valid_examples = examples['train'], examples['validation']","metadata":{"id":"uGPPle5XjaZC","outputId":"3861624b-cec6-447a-a099-dbfd3a4ccc4d","execution":{"iopub.status.busy":"2022-07-05T07:39:56.058685Z","iopub.execute_input":"2022-07-05T07:39:56.060059Z","iopub.status.idle":"2022-07-05T07:40:22.482625Z","shell.execute_reply.started":"2022-07-05T07:39:56.060029Z","shell.execute_reply":"2022-07-05T07:40:22.481291Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2022-07-05 07:39:56.464492: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Failed precondition: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1mDownloading and preparing dataset 124.94 MiB (download: 124.94 MiB, generated: Unknown size, total: 124.94 MiB) to /root/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0...\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Dl Completed...: 0 url [00:00, ? url/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"337188e6a33b479f9f09995d08affd99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dl Size...: 0 MiB [00:00, ? MiB/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c619242bfe8d4cdba92cae42a3a526e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extraction completed...: 0 file [00:00, ? file/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5573df17d1c04c67bb169f8addb32af1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train examples...:   0%|          | 0/51785 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Shuffling ted_hrlr_translate-train.tfrecord...:   0%|          | 0/51785 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation examples...:   0%|          | 0/1193 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Shuffling ted_hrlr_translate-validation.tfrecord...:   0%|          | 0/1193 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test examples...:   0%|          | 0/1803 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Shuffling ted_hrlr_translate-test.tfrecord...:   0%|          | 0/1803 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\u001b[1mDataset ted_hrlr_translate downloaded and prepared to /root/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"2022-07-05 07:40:21.990825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-05 07:40:21.992210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-05 07:40:21.993637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-05 07:40:21.995216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-05 07:40:21.996478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-05 07:40:21.997757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-05 07:40:21.999099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-05 07:40:22.000367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-07-05 07:40:22.001463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# let's see some of the entries in the dataset\nfor pt, en in train_examples.take(5):\n    print(\"Portuguese: \", pt.numpy().decode('utf-8'))\n    print(\"English: \", en.numpy().decode('utf-8'))\n    print('\\n')","metadata":{"id":"igltnpXOjaZD","outputId":"c0fe8365-ce95-4291-e5ae-770e8bf18162","execution":{"iopub.status.busy":"2022-07-05T07:40:22.486008Z","iopub.execute_input":"2022-07-05T07:40:22.487037Z","iopub.status.idle":"2022-07-05T07:40:22.598622Z","shell.execute_reply.started":"2022-07-05T07:40:22.486995Z","shell.execute_reply":"2022-07-05T07:40:22.597441Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2022-07-05 07:40:22.529888: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Portuguese:  e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\nEnglish:  and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n\n\nPortuguese:  mas e se estes fatores fossem ativos ?\nEnglish:  but what if it were active ?\n\n\nPortuguese:  mas eles não tinham a curiosidade de me testar .\nEnglish:  but they did n't test for curiosity .\n\n\nPortuguese:  e esta rebeldia consciente é a razão pela qual eu , como agnóstica , posso ainda ter fé .\nEnglish:  and this conscious defiance is why i , as an agnostic , can still have faith .\n\n\nPortuguese:  `` `` '' podem usar tudo sobre a mesa no meu corpo . ''\nEnglish:  you can use everything on the table on me .\n\n\n","output_type":"stream"},{"name":"stderr","text":"2022-07-05 07:40:22.586799: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Tokenization and detokenization","metadata":{"id":"gI8n7rr-jaZE"}},{"cell_type":"code","source":"tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus((en.numpy() for pt, en in train_examples),\n                                                                         target_vocab_size = 2**13)\ntokenizer_pt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus((pt.numpy() for pt, en in train_examples),\n                                                                        target_vocab_size = 2**13)","metadata":{"id":"0fgHtXkojaZF","execution":{"iopub.status.busy":"2022-07-05T07:40:22.602185Z","iopub.execute_input":"2022-07-05T07:40:22.602508Z","iopub.status.idle":"2022-07-05T07:44:12.904744Z","shell.execute_reply.started":"2022-07-05T07:40:22.602480Z","shell.execute_reply":"2022-07-05T07:44:12.903377Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"sample_text = 'Transformer is awesome'\ntokenized_text = tokenizer_en.encode(sample_text)\ntokenized_text","metadata":{"id":"e5Lp3bNWjaZG","outputId":"86606689-d5e9-4131-d014-e1678bdcf526","execution":{"iopub.status.busy":"2022-07-05T07:44:12.906564Z","iopub.execute_input":"2022-07-05T07:44:12.907377Z","iopub.status.idle":"2022-07-05T07:44:12.918769Z","shell.execute_reply.started":"2022-07-05T07:44:12.907333Z","shell.execute_reply":"2022-07-05T07:44:12.917410Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"[7915, 1248, 7946, 7194, 13, 2799]"},"metadata":{}}]},{"cell_type":"code","source":"original_text = tokenizer_en.decode(tokenized_text)\noriginal_text","metadata":{"id":"Tdnk4G8BjaZH","outputId":"19fb681a-3542-467a-ecfb-3d3f207cd0a0","execution":{"iopub.status.busy":"2022-07-05T07:44:12.920786Z","iopub.execute_input":"2022-07-05T07:44:12.921423Z","iopub.status.idle":"2022-07-05T07:44:12.937064Z","shell.execute_reply.started":"2022-07-05T07:44:12.921366Z","shell.execute_reply":"2022-07-05T07:44:12.930969Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'Transformer is awesome'"},"metadata":{}}]},{"cell_type":"code","source":"for ch in tokenized_text:\n    print(f\"{ch} --> {tokenizer_en.decode([ch])}\")","metadata":{"id":"B9iEX7dZjaZH","outputId":"dc76aabe-4e28-4665-98b0-2e916464265c","execution":{"iopub.status.busy":"2022-07-05T07:44:12.938874Z","iopub.execute_input":"2022-07-05T07:44:12.940235Z","iopub.status.idle":"2022-07-05T07:44:12.947865Z","shell.execute_reply.started":"2022-07-05T07:44:12.940194Z","shell.execute_reply":"2022-07-05T07:44:12.945984Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"7915 --> T\n1248 --> ran\n7946 --> s\n7194 --> former \n13 --> is \n2799 --> awesome\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The transformer needs special tokens to know the start and the end of each text. So here's a function to add it","metadata":{"id":"yJbk1KFmjaZI"}},{"cell_type":"code","source":"def encode(lang1, lang2):\n    lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(lang1.numpy()) + [tokenizer_pt.vocab_size + 1]\n    lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(lang2.numpy()) + [tokenizer_en.vocab_size + 1]\n\n    return lang1, lang2","metadata":{"id":"llPWcVbejaZI","execution":{"iopub.status.busy":"2022-07-05T07:44:12.950123Z","iopub.execute_input":"2022-07-05T07:44:12.951133Z","iopub.status.idle":"2022-07-05T07:44:12.960422Z","shell.execute_reply.started":"2022-07-05T07:44:12.951090Z","shell.execute_reply":"2022-07-05T07:44:12.958787Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def tf_encode(pt, en):\n    result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n    result_pt.set_shape([None])\n    result_en.set_shape([None])\n    \n    return result_pt, result_en","metadata":{"id":"8UL5-jfUjaZI","execution":{"iopub.status.busy":"2022-07-05T07:44:12.967429Z","iopub.execute_input":"2022-07-05T07:44:12.968250Z","iopub.status.idle":"2022-07-05T07:44:12.975579Z","shell.execute_reply.started":"2022-07-05T07:44:12.968092Z","shell.execute_reply":"2022-07-05T07:44:12.974086Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"A simple input pipeline that processes, shuffles and batches the data:","metadata":{"id":"MY_8bwbqjaZJ"}},{"cell_type":"code","source":"MAX_LENGTH = 128\ndef filter_max_length(x, y, maxlen = MAX_LENGTH):\n    return tf.logical_and(tf.size(x) <= maxlen, tf.size(y) <= maxlen)","metadata":{"id":"JIZ6wWfXjaZJ","execution":{"iopub.status.busy":"2022-07-05T07:44:12.977504Z","iopub.execute_input":"2022-07-05T07:44:12.978667Z","iopub.status.idle":"2022-07-05T07:44:12.987179Z","shell.execute_reply.started":"2022-07-05T07:44:12.978604Z","shell.execute_reply":"2022-07-05T07:44:12.985635Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"BUFFER_SIZE = 20000\nBATCH_SIZE = 64","metadata":{"id":"-pO97ltujaZK","execution":{"iopub.status.busy":"2022-07-05T07:44:12.989244Z","iopub.execute_input":"2022-07-05T07:44:12.990222Z","iopub.status.idle":"2022-07-05T07:44:13.001660Z","shell.execute_reply.started":"2022-07-05T07:44:12.990176Z","shell.execute_reply":"2022-07-05T07:44:13.000112Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_dataset = train_examples.map(tf_encode)\ntrain_dataset = train_dataset.filter(filter_max_length)\n# cache the dataset to memory to get a speedup while reading from it.\ntrain_dataset = train_dataset.cache()\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\ntrain_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\n\nval_dataset = valid_examples.map(tf_encode)\nval_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)","metadata":{"id":"wEZzh7mtjaZK","execution":{"iopub.status.busy":"2022-07-05T07:44:13.004797Z","iopub.execute_input":"2022-07-05T07:44:13.005730Z","iopub.status.idle":"2022-07-05T07:44:13.178711Z","shell.execute_reply.started":"2022-07-05T07:44:13.005685Z","shell.execute_reply":"2022-07-05T07:44:13.177477Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# let's see if it works as intended\npt_batch, en_batch = next(iter(val_dataset))\npt_batch, en_batch","metadata":{"id":"ty3gvA9sjaZK","outputId":"4349d584-6ebb-46b8-8c84-c7c99ad93aa9","execution":{"iopub.status.busy":"2022-07-05T07:44:13.180738Z","iopub.execute_input":"2022-07-05T07:44:13.181627Z","iopub.status.idle":"2022-07-05T07:44:13.351441Z","shell.execute_reply.started":"2022-07-05T07:44:13.181580Z","shell.execute_reply":"2022-07-05T07:44:13.349969Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"2022-07-05 07:44:13.337816: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(<tf.Tensor: shape=(64, 74), dtype=int64, numpy=\n array([[8214,  342, 3032, ...,    0,    0,    0],\n        [8214,   95,  198, ...,    0,    0,    0],\n        [8214, 4479, 7990, ...,    0,    0,    0],\n        ...,\n        [8214,   87, 3427, ...,    0,    0,    0],\n        [8214,  291,   99, ...,    0,    0,    0],\n        [8214,   95,    3, ...,    0,    0,    0]])>,\n <tf.Tensor: shape=(64, 70), dtype=int64, numpy=\n array([[8087,   98,   25, ...,    0,    0,    0],\n        [8087,   12,   20, ...,    0,    0,    0],\n        [8087,   12, 5453, ...,    0,    0,    0],\n        ...,\n        [8087,   25,   24, ...,    0,    0,    0],\n        [8087,   32, 1610, ...,    0,    0,    0],\n        [8087,   17,   20, ...,    0,    0,    0]])>)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Positional Encoding","metadata":{"id":"h5s1JlKdjaZL"}},{"cell_type":"markdown","source":"Since this model doesn't contain any recurrence or convolution, positional encoding is added to give the model some information about the relative position of the words in the sentence. \n​\nThe positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do not encode the relative position of words in a sentence. So after adding the positional encoding, words will be closer to each other based on the *similarity of their meaning and their position in the sentence*, in the d-dimensional space.\n​\nThe formula for calculating the positional encoding is as follows:\n​\n$$\\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $$\n$$\\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $$","metadata":{}},{"cell_type":"code","source":"def get_angles(pos, i, d_model):\n    \"\"\"Function to add positional encoding to the vector embeddings to be fed to the transformer\n    Follows the formula given by the Transformer paper.\n    \n    d_model: refers to the dimension of the model used\n    \"\"\"\n    angle_rates = 1 / np.power(10000, (2 * (i//2))/np.float32(d_model))\n    return pos * angle_rates\n\ndef positional_encoding(position, d_model):\n    angle_rads = get_angles(np.arange(position)[:, np.newaxis], \n                            np.arange(d_model)[np.newaxis,:], \n                            d_model)\n    \n    # apply sin to even indices in the array\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n    \n    # apply cos to odd indices in the array\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n    \n    pos_encoding = angle_rads[np.newaxis, ...]\n    \n    return tf.cast(pos_encoding, dtype=tf.float32)","metadata":{"id":"u-N4NzLGjaZL","execution":{"iopub.status.busy":"2022-07-05T07:44:13.353683Z","iopub.execute_input":"2022-07-05T07:44:13.354178Z","iopub.status.idle":"2022-07-05T07:44:13.364360Z","shell.execute_reply.started":"2022-07-05T07:44:13.354136Z","shell.execute_reply":"2022-07-05T07:44:13.362971Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"pos_encoding = positional_encoding(50, 512)\npos_encoding","metadata":{"id":"zvVCgIwujaZL","outputId":"4f864aab-07d2-4ee2-f1ae-d8482daf048a","execution":{"iopub.status.busy":"2022-07-05T07:44:13.367076Z","iopub.execute_input":"2022-07-05T07:44:13.367630Z","iopub.status.idle":"2022-07-05T07:44:13.388388Z","shell.execute_reply.started":"2022-07-05T07:44:13.367587Z","shell.execute_reply":"2022-07-05T07:44:13.386777Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(1, 50, 512), dtype=float32, numpy=\narray([[[ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00, ...,\n          1.00000000e+00,  0.00000000e+00,  1.00000000e+00],\n        [ 8.41470957e-01,  5.40302277e-01,  8.21856201e-01, ...,\n          1.00000000e+00,  1.03663289e-04,  1.00000000e+00],\n        [ 9.09297407e-01, -4.16146845e-01,  9.36414719e-01, ...,\n          1.00000000e+00,  2.07326579e-04,  1.00000000e+00],\n        ...,\n        [ 1.23573124e-01, -9.92335498e-01,  9.77189839e-01, ...,\n          9.99987245e-01,  4.87215538e-03,  9.99988139e-01],\n        [-7.68254638e-01, -6.40144348e-01,  7.31235921e-01, ...,\n          9.99986708e-01,  4.97581763e-03,  9.99987602e-01],\n        [-9.53752637e-01,  3.00592542e-01, -1.44026920e-01, ...,\n          9.99986112e-01,  5.07947942e-03,  9.99987125e-01]]],\n      dtype=float32)>"},"metadata":{}}]},{"cell_type":"code","source":"plt.pcolormesh(pos_encoding[0], cmap='RdBu')\nplt.xlabel('Depth')\nplt.xlim((0, 512))\nplt.ylabel('Position')\nplt.colorbar()\nplt.show()","metadata":{"id":"ZgnOqQNcjaZM","outputId":"df3589a6-1e94-4860-a2ae-d27d13e65147","execution":{"iopub.status.busy":"2022-07-05T07:44:13.390642Z","iopub.execute_input":"2022-07-05T07:44:13.391468Z","iopub.status.idle":"2022-07-05T07:44:13.728704Z","shell.execute_reply.started":"2022-07-05T07:44:13.391425Z","shell.execute_reply":"2022-07-05T07:44:13.727266Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"## Masking","metadata":{"id":"W8XQNTp7jaZM"}},{"cell_type":"markdown","source":"Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value 0 is present: it outputs a 1 at those locations, and a 0 otherwise","metadata":{"id":"QavR1KH2jaZM"}},{"cell_type":"code","source":"def create_padding_mask(seq):\n    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n    \n    # add extra dimension to add padding to the attention logits\n    return seq[:, tf.newaxis, tf.newaxis, :] # (batch_size, 1, 1, seq_len)","metadata":{"id":"u7ab6SytjaZM","execution":{"iopub.status.busy":"2022-07-05T07:44:13.730412Z","iopub.execute_input":"2022-07-05T07:44:13.731882Z","iopub.status.idle":"2022-07-05T07:44:13.739462Z","shell.execute_reply.started":"2022-07-05T07:44:13.731819Z","shell.execute_reply":"2022-07-05T07:44:13.738107Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# let's test this out\nx = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\ncreate_padding_mask(x)","metadata":{"id":"6BgUcig7jaZN","outputId":"fd383b38-efd4-4193-95d8-479eb6d9d713","execution":{"iopub.status.busy":"2022-07-05T07:44:13.741756Z","iopub.execute_input":"2022-07-05T07:44:13.742877Z","iopub.status.idle":"2022-07-05T07:44:13.766634Z","shell.execute_reply.started":"2022-07-05T07:44:13.742847Z","shell.execute_reply":"2022-07-05T07:44:13.764673Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\narray([[[[0., 0., 1., 1., 0.]]],\n\n\n       [[[0., 0., 0., 1., 1.]]],\n\n\n       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"},"metadata":{}}]},{"cell_type":"code","source":"def create_look_ahead_mask(size):\n    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n    return mask  # (seq_len, seq_len)","metadata":{"id":"i_PigtnyjaZN","execution":{"iopub.status.busy":"2022-07-05T07:44:13.768716Z","iopub.execute_input":"2022-07-05T07:44:13.769359Z","iopub.status.idle":"2022-07-05T07:44:13.776972Z","shell.execute_reply.started":"2022-07-05T07:44:13.769306Z","shell.execute_reply":"2022-07-05T07:44:13.775571Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def create_masks(inp, tar):\n    # encoder padding mask (also necessary for 2nd attention block in the decoder)\n    enc_padding_mask = create_padding_mask(inp)\n        \n    # dec padding mask - used in the 2nd attention block in the decoder\n    dec_padding_mask = create_padding_mask(inp)\n        \n    # used in the first attention block\n    # used to pad and mask future tokens in the tokens received by the decoder\n    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n    dec_target_padding_mask = create_padding_mask(tar)\n    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n        \n    return enc_padding_mask, combined_mask, dec_padding_mask","metadata":{"id":"5Hkf2q-NjaZV","execution":{"iopub.status.busy":"2022-07-05T07:44:13.778514Z","iopub.execute_input":"2022-07-05T07:44:13.779670Z","iopub.status.idle":"2022-07-05T07:44:13.788453Z","shell.execute_reply.started":"2022-07-05T07:44:13.779613Z","shell.execute_reply":"2022-07-05T07:44:13.786935Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Attention","metadata":{"id":"AzISh7zpjaZN"}},{"cell_type":"markdown","source":"![](https://e2eml.school/images/transformers/architecture_single_head.png)\n\nThe primary concept to grasp about Transformers is the self-attention or intra attention mechanism. The self in self-attention refers to the fact that the attention weights are computed for all hidden states in the same set - for example, all the hidden states of the encoder. As the transformer model processes each word, self-attention allows it to look at other positions in the input sequences for clues that can help it lead to a better encoding for the particular word.\n\nThe equation used to calculate the attention weights is:\n\n$$\\Large{Attention(Q, K, V) = softmax_k\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V} $$\n\nThe main idea behind attention is that instead of producing a single hidden state for the input sequence, the encoder outputs a hidden state at each step that the decoder can access. However, using all the states at the same time would create a huge input for the decoder. This is where attention comes in - it lets the decoder assign a different amount of weight or attention to each of the encoder states at every decoding timestep.\n\nSteps:\n\na. Create three vectors from each of the encoder's input vectors(embeddings of each word). The dimension of the input embeddings is given as d_model in the paper and they used an embedding size of 512. For each attention unit the transformer model learns three weight matrices; the query weights, the key weights, and the value weights. For each token, the input word embedding is multiplied with each of the three weight matrices to produce a query vector Q((vector representation of one word in the sequence), a key vector K (vector representations of all the words in the sequence), and a value vector V(which are the vector representations of all the words in the sequence).\n\nThe query Q represents the feature of interest and the matrix K represents the collection of masks. Because it's stored with masks in columns, rather than rows, it needs to be transposed (with the T operator) before multiplying. The vectors q, k, v are smaller in dimension than the embedding vector. The reason for this will be explained soon.\n\nb. Calculate a score: The attention score is calculated by taking the dot product of the query vector and the key vector of the respective word we're scoring. For a sequence of n inputs, we'd have a corresponding n x n matrix of attention scores.\n\nc. Scaling: The attention scores are divided by √dk (dimension of the key vector or depth). This dimension is equal to 64 in the paper, so the matrix of attention scores is scaled by 1/8. This is done because for large values of depth, the dot product grows large in magnitude pushing the softmax function where it has small gradients resulting in a very hard softmax. For example, consider that Q and K have a mean of 0 and variance of 1. Their matrix multiplication will have a mean of 0 and variance of dk. So the square root of dk is used for scaling, so you get a consistent variance regardless of the value of dk. If the variance is too low the output may be too flat to optimize effectively. If the variance is too high the softmax may saturate at initialization making it difficult to learn\n\nd. Normalized: The attention scores in the matrix are normalized so that they add up to 1. This is done with the softmax function. The matrix obtained after this step is known as the attention weights.\n\ne. Multiply the value vector V for each word by the attention weights and then sum up to produce the output of the self-attention lyer for that position. It is probably helpful to think of the attention weights as where to look and the Value vector as what one wants to actually get. ","metadata":{}},{"cell_type":"code","source":"def scaled_dot_product_attention(q, k, v, mask):\n    \"\"\"calculate the attention weights. \n    q - query vectors; shape == (..., seq_len_q, depth)\n    k: key;            shape == (..., seq_len_k, depth)\n    v: value;          shape == (..., seq_len_v, depth_v)\n   \n    The process in the encoder is called self-attention because these three vectors are from the same source\n    The q, k, v must have the same dimensions - d_q, d_k, d_v respectively. In this implementation, they'll be set to 64\n    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n    \n    The mask has different shapes depending on its type(padding or look ahead)\n    but it must be broadcastable for addition.\n    \"\"\"\n    \n    matmul_qk = tf.matmul(q, k, transpose_b = True)\n    \n    # scale matmul_qk\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk/tf.math.sqrt(dk)\n    \n    # add mask\n    if mask is not None:\n        scaled_attention_logits += (mask * -1e9)\n    \n    # normalize the scaled_attention_logits with softmax so they add up to 1\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1) # (..., seq_len_q, seq_len_k)\n    \n    output = tf.matmul(attention_weights, v)\n    return output, attention_weights","metadata":{"id":"TjllbYqOjaZN","execution":{"iopub.status.busy":"2022-07-05T07:44:13.790716Z","iopub.execute_input":"2022-07-05T07:44:13.791277Z","iopub.status.idle":"2022-07-05T07:44:13.803058Z","shell.execute_reply.started":"2022-07-05T07:44:13.791235Z","shell.execute_reply":"2022-07-05T07:44:13.801755Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# let's see if this works as intended\n\ndef print_out(q, k, v):\n    temp_out, temp_attn = scaled_dot_product_attention(q, k, v, None)\n    print('Attention weights are:')\n    print(temp_attn)\n    print('\\nOutput is:')\n    print(temp_out)","metadata":{"id":"7P7T_RW1jaZN","execution":{"iopub.status.busy":"2022-07-05T07:44:13.806737Z","iopub.execute_input":"2022-07-05T07:44:13.807595Z","iopub.status.idle":"2022-07-05T07:44:13.818813Z","shell.execute_reply.started":"2022-07-05T07:44:13.807567Z","shell.execute_reply":"2022-07-05T07:44:13.817253Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"np.set_printoptions(suppress=True)\n\ntemp_k = tf.constant([[10, 0, 0],\n                      [0, 10, 0],\n                      [0, 0, 10],\n                      [0, 0, 10]], dtype=tf.float32)  # (4, 3)\n\ntemp_v = tf.constant([[1, 0],\n                      [10, 0],\n                      [100, 5],\n                      [1000, 6]], dtype=tf.float32) # (4, 2)\n\ntemp_q = tf.constant([[0, 0, 10],\n                      [0, 10, 0],\n                      [10, 10, 0]], dtype=tf.float32)\n                     \nprint_out(temp_q, temp_k, temp_v)","metadata":{"id":"O8e2PDYnjaZO","outputId":"0d4a4fd7-a427-41b9-93ba-a749b7ebcd87","execution":{"iopub.status.busy":"2022-07-05T07:44:13.821410Z","iopub.execute_input":"2022-07-05T07:44:13.822638Z","iopub.status.idle":"2022-07-05T07:44:14.818464Z","shell.execute_reply.started":"2022-07-05T07:44:13.822478Z","shell.execute_reply":"2022-07-05T07:44:14.817128Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Attention weights are:\ntf.Tensor(\n[[0.  0.  0.5 0.5]\n [0.  1.  0.  0. ]\n [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n\nOutput is:\ntf.Tensor(\n[[550.    5.5]\n [ 10.    0. ]\n [  5.5   0. ]], shape=(3, 2), dtype=float32)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Multi-Head Attention","metadata":{"id":"Abb4lufJjaZO"}},{"cell_type":"markdown","source":"<img src=\"https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png\" width=\"500\" alt=\"multi-head attention\">\n\nMulti-head attention consists of these parts:\n*    Linear layers.\n*    Scaled dot-product attention.\n*    Final linear layer.\n\n\nThe self-attention is refined by carrying it out across several heads. In the paper, they carried out the self-attention step across 8 head. This is known as Multi-Head Attention. Each attention head( (an instance of attention computation) computes its own attention.\n\nSo far according to the paper, we have d_model = 512 (dimension of the embeddings), d_k = 64 (dimension of the key and query vectors), and h = 8 (number of attention heads). The paper also used d_v (dimension of the value vector) to be 64 just like the key and query vectors, however this doesn't have to be so.\n\n####  Why is Multi-Head Attention Needed?\n\nLooking at the attention formula given above, we see that the softmax functions which outputs probabilities forces the attention calculation in the range (0, 1) and it helps to emphasize the highest value, while agressively squashing the smallest. The problem with this is that it allows attention to focus on a single element (the one with the highest probability) and leave out the rest which still have a lot of information in them. Sometimes it's useful to keep several of the preceding words in mind when predicting the next, and the softmax just robbed us of that. This is a problem for the model.\n\nThe solution to this problem is to have several instances of attention or heads running at once. This lets the the transformer consider several previous words simultaneously when predicting the next. Instead of one single attention head, Q, K, and V are split into multiple heads because it allows the model to jointly attend to information from different representation subspaces at different positions. It brings back the power we had before we pulled the softmax into the picture. \n\nThe intuition behind multi-head attention is that it allows us to attend to different parts of the sequence differently each time. This practically means that:\n\na. The model can better capture positional information because each head will attend to different segments of the input. The combination of them will give us a more robust representation.\n\nb. Each head will capture different contextual information as well, by correlating words in a unique manner.\n\nHowever, computing many attention heads require a lot of computation. For the attention step to scale well, the query, key and value vectors are projected into lower-dimensional embedding space. This shrinks the matrices involved and dramatically reduces the computation time.\n\n#### Next step:\n\nHowever, the next step in the transformer architecture is not receiving matrices from *h* attention heads (8 in the paper). It is expecting a single matrix for each word. So, the multi-head attention matrices would need to be condensed:\n\na. Concatenate all attention matrices into one giant matrix.\n\nb. Multiply the result of a above by Wo (weight matrix) that was trained jointly with the model.\n\nThis is shown below:\n\n![](https://e2eml.school/images/transformers/multihead_attention_equation.png)","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n        \n        assert d_model % self.num_heads == 0\n        \n        self.depth = d_model//self.num_heads\n        \n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n        \n        self.dense = tf.keras.layers.Dense(d_model)\n        \n    def split_heads(self, x, batch_size):\n        \"\"\"Split the last dimension into (num_heads, depth)\n        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n        \"\"\"\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n    \n    def get_config(self):\n        \"\"\"Implement serialization so we can save the model\"\"\"\n        config = super().get_config()\n        config.update({\n            \"d_model\": self.d_model,\n            \"num_heads\": self.num_heads,\n        })\n        return config\n    \n    def call(self, v, k, q, mask):\n        batch_size = tf.shape(q)[0]\n        \n        q = self.wq(q) # (batch_size, seq_len, d_model)\n        k = self.wk(k) # (batch_size, seq_len, d_model)\n        v = self.wv(v) # # (batch_size, seq_len, d_model)\n        \n        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n        \n        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n        \n        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3]) # (batch_size, seq_len_q, num_heads, depth)\n        \n        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n        output = self.dense(concat_attention) #(batch_size, seq_len_q, d_model)\n        \n        return output, attention_weights","metadata":{"id":"mI70-mfrjaZO","execution":{"iopub.status.busy":"2022-07-05T07:44:14.822692Z","iopub.execute_input":"2022-07-05T07:44:14.823082Z","iopub.status.idle":"2022-07-05T07:44:15.788671Z","shell.execute_reply.started":"2022-07-05T07:44:14.823054Z","shell.execute_reply":"2022-07-05T07:44:15.787430Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# let's check if this works as intended\ntemp_multihead = MultiHeadAttention(d_model=512, num_heads=8)\n\ny = tf.random.uniform((1, 60, 512))\nout, attn = temp_multihead(y, k=y, q=y, mask=None)","metadata":{"id":"SpN2nNkfjaZP","execution":{"iopub.status.busy":"2022-07-05T07:44:15.790294Z","iopub.execute_input":"2022-07-05T07:44:15.790728Z","iopub.status.idle":"2022-07-05T07:44:15.873206Z","shell.execute_reply.started":"2022-07-05T07:44:15.790684Z","shell.execute_reply":"2022-07-05T07:44:15.871744Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"out.shape, attn.shape","metadata":{"id":"2y3d7AofjaZP","outputId":"ca202ba6-a460-4d45-999e-d75d668237fd","execution":{"iopub.status.busy":"2022-07-05T07:44:15.875936Z","iopub.execute_input":"2022-07-05T07:44:15.876666Z","iopub.status.idle":"2022-07-05T07:44:15.886467Z","shell.execute_reply.started":"2022-07-05T07:44:15.876618Z","shell.execute_reply":"2022-07-05T07:44:15.884994Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"},"metadata":{}}]},{"cell_type":"markdown","source":"## Pointwise Feed-Forward Network","metadata":{"id":"sI0wwcH6jaZP"}},{"cell_type":"markdown","source":"The FFN layer in the encoder and decoder is just a simple two-layer fully-connected network which processes each embeddings independently. A rule of thumb from the paper is that the hidden units of the first layer to be 4x the size of the embeddings. That is, dff = 4 x d_model. \nThe FFN is labeled as Linear in the transformer architecture diagram above.","metadata":{}},{"cell_type":"code","source":"def point_wise_feed_forward_network(d_model, dff):\n    return tf.keras.Sequential([\n      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n  ])","metadata":{"id":"TG1fCFXrjaZP","execution":{"iopub.status.busy":"2022-07-05T07:44:15.899403Z","iopub.execute_input":"2022-07-05T07:44:15.900409Z","iopub.status.idle":"2022-07-05T07:44:15.907637Z","shell.execute_reply.started":"2022-07-05T07:44:15.900363Z","shell.execute_reply":"2022-07-05T07:44:15.905795Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"sample_ffn = point_wise_feed_forward_network(512, 30)\nsample_ffn(tf.random.uniform((64, 50, 512))).shape","metadata":{"id":"fh1pbyXJjaZP","outputId":"979d4c49-a110-4677-c877-ff0b0827beab","execution":{"iopub.status.busy":"2022-07-05T07:44:15.909535Z","iopub.execute_input":"2022-07-05T07:44:15.910763Z","iopub.status.idle":"2022-07-05T07:44:15.959477Z","shell.execute_reply.started":"2022-07-05T07:44:15.910713Z","shell.execute_reply":"2022-07-05T07:44:15.958051Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"TensorShape([64, 50, 512])"},"metadata":{}}]},{"cell_type":"markdown","source":"## The Encoder Layer","metadata":{"id":"OidGQJKcjaZQ"}},{"cell_type":"markdown","source":"Each encoder layer consists of:\n\na. Multi-Head Attention layer\n\nb. Feed Forward layer\n\nThe encoder layers also include layer normalization and skip connections. This is represented by the Add & Norm section of the transformer architecture diagram above. \n\n#### Skip Connections\nSkip connections occur around the Multi-Head attention blocks and around the element-wise feed forward blocks. Skip connections give a transformer a tiny ability to allow the representations of different levels of processing to interact.\n\nIn skip connections, a copy of the input is added to the output of a set of calculations. This is seen in the code block below as the output of the multi-head attention is added to the vector k, q, v again - the inputs to the attention block are added back to its output. Also, the input to the element-wise feed forward network are added to its outputs. \n\nThis serves two purposes:\na. It helps keep the gradient smooth which is a big help for backpropagtion: Attention is a filter, which means that when it is working correctly, it will block most of what tries to pass through it. The result of this is small changes in a lot of the inputs may not produce much change in the output if they happen to fall into the channels that are blocked. This produces dead spots in the gradients where it is flat. Skip connections help to smooth these out.\n\nb. It helps to preserve the original input sequence: Even with lots of attention heads, there's no guarantee a word will attend to its own position. It's possible for the attention filter to forget entirely about the most recent word in favour of watching all of the earlier words that might be relevant. A skip connection takes the original word and manually adds it back into the signal so that there's no way it can be dropped or forgotten.\n\nThanks to skip connections, successive layers don't provide increasingly sophisticated abstractions as much as they provide redundancy. Whatever opportunities for focusing attention and creating useful features and making accurate predictions were missed in one layer can always be caught by the next.\n\n#### Layer Normalization\n\nThe values of the matrix are shifted to have a mean of zero and scaled to have a standard deviation of one.","metadata":{}},{"cell_type":"code","source":"class EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(EncoderLayer, self).__init__()\n        \n        self.multihead = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n        \n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        \n        self.dropout1 = tf.keras.layers.Dropout(rate=rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate=rate)\n        \n    def call(self, x, training, mask):\n        attn_output, _ = self.multihead(x, x, x, mask)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(x + attn_output)\n        \n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm2(out1 + ffn_output)\n        \n        return out2","metadata":{"id":"1__OYq8GjaZQ","execution":{"iopub.status.busy":"2022-07-05T07:44:15.961232Z","iopub.execute_input":"2022-07-05T07:44:15.961937Z","iopub.status.idle":"2022-07-05T07:44:15.976101Z","shell.execute_reply.started":"2022-07-05T07:44:15.961870Z","shell.execute_reply":"2022-07-05T07:44:15.974742Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# test\nsample_encoder_layer = EncoderLayer(d_model=512, num_heads=8, dff=2048)\n\nsample_encoder_layer_output = sample_encoder_layer(tf.random.uniform((64, 43, 512)), False, mask=None)\nsample_encoder_layer_output.shape","metadata":{"id":"GeigGGjwjaZQ","outputId":"18a34c92-bf98-4461-bcb4-0ffc16c504b1","execution":{"iopub.status.busy":"2022-07-05T07:44:15.980113Z","iopub.execute_input":"2022-07-05T07:44:15.980941Z","iopub.status.idle":"2022-07-05T07:44:16.075264Z","shell.execute_reply.started":"2022-07-05T07:44:15.980875Z","shell.execute_reply":"2022-07-05T07:44:16.073624Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"TensorShape([64, 43, 512])"},"metadata":{}}]},{"cell_type":"markdown","source":"## The Decoder Layer","metadata":{"id":"GNSSmyCojaZQ"}},{"cell_type":"markdown","source":"Each decoder layer consists of sublayers:\n\n1.   Masked multi-head attention (with look ahead mask and padding mask)\n2.   Multi-head attention (with padding mask). V (value) and K (key) receive the *encoder output* as inputs. Q (query) receives the *output from the masked multi-head attention sublayer.*\n3.   Point wise feed forward networks\n\nI'll skip the explanation of differences until we stack up the decoder layers later below","metadata":{}},{"cell_type":"code","source":"class DecoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(DecoderLayer, self).__init__()\n        self.multihead1 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n        self.multihead2 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n        \n        self.ffn = point_wise_feed_forward_network(d_model=d_model, dff=dff)\n        \n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        \n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n        self.dropout3 = tf.keras.layers.Dropout(rate)\n        \n    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n        attn1, attn_weights_block1 = self.multihead1(x, x, x, look_ahead_mask)\n        attn1 = self.dropout1(attn1, training=training)\n        out1 = self.layernorm1(attn1 + x) # becomes the query vector in the next step\n        \n        attn2, attn_weights_block2 = self.multihead2(enc_output, enc_output, out1, padding_mask)\n        attn2 = self.dropout2(attn2, training=training)\n        out2 = self.layernorm2(attn2 + out1)\n        \n        ffn_output = self.ffn(out2)\n        ffn_output = self.dropout3(ffn_output, training=training)\n        out3 = self.layernorm3(ffn_output + out2)\n        \n        return out3, attn_weights_block1, attn_weights_block2","metadata":{"id":"DxT7G_38jaZQ","execution":{"iopub.status.busy":"2022-07-05T07:44:16.077208Z","iopub.execute_input":"2022-07-05T07:44:16.078051Z","iopub.status.idle":"2022-07-05T07:44:16.091815Z","shell.execute_reply.started":"2022-07-05T07:44:16.077989Z","shell.execute_reply":"2022-07-05T07:44:16.090302Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# let's test this out\nsample_decoder_layer = DecoderLayer(d_model=512, num_heads=8, dff=2048)\nsample_decoder_layer_output, _, _ = sample_decoder_layer(x=tf.random.uniform((64, 50, 512)), enc_output=sample_encoder_layer_output, \n                                                        training=False, look_ahead_mask=None, padding_mask=None)\n\nsample_decoder_layer_output.shape","metadata":{"id":"UsSLAs8yjaZR","outputId":"8dde9032-4af5-4043-adac-f1ffe9be1622","execution":{"iopub.status.busy":"2022-07-05T07:44:16.093818Z","iopub.execute_input":"2022-07-05T07:44:16.094690Z","iopub.status.idle":"2022-07-05T07:44:16.197501Z","shell.execute_reply.started":"2022-07-05T07:44:16.094618Z","shell.execute_reply":"2022-07-05T07:44:16.195940Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"TensorShape([64, 50, 512])"},"metadata":{}}]},{"cell_type":"markdown","source":"## The Encoder","metadata":{"id":"pWkRfVDPjaZR"}},{"cell_type":"markdown","source":"The Encoder class implemented below consists of the following layers:\n\na. Embedding layer\n\nb. Positional Encoding\n\nc. Stacks of the encoder sublayers\n\n#### Embedding\n\nWord embeddings are a representation of a symbol (word, character, sentence) in a distributed low-dimensional space of continuous-valued vectors. Word embeddings map human language into a structured geometric space. As opposed to one-hot encodings, word embeddings consist of floating-point vectors (that is, dense vectors as opposed to sparse vectors). It’s common to seeword embeddings that are 256-dimensional, 512-dimensional, or 1,024-dimensional unlike the one-hot encoding paradign with thousands of dimensions. This means that embeddings packs more information into far fewer dimensions (high embedding dimensions is more information retained).\n\nA good embedding groups words with similar meanings together. A model that works with an embedding learns patterns in the embedded space. That means that whatever it learns to do with one word automatically gets applied to all the words right next to it. This has the added benefit of reducing the amount of training data needed. Each example gives a little bit of learning that gets applied across a whole neighborhood of words. The embeddings is done below with the tf.keras,Embedding(sequence_length, embedding_dimensionality). The dimension for embedding (from the paper) is d_model = 512.\n\n#### Positional Encoding\n\nAs explained in the section above, position information need to be added to the word embeddings. As it's an addition operation, it doesn't alter the shape of the embeddings vector. \n\n#### Stack of Encoder sublayers\n\nAsides the embeddings and positional encoding, the rest of the encoder consists of multi-head attention block and the element-wise feed forward network. Each encoder sublayer passes its output to the next encoder layer until the end which is then passed on to the decoder for the decoding to begin. The number of encoding sublayers can also be controlled by the num_layers argument (6 layers was used in the paper) in the code below","metadata":{}},{"cell_type":"code","source":"class Encoder(tf.keras.layers.Layer):\n    def __init__(self, *, num_layers, d_model, num_heads, dff, input_vocab_size, \n                 maximum_position_encoding, rate=0.1):\n        super(Encoder, self).__init__()\n        \n        self.d_model = d_model\n        self.num_layers = num_layers\n        \n        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n        \n        self.enc_layers = [EncoderLayer(d_model=self.d_model, num_heads=num_heads, dff=dff, rate=rate)\n                          for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(rate)\n        \n    def get_config(self):\n        \"\"\"Initialize serialization so we can save model\"\"\"\n        config = super().get_config()\n        config.update({\n            \"num_layers\": num_layers,\n            \"d_model\": d_model,\"num_heads\": num_heads,\n            \"dff\": dff, \"input_vocab_size\": input_vocab_size,\n            \"maximum_position_encoding\": MAX_LENGTH,\n            \"rate\": dropout_rate\n        })\n        return config\n        \n    def call(self, x, training, mask):\n        seq_len = tf.shape(x)[1]\n        \n        # add embedding and position encoding\n        x = self.embedding(x)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) # tf.math.sqrt doesn't support int types\n        x += self.pos_encoding[:, :tf.shape(x)[1], :]\n        \n        x = self.dropout(x, training = training)\n        \n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x, training, mask)\n        return x","metadata":{"id":"AILa3v_4jaZR","execution":{"iopub.status.busy":"2022-07-05T07:44:16.201712Z","iopub.execute_input":"2022-07-05T07:44:16.202208Z","iopub.status.idle":"2022-07-05T07:44:16.214988Z","shell.execute_reply.started":"2022-07-05T07:44:16.202177Z","shell.execute_reply":"2022-07-05T07:44:16.213385Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# let's test this\nsample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n                         dff=2048, input_vocab_size=8500,\n                         maximum_position_encoding=10000)\ntemp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n\nsample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n\nprint (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)","metadata":{"id":"pxXDY6h3jaZR","outputId":"d0767281-9a72-4dd5-daef-598151af199c","execution":{"iopub.status.busy":"2022-07-05T07:44:16.217048Z","iopub.execute_input":"2022-07-05T07:44:16.218233Z","iopub.status.idle":"2022-07-05T07:44:16.532648Z","shell.execute_reply.started":"2022-07-05T07:44:16.218177Z","shell.execute_reply":"2022-07-05T07:44:16.531229Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"(64, 62, 512)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## The Decoder","metadata":{"id":"FR0qswrxjaZS"}},{"cell_type":"markdown","source":"Just like the Encoder, the Decoder also consists of the:\n\na. Embedding\n\nb. Positional encoding\n\nc. Stacks of decoding layers.\n\nThe output of the decoder is passed on to the final layer which is followed by a softmax layer so it outputs the probabilities of a particular word for every step.\n\nA major difference between the Encoder and the Decoder is that the decoder has two attention sublayers\n\na. Masked multi-head attention layer: ensures that the tokens we generate at each timestep are only based on the past outputs and current tokens being predicted. To achieve this, we mask the next word embeddings (by setting them to −inf)\n\nb. Encoder - decoder attention layer:This is actually where the decoder processes the encoded representation. The attention matrix generated by the encoder is passed to another attention layer alongside the result of the previous Masked Multi-head attention block.\n\nThis performs multihead attention over the output key and value vectors of the encoder stack with the intermediate representations of the decoder acting as the queries. The output of the top encoder is transformed into a set of attention vectors K and V. These are to be used by each decoder in its encoder-decoder attention layer. These are to be used by each decoder focus on appropriate places in the input sequence.\n\nThe results of the decoder inputs are embedded with positional encoding to indicate the position of each word. \nThe self-attention layer in the decoder is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions.","metadata":{}},{"cell_type":"code","source":"class Decoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, \n                 maximum_position_encoding, rate=0.1):\n        super(Decoder, self).__init__()\n        \n        self.d_model = d_model\n        self.num_layers = num_layers\n        \n        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n        \n        self.dec_layers = [DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate) \n                           for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(rate)\n        \n    \n    def get_config(self):\n        \"\"\"Initialize serialization so we can save model\"\"\"\n        config = super().get_config()\n        config.update({\n            \"num_layers\": num_layers,\n            \"d_model\": d_model,\"num_heads\": num_heads,\n            \"dff\": dff, \"target_vocab_size\": target_vocab_size,\n            \"maximum_position_encoding\": MAX_LENGTH,\n            \"rate\": dropout_rate\n        })\n        return config\n        \n    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n        seq_len = tf.shape(x)[1]\n        attention_weights = {}\n        \n        x = self.embedding(x)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n        x = self.dropout(x, training=training)\n        \n        for i in range(self.num_layers):\n            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n            \n            attention_weights[f\"decoder_layer{i + 1}_block1\"] = block1\n            attention_weights[f\"decoder_layer{i + 1}_block2\"] = block2\n        \n        return x, attention_weights","metadata":{"id":"1-pAJ84NjaZS","execution":{"iopub.status.busy":"2022-07-05T07:44:16.534319Z","iopub.execute_input":"2022-07-05T07:44:16.536200Z","iopub.status.idle":"2022-07-05T07:44:16.551394Z","shell.execute_reply.started":"2022-07-05T07:44:16.536156Z","shell.execute_reply":"2022-07-05T07:44:16.549838Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n                         dff=2048, target_vocab_size=8000,\n                         maximum_position_encoding=5000)\ntemp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n\noutput, attn = sample_decoder(temp_input, \n                              enc_output=sample_encoder_output, \n                              training=False,\n                              look_ahead_mask=None, \n                              padding_mask=None)\n\noutput.shape, attn['decoder_layer2_block2'].shape","metadata":{"id":"472BQk66jaZS","outputId":"61584846-cbe9-40b4-c58e-bbc79e2eb36f","execution":{"iopub.status.busy":"2022-07-05T07:44:16.555099Z","iopub.execute_input":"2022-07-05T07:44:16.555421Z","iopub.status.idle":"2022-07-05T07:44:16.800457Z","shell.execute_reply.started":"2022-07-05T07:44:16.555394Z","shell.execute_reply":"2022-07-05T07:44:16.799047Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"},"metadata":{}}]},{"cell_type":"markdown","source":"## The Transformer\n\n![](https://miro.medium.com/max/700/1*BHzGVskWGS_3jEcYYi6miQ.png)\n\nLet's offer a high-level explanation of the transformer architecture here:\n\na. The Transformer consists of two arms - the Encoder and Decoder. The Encoder generates a rich vector representation of the input while the decoder predicts the next token based on the tokens it's being fed. The transformer architecture can be used for sequence-sequence tasks like translation.\n\nb. The input to the Encoder is the word vectors obtained after tokenizing and padding the sequences from the text input. The STARTOFSEQUENCE and ENDOFSEQUENCE tokens needs to be added at the start and end of each text input. \n\nc. A dense vector embeddings of the input text is created. Positional encodings are added to the embeddings and this is fed to several attention heads. Layer normalization and skip connections are involved with this process so as to ensure smooth gradients for backpropagation.\n\nd. The Encoder has N encoder sublayers and the output of one is passed to the next and the output at the top encoder is passed to the decoder.\n\ne. The decoder has almost the exact architecture as the decoder but it computes a second type of attention - ensuring that future predictions only take the current and past tokens into considerations. This is done via masking as implemented in the previous section.\n\nf. The output of the top decoder is passed through a final Dense layer and a softmax layer to output probabilities at each position step. At each step of prediction, the decoder uses only the current and past tokens, and the token with the highest probability at that stage is the output which is further being decoded to have it in text.","metadata":{}},{"cell_type":"markdown","source":"## Training Using Keras Functional API","metadata":{"id":"qeoUaGNh0_62"}},{"cell_type":"markdown","source":"### Hyperparameters and optimizer","metadata":{"id":"g9Cl4orB1HVb"}},{"cell_type":"code","source":"num_layers = 4\nd_model = 128\ndff = 512\nnum_heads = 8\ndropout_rate = 0.1\n\ninput_vocab_size = tokenizer_pt.vocab_size + 2\ntarget_vocab_size = tokenizer_en.vocab_size + 2","metadata":{"id":"kCCIKgiRjaZT","execution":{"iopub.status.busy":"2022-07-05T07:44:16.802019Z","iopub.execute_input":"2022-07-05T07:44:16.803057Z","iopub.status.idle":"2022-07-05T07:44:16.810262Z","shell.execute_reply.started":"2022-07-05T07:44:16.803010Z","shell.execute_reply":"2022-07-05T07:44:16.808691Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# learning rate\nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps = 4000):\n        super(CustomSchedule, self).__init__()\n        \n        self.d_model = d_model\n        self.d_model = tf.cast(self.d_model, tf.float32)\n        self.warmup_steps = warmup_steps\n        \n    #def get_config(self):\n        #config = super().get_config()\n        #return {\"d_model\": self.d_model, \"warmup_steps\": self.warmup_steps}\n        \n        \n    def __call__(self, step):\n        arg1 = tf.math.rsqrt(step)\n        arg2 = step * (self.warmup_steps ** -1.5)\n        \n        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n    \n   ","metadata":{"id":"XKF34-9XjaZT","execution":{"iopub.status.busy":"2022-07-05T07:44:16.811713Z","iopub.execute_input":"2022-07-05T07:44:16.812960Z","iopub.status.idle":"2022-07-05T07:44:16.824161Z","shell.execute_reply.started":"2022-07-05T07:44:16.812883Z","shell.execute_reply":"2022-07-05T07:44:16.822869Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"learning_rate = CustomSchedule(d_model)\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2 = 0.98, epsilon = 1e-9)","metadata":{"id":"cCTPPCR5jaZU","execution":{"iopub.status.busy":"2022-07-05T07:44:16.826170Z","iopub.execute_input":"2022-07-05T07:44:16.827415Z","iopub.status.idle":"2022-07-05T07:44:16.836276Z","shell.execute_reply.started":"2022-07-05T07:44:16.827371Z","shell.execute_reply":"2022-07-05T07:44:16.835039Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"temp_learning_rate_schedule = CustomSchedule(d_model)\n\nplt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\nplt.ylabel('Learning Rate')\nplt.xlabel('Train Step')","metadata":{"id":"mJCeV2-QjaZU","outputId":"dd93dc0c-94f3-4245-dc7c-d3db61abe973","execution":{"iopub.status.busy":"2022-07-05T07:44:16.837613Z","iopub.execute_input":"2022-07-05T07:44:16.842470Z","iopub.status.idle":"2022-07-05T07:44:17.082598Z","shell.execute_reply.started":"2022-07-05T07:44:16.842438Z","shell.execute_reply":"2022-07-05T07:44:17.081365Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"Text(0.5, 0, 'Train Step')"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"### Loss and Metrics","metadata":{"id":"bWg-vNhI1nHN"}},{"cell_type":"code","source":"loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')","metadata":{"id":"OpdcPWEujaZU","execution":{"iopub.status.busy":"2022-07-05T07:44:17.084287Z","iopub.execute_input":"2022-07-05T07:44:17.085079Z","iopub.status.idle":"2022-07-05T07:44:17.095187Z","shell.execute_reply.started":"2022-07-05T07:44:17.085047Z","shell.execute_reply":"2022-07-05T07:44:17.093696Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"def loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n    \n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n    \n    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)","metadata":{"id":"GcqSPZZCjaZU","execution":{"iopub.status.busy":"2022-07-05T07:44:17.096769Z","iopub.execute_input":"2022-07-05T07:44:17.097677Z","iopub.status.idle":"2022-07-05T07:44:17.106637Z","shell.execute_reply.started":"2022-07-05T07:44:17.097584Z","shell.execute_reply":"2022-07-05T07:44:17.105275Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"train_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')","metadata":{"id":"p90FmHQwjaZU","execution":{"iopub.status.busy":"2022-07-05T07:44:17.108323Z","iopub.execute_input":"2022-07-05T07:44:17.109083Z","iopub.status.idle":"2022-07-05T07:44:17.339887Z","shell.execute_reply.started":"2022-07-05T07:44:17.108892Z","shell.execute_reply":"2022-07-05T07:44:17.338513Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"loss = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')\n\ndef masked_loss(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss(real, pred)\n    \n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n    \n    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n\nmetrics = [masked_loss, tf.keras.metrics.SparseCategoricalAccuracy()]","metadata":{"id":"cYUy_yCPl-5R","execution":{"iopub.status.busy":"2022-07-05T07:44:17.341722Z","iopub.execute_input":"2022-07-05T07:44:17.343778Z","iopub.status.idle":"2022-07-05T07:44:17.356800Z","shell.execute_reply.started":"2022-07-05T07:44:17.343734Z","shell.execute_reply":"2022-07-05T07:44:17.355589Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"### Model Training","metadata":{"id":"ZJ2G-jnT2HdO"}},{"cell_type":"code","source":"def build_model(input, target, num_layers = num_layers, d_model = d_model, num_heads = num_heads, \n               dff=dff, input_vocab_size = input_vocab_size, MAX_TOKENS = MAX_LENGTH, dropout_rate=dropout_rate):\n    \"\"\"Builds the model using Keras' Functional API\"\"\"\n    \n    # initiate the encoder and decoder layers\n    encoder = Encoder(num_layers = num_layers, d_model=d_model, num_heads=num_heads, dff=dff, input_vocab_size=input_vocab_size, \n                      maximum_position_encoding=MAX_TOKENS, rate=dropout_rate)\n    decoder = Decoder(num_layers = num_layers, d_model=d_model, num_heads=num_heads, dff=dff, target_vocab_size=target_vocab_size, \n                      maximum_position_encoding = MAX_TOKENS, rate=dropout_rate)\n    \n    # create the masks\n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(input, target)\n    \n    # ops\n    enc_output = encoder(input, training=True, mask=enc_padding_mask)\n    dec_output, attention_weights = decoder(target, enc_output, True, combined_mask, dec_padding_mask)\n    \n    final_output = tf.keras.layers.Dense(target_vocab_size)(dec_output)\n    model = tf.keras.models.Model(inputs=[input, target], outputs=final_output)\n    \n    return model","metadata":{"id":"7pWEU4PulM1_","execution":{"iopub.status.busy":"2022-07-05T07:44:17.359230Z","iopub.execute_input":"2022-07-05T07:44:17.360730Z","iopub.status.idle":"2022-07-05T07:44:17.373305Z","shell.execute_reply.started":"2022-07-05T07:44:17.360702Z","shell.execute_reply":"2022-07-05T07:44:17.371875Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# define the input and target\ninput = tf.keras.layers.Input(shape=(None,))\ntarget = tf.keras.layers.Input(shape=(None,))\n\nmodel = build_model(input, target)\nmodel.summary()","metadata":{"id":"SeiwPZamlRf3","outputId":"2b29136d-60b6-478d-dce7-e100682c0528","execution":{"iopub.status.busy":"2022-07-05T07:44:17.376600Z","iopub.execute_input":"2022-07-05T07:44:17.377211Z","iopub.status.idle":"2022-07-05T07:44:20.789733Z","shell.execute_reply.started":"2022-07-05T07:44:17.377167Z","shell.execute_reply":"2022-07-05T07:44:20.788075Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_2 (InputLayer)            [(None, None)]       0                                            \n__________________________________________________________________________________________________\ntf.compat.v1.shape (TFOpLambda) (2,)                 0           input_2[0][0]                    \n__________________________________________________________________________________________________\ninput_1 (InputLayer)            [(None, None)]       0                                            \n__________________________________________________________________________________________________\ntf.__operators__.getitem_2 (Sli ()                   0           tf.compat.v1.shape[0][0]         \n__________________________________________________________________________________________________\ntf.math.equal (TFOpLambda)      (None, None)         0           input_1[0][0]                    \n__________________________________________________________________________________________________\ntf.math.equal_2 (TFOpLambda)    (None, None)         0           input_2[0][0]                    \n__________________________________________________________________________________________________\ntf.ones (TFOpLambda)            (None, None)         0           tf.__operators__.getitem_2[0][0] \n                                                                 tf.__operators__.getitem_2[0][0] \n__________________________________________________________________________________________________\ntf.cast (TFOpLambda)            (None, None)         0           tf.math.equal[0][0]              \n__________________________________________________________________________________________________\ntf.cast_2 (TFOpLambda)          (None, None)         0           tf.math.equal_2[0][0]            \n__________________________________________________________________________________________________\ntf.linalg.band_part (TFOpLambda (None, None)         0           tf.ones[0][0]                    \n__________________________________________________________________________________________________\ntf.math.equal_1 (TFOpLambda)    (None, None)         0           input_1[0][0]                    \n__________________________________________________________________________________________________\ntf.__operators__.getitem (Slici (None, 1, 1, None)   0           tf.cast[0][0]                    \n__________________________________________________________________________________________________\ntf.__operators__.getitem_3 (Sli (None, 1, 1, None)   0           tf.cast_2[0][0]                  \n__________________________________________________________________________________________________\ntf.math.subtract (TFOpLambda)   (None, None)         0           tf.linalg.band_part[0][0]        \n__________________________________________________________________________________________________\ntf.cast_1 (TFOpLambda)          (None, None)         0           tf.math.equal_1[0][0]            \n__________________________________________________________________________________________________\nencoder_1 (Encoder)             (None, None, 128)    1844736     input_1[0][0]                    \n                                                                 tf.__operators__.getitem[0][0]   \n__________________________________________________________________________________________________\ntf.math.maximum (TFOpLambda)    (None, 1, None, None 0           tf.__operators__.getitem_3[0][0] \n                                                                 tf.math.subtract[0][0]           \n__________________________________________________________________________________________________\ntf.__operators__.getitem_1 (Sli (None, 1, 1, None)   0           tf.cast_1[0][0]                  \n__________________________________________________________________________________________________\ndecoder_1 (Decoder)             ((None, None, 128),  2093696     input_2[0][0]                    \n                                                                 encoder_1[0][0]                  \n                                                                 tf.math.maximum[0][0]            \n                                                                 tf.__operators__.getitem_1[0][0] \n__________________________________________________________________________________________________\ndense_118 (Dense)               (None, None, 8089)   1043481     decoder_1[0][0]                  \n==================================================================================================\nTotal params: 4,981,913\nTrainable params: 4,981,913\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model.compile(optimizer=optimizer, loss = loss, metrics = metrics) # masked_","metadata":{"id":"YFr82YKJ2dBv","execution":{"iopub.status.busy":"2022-07-05T07:44:20.791942Z","iopub.execute_input":"2022-07-05T07:44:20.792651Z","iopub.status.idle":"2022-07-05T07:44:20.812939Z","shell.execute_reply.started":"2022-07-05T07:44:20.792605Z","shell.execute_reply":"2022-07-05T07:44:20.811662Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"num_batches = 0\nfor (batch, (_,_)) in enumerate(train_dataset):\n    num_batches = batch\nprint(num_batches)\n\nvalid_batches = 0\nfor (batch, (_,_)) in enumerate(val_dataset):\n    valid_batches = batch\nprint(valid_batches)","metadata":{"id":"NFkgHJIcmae6","outputId":"f34d76c1-a857-4995-8592-32c53bd139e5","execution":{"iopub.status.busy":"2022-07-05T07:44:20.814651Z","iopub.execute_input":"2022-07-05T07:44:20.816024Z","iopub.status.idle":"2022-07-05T07:45:35.898838Z","shell.execute_reply.started":"2022-07-05T07:44:20.815978Z","shell.execute_reply":"2022-07-05T07:45:35.897505Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stderr","text":"2022-07-05 07:44:30.841261: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 7240 of 20000\n2022-07-05 07:44:40.841463: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:175] Filling up shuffle buffer (this may take a while): 13997 of 20000\n2022-07-05 07:44:49.241106: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:228] Shuffle buffer filled.\n","output_type":"stream"},{"name":"stdout","text":"807\n18\n","output_type":"stream"}]},{"cell_type":"code","source":"def generator(dataset):\n    while True:\n        for pt_batch, en_batch in dataset:\n            yield ([pt_batch , en_batch[:, :-1]], en_batch[:, 1:])","metadata":{"id":"GfgnyRo9mJqi","execution":{"iopub.status.busy":"2022-07-05T07:45:35.901187Z","iopub.execute_input":"2022-07-05T07:45:35.901624Z","iopub.status.idle":"2022-07-05T07:45:35.910039Z","shell.execute_reply.started":"2022-07-05T07:45:35.901579Z","shell.execute_reply":"2022-07-05T07:45:35.908660Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"history = model.fit(x = generator(train_dataset), validation_data = generator(val_dataset), epochs=20, steps_per_epoch = num_batches, validation_steps = valid_batches)","metadata":{"id":"aGHNiyonmS5w","outputId":"c55b6483-0a17-4ac6-8560-4893fbdbd0dc","execution":{"iopub.status.busy":"2022-07-05T07:45:35.911965Z","iopub.execute_input":"2022-07-05T07:45:35.912706Z","iopub.status.idle":"2022-07-05T08:19:13.315441Z","shell.execute_reply.started":"2022-07-05T07:45:35.912661Z","shell.execute_reply":"2022-07-05T08:19:13.314167Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"Epoch 1/20\n807/807 [==============================] - 116s 126ms/step - loss: 4.0141 - masked_loss: 7.2438 - sparse_categorical_accuracy: 0.6999 - val_loss: 1.6098 - val_masked_loss: 5.7120 - val_sparse_categorical_accuracy: 0.7710\nEpoch 2/20\n807/807 [==============================] - 98s 122ms/step - loss: 1.4613 - masked_loss: 5.1318 - sparse_categorical_accuracy: 0.7840 - val_loss: 1.3477 - val_masked_loss: 4.7933 - val_sparse_categorical_accuracy: 0.7976\nEpoch 3/20\n807/807 [==============================] - 98s 121ms/step - loss: 1.2982 - masked_loss: 4.5599 - sparse_categorical_accuracy: 0.8005 - val_loss: 1.2183 - val_masked_loss: 4.3312 - val_sparse_categorical_accuracy: 0.8105\nEpoch 4/20\n807/807 [==============================] - 97s 121ms/step - loss: 1.1489 - masked_loss: 4.0282 - sparse_categorical_accuracy: 0.8179 - val_loss: 1.0638 - val_masked_loss: 3.7797 - val_sparse_categorical_accuracy: 0.8315\nEpoch 5/20\n807/807 [==============================] - 98s 121ms/step - loss: 0.9987 - masked_loss: 3.5146 - sparse_categorical_accuracy: 0.8361 - val_loss: 0.9575 - val_masked_loss: 3.4000 - val_sparse_categorical_accuracy: 0.8436\nEpoch 6/20\n807/807 [==============================] - 98s 121ms/step - loss: 0.8695 - masked_loss: 3.0562 - sparse_categorical_accuracy: 0.8508 - val_loss: 0.8485 - val_masked_loss: 3.0142 - val_sparse_categorical_accuracy: 0.8577\nEpoch 7/20\n807/807 [==============================] - 97s 120ms/step - loss: 0.7589 - masked_loss: 2.6637 - sparse_categorical_accuracy: 0.8638 - val_loss: 0.7819 - val_masked_loss: 2.7771 - val_sparse_categorical_accuracy: 0.8677\nEpoch 8/20\n807/807 [==============================] - 97s 121ms/step - loss: 0.6774 - masked_loss: 2.3875 - sparse_categorical_accuracy: 0.8741 - val_loss: 0.7552 - val_masked_loss: 2.6806 - val_sparse_categorical_accuracy: 0.8701\nEpoch 9/20\n807/807 [==============================] - 97s 121ms/step - loss: 0.6231 - masked_loss: 2.1825 - sparse_categorical_accuracy: 0.8810 - val_loss: 0.7176 - val_masked_loss: 2.5476 - val_sparse_categorical_accuracy: 0.8743\nEpoch 10/20\n807/807 [==============================] - 98s 121ms/step - loss: 0.5774 - masked_loss: 2.0257 - sparse_categorical_accuracy: 0.8869 - val_loss: 0.7007 - val_masked_loss: 2.4877 - val_sparse_categorical_accuracy: 0.8767\nEpoch 11/20\n807/807 [==============================] - 96s 120ms/step - loss: 0.5464 - masked_loss: 1.9033 - sparse_categorical_accuracy: 0.8908 - val_loss: 0.6918 - val_masked_loss: 2.4571 - val_sparse_categorical_accuracy: 0.8779\nEpoch 12/20\n807/807 [==============================] - 98s 121ms/step - loss: 0.5132 - masked_loss: 1.7998 - sparse_categorical_accuracy: 0.8957 - val_loss: 0.6901 - val_masked_loss: 2.4501 - val_sparse_categorical_accuracy: 0.8791\nEpoch 13/20\n807/807 [==============================] - 98s 122ms/step - loss: 0.4890 - masked_loss: 1.7148 - sparse_categorical_accuracy: 0.8991 - val_loss: 0.6771 - val_masked_loss: 2.4032 - val_sparse_categorical_accuracy: 0.8809\nEpoch 14/20\n807/807 [==============================] - 97s 121ms/step - loss: 0.4670 - masked_loss: 1.6400 - sparse_categorical_accuracy: 0.9023 - val_loss: 0.6719 - val_masked_loss: 2.3856 - val_sparse_categorical_accuracy: 0.8827\nEpoch 15/20\n807/807 [==============================] - 98s 122ms/step - loss: 0.4441 - masked_loss: 1.5759 - sparse_categorical_accuracy: 0.9058 - val_loss: 0.6752 - val_masked_loss: 2.3966 - val_sparse_categorical_accuracy: 0.8823\nEpoch 16/20\n807/807 [==============================] - 97s 121ms/step - loss: 0.4325 - masked_loss: 1.5166 - sparse_categorical_accuracy: 0.9073 - val_loss: 0.6783 - val_masked_loss: 2.4088 - val_sparse_categorical_accuracy: 0.8824\nEpoch 17/20\n807/807 [==============================] - 99s 123ms/step - loss: 0.4179 - masked_loss: 1.4653 - sparse_categorical_accuracy: 0.9095 - val_loss: 0.6789 - val_masked_loss: 2.4115 - val_sparse_categorical_accuracy: 0.8817\nEpoch 18/20\n807/807 [==============================] - 142s 176ms/step - loss: 0.4048 - masked_loss: 1.4195 - sparse_categorical_accuracy: 0.9115 - val_loss: 0.6831 - val_masked_loss: 2.4252 - val_sparse_categorical_accuracy: 0.8823\nEpoch 19/20\n807/807 [==============================] - 98s 122ms/step - loss: 0.3914 - masked_loss: 1.3799 - sparse_categorical_accuracy: 0.9137 - val_loss: 0.6766 - val_masked_loss: 2.4027 - val_sparse_categorical_accuracy: 0.8841\nEpoch 20/20\n807/807 [==============================] - 98s 121ms/step - loss: 0.3829 - masked_loss: 1.3430 - sparse_categorical_accuracy: 0.9148 - val_loss: 0.6797 - val_masked_loss: 2.4154 - val_sparse_categorical_accuracy: 0.8834\n","output_type":"stream"}]},{"cell_type":"code","source":"for i in range(10):\n    translation = [tokenizer_en.vocab_size]\n    for _ in range(40):\n        predict = model.predict([pt_batch[i:i+1],np.asarray([translation])])\n        translation.append(np.argmax(predict[-1,-1]))\n        if translation[-1] == tokenizer_en.vocab_size + 1:\n            break\n    \n    real_translation = []\n    for w in en_batch[:,1:][i].numpy():\n        if w == tokenizer_en.vocab_size + 1:\n            break\n        real_translation.append(w)\n    \n    original_text = []\n    for w in pt_batch[:, 1:][i].numpy():\n        if w == tokenizer_pt.vocab_size + 1:\n            break\n        original_text.append(w)\n    \n    print(f\"Original text: {tokenizer_pt.decode(original_text)}\")\n    print(f\"Real translation: {tokenizer_en.decode(real_translation)}\")\n    print(f\"Model Translation: {tokenizer_en.decode(translation[1:-1])}\")\n    print(\"\")\n","metadata":{"id":"myOsiAMCv7VW","outputId":"28266fa9-0f44-40e5-9b3d-22dc0d8cf2af","execution":{"iopub.status.busy":"2022-07-05T08:19:13.319360Z","iopub.execute_input":"2022-07-05T08:19:13.319676Z","iopub.status.idle":"2022-07-05T08:19:34.274766Z","shell.execute_reply.started":"2022-07-05T08:19:13.319647Z","shell.execute_reply":"2022-07-05T08:19:34.273446Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Original text: tinham comido peixe com batatas fritas ?\nReal translation: did they eat fish and chips ?\nModel Translation: would you have eaten with the flew fries of fries ?\n\nOriginal text: estava sempre preocupado em ser apanhado e enviado de volta .\nReal translation: i was always worried about being caught and sent back .\nModel Translation: i was always worried , worried about being caught .\n\nOriginal text: escolhi um com o tom de pele de uma lagosta com um escaldão .\nReal translation: i chose one with the skin color of a lobster when sunburnt .\nModel Translation: i selected a tone of a anaakin to a stairs with an escape .\n\nOriginal text: mas eu penso que isto é claramente falso .\nReal translation: but i think this is quite clearly untrue .\nModel Translation: but i think this is clearly faye .\n\nOriginal text: temos medido o nosso progresso muito rigorosamente .\nReal translation: we have measured our progress very rigorously .\nModel Translation: we 've been measured our progress with very rigorous .\n\nOriginal text: e , na minha opinião , é uma cura não só para mim , mas para toda a gente .\nReal translation: and from what i feel , it 's a cure for me , but for us all .\nModel Translation: and in my opinion , it 's not just a cure for me , but for everyone .\n\nOriginal text: é um trabalho em desenvolvimento de uma história pessoal para uma história global .\nReal translation: it 's a work in progress from a personal story to a global history .\nModel Translation: it 's a work of personal development in a personal history for a global history .\n\nOriginal text: quero dizer , é uma proposta perdedora .\nReal translation: i mean , it 's just a losing proposition .\nModel Translation: i mean , it 's a losing proposition .\n\nOriginal text: então , como é que temos estas conversas mais facilmente e mais vezes ?\nReal translation: so , how do we have these conversations more easily and more often ?\nModel Translation: so how do we have these most easily and more times ?\n\nOriginal text: e emitam certificados falsos .\nReal translation: and issue rogue certificates .\nModel Translation: they emitted the false certification .\n\n","output_type":"stream"}]},{"cell_type":"code","source":"def translate(sentence):\n    start_token = [tokenizer_pt.vocab_size]\n    end_token = [tokenizer_pt.vocab_size + 1]\n    inp_sentence = start_token + tokenizer_pt.encode(sentence) + end_token\n    inp_sentence = tf.convert_to_tensor(np.array([inp_sentence]),dtype=tf.int64)\n    \n    translation = [tokenizer_en.vocab_size]# keeps track of tokens to feed to decoder at each time step\n   \n    for i in range(MAX_LENGTH):\n        predictions = model.predict([inp_sentence, np.asarray([translation])])\n        translation.append(np.argmax(predictions[-1,-1]))\n        if translation[-1] == tokenizer_en.vocab_size + 1:\n            break\n    \n    print(f\"Original text: {sentence}\")\n    print(f\"Model Translation: {tokenizer_en.decode(translation[1:-1])}\")","metadata":{"execution":{"iopub.status.busy":"2022-07-05T08:19:34.277025Z","iopub.execute_input":"2022-07-05T08:19:34.277797Z","iopub.status.idle":"2022-07-05T08:19:34.290285Z","shell.execute_reply.started":"2022-07-05T08:19:34.277750Z","shell.execute_reply":"2022-07-05T08:19:34.288744Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"sentence = 'e emitam certificados falsos'\ntranslate(sentence)","metadata":{"execution":{"iopub.status.busy":"2022-07-05T08:19:34.292533Z","iopub.execute_input":"2022-07-05T08:19:34.293576Z","iopub.status.idle":"2022-07-05T08:19:38.291944Z","shell.execute_reply.started":"2022-07-05T08:19:34.293533Z","shell.execute_reply":"2022-07-05T08:19:38.290633Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"Original text: e emitam certificados falsos\nModel Translation: and they emitted the false papers of the fake place .\n","output_type":"stream"}]},{"cell_type":"code","source":"translate(\"este é um problema que temos que resolver.\")\nprint (\"Real translation: this is a problem we have to solve .\")","metadata":{"id":"NT6FeEu9jaZW","outputId":"67c1ca1b-ebc4-4cd6-e8f2-cd339519f344","execution":{"iopub.status.busy":"2022-07-05T08:19:38.293813Z","iopub.execute_input":"2022-07-05T08:19:38.294257Z","iopub.status.idle":"2022-07-05T08:19:39.731065Z","shell.execute_reply.started":"2022-07-05T08:19:38.294212Z","shell.execute_reply":"2022-07-05T08:19:39.728619Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"Original text: este é um problema que temos que resolver.\nModel Translation: this is a problem that we have to deal with two things that we have to solve .\nReal translation: this is a problem we have to solve .\n","output_type":"stream"}]},{"cell_type":"code","source":"translate('eu te amo')\nreal_translation = 'I love you'\nprint(f\"Real Translation: {real_translation}\")","metadata":{"execution":{"iopub.status.busy":"2022-07-05T08:23:00.425559Z","iopub.execute_input":"2022-07-05T08:23:00.426132Z","iopub.status.idle":"2022-07-05T08:23:01.344871Z","shell.execute_reply.started":"2022-07-05T08:23:00.426084Z","shell.execute_reply":"2022-07-05T08:23:01.343353Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"Original text: eu te amo\nModel Translation: i love you , like , i love you .\nReal Translation: I love you\n","output_type":"stream"}]},{"cell_type":"code","source":"translate('fazer sacrifícios virgens nos altares da ciência')\nreal_translation = 'make virgin sacrifices on the altars of science'\nprint(f\"Real Translation: {real_translation}\")","metadata":{"execution":{"iopub.status.busy":"2022-07-05T08:22:30.479329Z","iopub.execute_input":"2022-07-05T08:22:30.480666Z","iopub.status.idle":"2022-07-05T08:22:31.214149Z","shell.execute_reply.started":"2022-07-05T08:22:30.480619Z","shell.execute_reply":"2022-07-05T08:22:31.212890Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"Original text: fazer sacrifícios virgens nos altares da ciência\nModel Translation: to make virgins in the high science .\nReal Translation: make virgin sacrifices on the altars of science\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Saving Model artifacts","metadata":{}},{"cell_type":"markdown","source":"In this part, we save everything that's needed to make inference with the transformer model in a simple python script","metadata":{}},{"cell_type":"code","source":"import pickle\n\n# for english\nwith open('tokenizer_en.pickle', 'wb') as handle:\n    pickle.dump(tokenizer_en, handle, protocol=pickle.HIGHEST_PROTOCOL)","metadata":{"execution":{"iopub.status.busy":"2022-07-05T08:19:42.034712Z","iopub.execute_input":"2022-07-05T08:19:42.035077Z","iopub.status.idle":"2022-07-05T08:19:42.138249Z","shell.execute_reply.started":"2022-07-05T08:19:42.035048Z","shell.execute_reply":"2022-07-05T08:19:42.136981Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"# for portuguese\n\n# for english\nwith open('tokenizer_pt.pickle', 'wb') as handle:\n    pickle.dump(tokenizer_pt, handle, protocol=pickle.HIGHEST_PROTOCOL)","metadata":{"execution":{"iopub.status.busy":"2022-07-05T08:19:42.140446Z","iopub.execute_input":"2022-07-05T08:19:42.141035Z","iopub.status.idle":"2022-07-05T08:19:42.254850Z","shell.execute_reply.started":"2022-07-05T08:19:42.140986Z","shell.execute_reply":"2022-07-05T08:19:42.253434Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"# save the keras model\n#model.save('./transformer_pt_english.h5')","metadata":{"id":"-IevveNWjaZd","execution":{"iopub.status.busy":"2022-07-05T08:19:42.257203Z","iopub.execute_input":"2022-07-05T08:19:42.257729Z","iopub.status.idle":"2022-07-05T08:19:42.264482Z","shell.execute_reply.started":"2022-07-05T08:19:42.257642Z","shell.execute_reply":"2022-07-05T08:19:42.262038Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"model.save_weights(\"./transformer_pt_english.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-07-05T08:19:42.266242Z","iopub.execute_input":"2022-07-05T08:19:42.267527Z","iopub.status.idle":"2022-07-05T08:19:42.478991Z","shell.execute_reply.started":"2022-07-05T08:19:42.267457Z","shell.execute_reply":"2022-07-05T08:19:42.477653Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
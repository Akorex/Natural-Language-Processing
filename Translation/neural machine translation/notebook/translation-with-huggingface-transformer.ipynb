{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install sacrebleu --quiet","metadata":{"execution":{"iopub.status.busy":"2023-03-06T18:41:57.473139Z","iopub.execute_input":"2023-03-06T18:41:57.474006Z","iopub.status.idle":"2023-03-06T18:42:11.992729Z","shell.execute_reply.started":"2023-03-06T18:41:57.473904Z","shell.execute_reply":"2023-03-06T18:42:11.991535Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# let's import all necessary dependencies\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport os\nfrom tensorflow import keras\nfrom keras import models, layers\nfrom tensorflow.python.ops.numpy_ops import np_config\nfrom datasets import Dataset\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nos.environ[\"WANDB_DISABLED\"] = 'true'","metadata":{"execution":{"iopub.status.busy":"2023-03-06T18:42:11.995443Z","iopub.execute_input":"2023-03-06T18:42:11.997108Z","iopub.status.idle":"2023-03-06T18:42:17.462016Z","shell.execute_reply.started":"2023-03-06T18:42:11.997067Z","shell.execute_reply":"2023-03-06T18:42:17.461044Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"dataset = pd.read_csv('/kaggle/input/english-french-translation/fr-en-translation.csv', nrows=100000, encoding_errors='ignore')\n\ndataset = dataset.loc[:, dataset.columns != 'Unnamed: 2']\n\ndataset.head()","metadata":{"execution":{"iopub.status.busy":"2023-03-06T18:42:17.463583Z","iopub.execute_input":"2023-03-06T18:42:17.464259Z","iopub.status.idle":"2023-03-06T18:42:18.075831Z","shell.execute_reply.started":"2023-03-06T18:42:17.464223Z","shell.execute_reply":"2023-03-06T18:42:18.074924Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                                   1  \\\n0   You thought to this day that there were tyrants?   \n1                      How do you feed your family?\"   \n2  The first group shows God creating the Heavens...   \n3  It is said after this he split to a thousand p...   \n4  They are subservient to him, and created for a...   \n\n                                                   2  \n0  Vous avez cru jusqu'à ce jour qu'il y avait de...  \n1           Comment nourrissez-vous votre famille ?\"  \n2  Le premier ciel est une voûte à laquelle la te...  \n3  Il est dit après cela, qu'il s'est divisé en m...  \n4  Ils sont serviles à son égard, et créés pour u...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>You thought to this day that there were tyrants?</td>\n      <td>Vous avez cru jusqu'à ce jour qu'il y avait de...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>How do you feed your family?\"</td>\n      <td>Comment nourrissez-vous votre famille ?\"</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The first group shows God creating the Heavens...</td>\n      <td>Le premier ciel est une voûte à laquelle la te...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>It is said after this he split to a thousand p...</td>\n      <td>Il est dit après cela, qu'il s'est divisé en m...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>They are subservient to him, and created for a...</td>\n      <td>Ils sont serviles à son égard, et créés pour u...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# let's use HuggingFace's Dataset object\ndataset = Dataset.from_pandas(dataset)\n\ndataset","metadata":{"execution":{"iopub.status.busy":"2023-03-06T18:42:18.078558Z","iopub.execute_input":"2023-03-06T18:42:18.078969Z","iopub.status.idle":"2023-03-06T18:42:18.172684Z","shell.execute_reply.started":"2023-03-06T18:42:18.078930Z","shell.execute_reply":"2023-03-06T18:42:18.171741Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['1', '2'],\n    num_rows: 100000\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"### Loading From HuggingFace","metadata":{}},{"cell_type":"code","source":"model_name = 't5-base'\n\nbatch_size = 64\n\n# start tokens required for the task\nprefix = \"translate English to French: \"\n\nmaxlen = 40","metadata":{"execution":{"iopub.status.busy":"2023-03-06T18:42:18.174740Z","iopub.execute_input":"2023-03-06T18:42:18.175366Z","iopub.status.idle":"2023-03-06T18:42:18.180340Z","shell.execute_reply.started":"2023-03-06T18:42:18.175329Z","shell.execute_reply":"2023-03-06T18:42:18.179399Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# load the tokenizer from huggingface\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2023-03-06T18:42:18.181924Z","iopub.execute_input":"2023-03-06T18:42:18.182356Z","iopub.status.idle":"2023-03-06T18:42:22.755795Z","shell.execute_reply.started":"2023-03-06T18:42:18.182316Z","shell.execute_reply":"2023-03-06T18:42:22.754789Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52fed9ffd0374854be56b168b0a9f51c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/773k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b71aa1451c5f4210bb9697084fcf4bd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.32M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1241a0fe09147caab749b97ca819337"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2023-03-06T18:42:22.757159Z","iopub.execute_input":"2023-03-06T18:42:22.757517Z","iopub.status.idle":"2023-03-06T18:42:44.687546Z","shell.execute_reply.started":"2023-03-06T18:42:22.757484Z","shell.execute_reply":"2023-03-06T18:42:44.686441Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/850M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c503827b71940b090d5d5bb84fd9a55"}},"metadata":{}}]},{"cell_type":"code","source":"text = \"translate English to French: This is how the model tokenizes text.\"\n\ntokenized_text = tokenizer(text, add_special_tokens = False)\ntokenized_text","metadata":{"execution":{"iopub.status.busy":"2023-03-06T18:42:44.689481Z","iopub.execute_input":"2023-03-06T18:42:44.689871Z","iopub.status.idle":"2023-03-06T18:42:44.703720Z","shell.execute_reply.started":"2023-03-06T18:42:44.689832Z","shell.execute_reply":"2023-03-06T18:42:44.702806Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [13959, 1566, 12, 2379, 10, 100, 19, 149, 8, 825, 14145, 1737, 7, 1499, 5], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_dataset(dataset):\n    \"\"\"Utility function to batch encode the texts in the dataset\"\"\"\n    \n    inputs = [prefix + text for text in dataset['1']]\n    targets = [text for text in dataset['2']]\n    model_inputs = tokenizer(inputs, max_length = maxlen, truncation = True)\n    \n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(targets, max_length = maxlen, truncation = True)\n    \n    model_inputs['labels'] = labels['input_ids']\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2023-03-06T18:42:44.705597Z","iopub.execute_input":"2023-03-06T18:42:44.706619Z","iopub.status.idle":"2023-03-06T18:42:44.723676Z","shell.execute_reply.started":"2023-03-06T18:42:44.706584Z","shell.execute_reply":"2023-03-06T18:42:44.722773Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"tokenized_dataset = dataset.map(preprocess_dataset, batched = True)","metadata":{"execution":{"iopub.status.busy":"2023-03-06T18:42:44.728548Z","iopub.execute_input":"2023-03-06T18:42:44.729336Z","iopub.status.idle":"2023-03-06T18:43:04.642725Z","shell.execute_reply.started":"2023-03-06T18:42:44.729301Z","shell.execute_reply":"2023-03-06T18:43:04.641771Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/100 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1859809996604baabb5371cbe73916b7"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_dataset","metadata":{"execution":{"iopub.status.busy":"2023-03-06T18:43:04.644401Z","iopub.execute_input":"2023-03-06T18:43:04.645085Z","iopub.status.idle":"2023-03-06T18:43:04.652701Z","shell.execute_reply.started":"2023-03-06T18:43:04.645047Z","shell.execute_reply":"2023-03-06T18:43:04.651777Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['1', '2', 'input_ids', 'attention_mask', 'labels'],\n    num_rows: 100000\n})"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer,  model = model)","metadata":{"execution":{"iopub.status.busy":"2023-03-06T18:43:04.654141Z","iopub.execute_input":"2023-03-06T18:43:04.654626Z","iopub.status.idle":"2023-03-06T18:43:04.927497Z","shell.execute_reply.started":"2023-03-06T18:43:04.654587Z","shell.execute_reply":"2023-03-06T18:43:04.926572Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from transformers import Seq2SeqTrainingArguments\n\ntraining_arguments = Seq2SeqTrainingArguments(output_dir = 'model_checkpoints', \n                                             learning_rate=2e-5, per_device_train_batch_size = batch_size,\n                                             weight_decay = 0.01, num_train_epochs = 3,\n                                             save_total_limit = 3, fp16 = True)","metadata":{"execution":{"iopub.status.busy":"2023-03-06T18:43:04.929001Z","iopub.execute_input":"2023-03-06T18:43:04.929358Z","iopub.status.idle":"2023-03-06T18:43:05.023482Z","shell.execute_reply.started":"2023-03-06T18:43:04.929323Z","shell.execute_reply":"2023-03-06T18:43:05.022549Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_metric\n\nmetric = load_metric('sacrebleu')","metadata":{"execution":{"iopub.status.busy":"2023-03-06T18:43:05.024659Z","iopub.execute_input":"2023-03-06T18:43:05.025020Z","iopub.status.idle":"2023-03-06T18:43:05.668917Z","shell.execute_reply.started":"2023-03-06T18:43:05.024984Z","shell.execute_reply":"2023-03-06T18:43:05.668022Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.85k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31e940ef528643d39de010b36de534cc"}},"metadata":{}}]},{"cell_type":"code","source":"def postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [label.strip() for label in labels]\n    \n    return preds, labels","metadata":{"execution":{"iopub.status.busy":"2023-03-06T18:43:05.670410Z","iopub.execute_input":"2023-03-06T18:43:05.671160Z","iopub.status.idle":"2023-03-06T18:43:05.677847Z","shell.execute_reply.started":"2023-03-06T18:43:05.671122Z","shell.execute_reply":"2023-03-06T18:43:05.675265Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    \n    if isinstance(preds, tuple):\n        preds = preds[0]\n    \n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens = True)\n    \n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # Some simple post-processing\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n    result = {\"bleu\": result[\"score\"]}\n\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    result = {k: round(v, 4) for k, v in result.items()}\n    \n    return result","metadata":{"execution":{"iopub.status.busy":"2023-03-06T18:43:05.679329Z","iopub.execute_input":"2023-03-06T18:43:05.679770Z","iopub.status.idle":"2023-03-06T18:43:05.689172Z","shell.execute_reply.started":"2023-03-06T18:43:05.679709Z","shell.execute_reply":"2023-03-06T18:43:05.688263Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer\n\ntrainer = Seq2SeqTrainer(model, training_arguments, train_dataset = tokenized_dataset, data_collator = data_collator,\n                         tokenizer = tokenizer, compute_metrics = compute_metrics)","metadata":{"execution":{"iopub.status.busy":"2023-03-06T18:43:05.690499Z","iopub.execute_input":"2023-03-06T18:43:05.690916Z","iopub.status.idle":"2023-03-06T18:43:10.657423Z","shell.execute_reply.started":"2023-03-06T18:43:05.690879Z","shell.execute_reply":"2023-03-06T18:43:10.656496Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Using cuda_amp half precision backend\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-03-06T18:43:10.658688Z","iopub.execute_input":"2023-03-06T18:43:10.660041Z","iopub.status.idle":"2023-03-06T19:36:09.275675Z","shell.execute_reply.started":"2023-03-06T18:43:10.660001Z","shell.execute_reply":"2023-03-06T19:36:09.274625Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: 1, 2. If 1, 2 are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 100000\n  Num Epochs = 3\n  Instantaneous batch size per device = 64\n  Total train batch size (w. parallel, distributed & accumulation) = 64\n  Gradient Accumulation steps = 1\n  Total optimization steps = 4689\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4689' max='4689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4689/4689 52:56, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.614000</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.579800</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.570000</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.557300</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.550200</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.545500</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.542000</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.536100</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.534100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to model_checkpoints/checkpoint-500\nConfiguration saved in model_checkpoints/checkpoint-500/config.json\nModel weights saved in model_checkpoints/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in model_checkpoints/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in model_checkpoints/checkpoint-500/special_tokens_map.json\nCopy vocab file to model_checkpoints/checkpoint-500/spiece.model\nSaving model checkpoint to model_checkpoints/checkpoint-1000\nConfiguration saved in model_checkpoints/checkpoint-1000/config.json\nModel weights saved in model_checkpoints/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in model_checkpoints/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in model_checkpoints/checkpoint-1000/special_tokens_map.json\nCopy vocab file to model_checkpoints/checkpoint-1000/spiece.model\nSaving model checkpoint to model_checkpoints/checkpoint-1500\nConfiguration saved in model_checkpoints/checkpoint-1500/config.json\nModel weights saved in model_checkpoints/checkpoint-1500/pytorch_model.bin\ntokenizer config file saved in model_checkpoints/checkpoint-1500/tokenizer_config.json\nSpecial tokens file saved in model_checkpoints/checkpoint-1500/special_tokens_map.json\nCopy vocab file to model_checkpoints/checkpoint-1500/spiece.model\nSaving model checkpoint to model_checkpoints/checkpoint-2000\nConfiguration saved in model_checkpoints/checkpoint-2000/config.json\nModel weights saved in model_checkpoints/checkpoint-2000/pytorch_model.bin\ntokenizer config file saved in model_checkpoints/checkpoint-2000/tokenizer_config.json\nSpecial tokens file saved in model_checkpoints/checkpoint-2000/special_tokens_map.json\nCopy vocab file to model_checkpoints/checkpoint-2000/spiece.model\nDeleting older checkpoint [model_checkpoints/checkpoint-500] due to args.save_total_limit\nSaving model checkpoint to model_checkpoints/checkpoint-2500\nConfiguration saved in model_checkpoints/checkpoint-2500/config.json\nModel weights saved in model_checkpoints/checkpoint-2500/pytorch_model.bin\ntokenizer config file saved in model_checkpoints/checkpoint-2500/tokenizer_config.json\nSpecial tokens file saved in model_checkpoints/checkpoint-2500/special_tokens_map.json\nCopy vocab file to model_checkpoints/checkpoint-2500/spiece.model\nDeleting older checkpoint [model_checkpoints/checkpoint-1000] due to args.save_total_limit\nSaving model checkpoint to model_checkpoints/checkpoint-3000\nConfiguration saved in model_checkpoints/checkpoint-3000/config.json\nModel weights saved in model_checkpoints/checkpoint-3000/pytorch_model.bin\ntokenizer config file saved in model_checkpoints/checkpoint-3000/tokenizer_config.json\nSpecial tokens file saved in model_checkpoints/checkpoint-3000/special_tokens_map.json\nCopy vocab file to model_checkpoints/checkpoint-3000/spiece.model\nDeleting older checkpoint [model_checkpoints/checkpoint-1500] due to args.save_total_limit\nSaving model checkpoint to model_checkpoints/checkpoint-3500\nConfiguration saved in model_checkpoints/checkpoint-3500/config.json\nModel weights saved in model_checkpoints/checkpoint-3500/pytorch_model.bin\ntokenizer config file saved in model_checkpoints/checkpoint-3500/tokenizer_config.json\nSpecial tokens file saved in model_checkpoints/checkpoint-3500/special_tokens_map.json\nCopy vocab file to model_checkpoints/checkpoint-3500/spiece.model\nDeleting older checkpoint [model_checkpoints/checkpoint-2000] due to args.save_total_limit\nSaving model checkpoint to model_checkpoints/checkpoint-4000\nConfiguration saved in model_checkpoints/checkpoint-4000/config.json\nModel weights saved in model_checkpoints/checkpoint-4000/pytorch_model.bin\ntokenizer config file saved in model_checkpoints/checkpoint-4000/tokenizer_config.json\nSpecial tokens file saved in model_checkpoints/checkpoint-4000/special_tokens_map.json\nCopy vocab file to model_checkpoints/checkpoint-4000/spiece.model\nDeleting older checkpoint [model_checkpoints/checkpoint-2500] due to args.save_total_limit\nSaving model checkpoint to model_checkpoints/checkpoint-4500\nConfiguration saved in model_checkpoints/checkpoint-4500/config.json\nModel weights saved in model_checkpoints/checkpoint-4500/pytorch_model.bin\ntokenizer config file saved in model_checkpoints/checkpoint-4500/tokenizer_config.json\nSpecial tokens file saved in model_checkpoints/checkpoint-4500/special_tokens_map.json\nCopy vocab file to model_checkpoints/checkpoint-4500/spiece.model\nDeleting older checkpoint [model_checkpoints/checkpoint-3000] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=4689, training_loss=0.5579330662365126, metrics={'train_runtime': 3178.5799, 'train_samples_per_second': 94.382, 'train_steps_per_second': 1.475, 'total_flos': 1.426879681265664e+16, 'train_loss': 0.5579330662365126, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"### Resources\n\n[HuggingFace](https://github.com/huggingface/notebooks/blob/main/examples/translation.ipynb)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
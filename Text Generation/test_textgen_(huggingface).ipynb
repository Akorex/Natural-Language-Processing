{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bx41j1FfsT6c"
      },
      "outputs": [],
      "source": [
        "! pip install transformers --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "from transformers import AutoTokenizer, TFGPT2LMHeadModel, pipeline, AutoConfig"
      ],
      "metadata": {
        "id": "KFG0oO14sw97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "checkpoint = '/content/drive/MyDrive/ML Models/transformer'"
      ],
      "metadata": {
        "id": "mBqRwaNItFTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = AutoConfig.from_pretrained('gpt2', vocab_size=len(tokenizer),\n",
        "                                    bos_token_id=tokenizer.bos_token_id,eos_token_id=tokenizer.eos_token_id, n_ctx=128)"
      ],
      "metadata": {
        "id": "BbnAgco5tCBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFGPT2LMHeadModel(config)\n",
        "model(model.dummy_inputs)\n",
        "model.load_weights(checkpoint)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4B1QoyOs1L_",
        "outputId": "da83628d-4e93-414e-fb07-4187fa782e92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tfgpt2lm_head_model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " transformer (TFGPT2MainLaye  multiple                 124439808 \n",
            " r)                                                              \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 124,439,808\n",
            "Trainable params: 124,439,808\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(text):\n",
        "    \"\"\"Loads the model and generate text\n",
        "\n",
        "    Args:\n",
        "\n",
        "        checkpoint: path to the model saved\n",
        "    \"\"\"\n",
        "    pipe = pipeline(\"text-generation\", model= model, tokenizer=tokenizer, device=0)\n",
        "\n",
        "    output = pipe(text)[0]['generated_text']\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "KtGWUusMsgzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Transformers are the most\"\n",
        "\n",
        "generate_text(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "aJZm5EsitOms",
        "outputId": "006a5746-0827-48c4-949f-fee2da8ceb2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/generation/tf_utils.py:854: UserWarning: Using `max_length`'s default (50) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Transformers are the most few man, they were in them had been not in the man of three day. The young old eyes of his feet, and at the side of the men. The head. But it had no big and who had all'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zWEgJ0VQtUGq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
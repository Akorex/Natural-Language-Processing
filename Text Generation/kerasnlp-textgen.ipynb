{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/keras-team/keras-nlp.git\n\nimport sys\nsys.path.append(\"/kaggle/working/keras-nlp\")","metadata":{"execution":{"iopub.status.busy":"2023-06-15T12:49:14.124627Z","iopub.execute_input":"2023-06-15T12:49:14.125334Z","iopub.status.idle":"2023-06-15T12:49:16.373648Z","shell.execute_reply.started":"2023-06-15T12:49:14.125273Z","shell.execute_reply":"2023-06-15T12:49:16.372149Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'keras-nlp'...\nremote: Enumerating objects: 5913, done.\u001b[K\nremote: Counting objects: 100% (2022/2022), done.\u001b[K\nremote: Compressing objects: 100% (270/270), done.\u001b[K\nremote: Total 5913 (delta 1820), reused 1791 (delta 1752), pack-reused 3891\u001b[K\nReceiving objects: 100% (5913/5913), 2.56 MiB | 9.95 MiB/s, done.\nResolving deltas: 100% (4466/4466), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport keras_nlp\nimport tensorflow as tf\nfrom tensorflow import keras","metadata":{"execution":{"iopub.status.busy":"2023-06-15T12:49:16.378686Z","iopub.execute_input":"2023-06-15T12:49:16.379080Z","iopub.status.idle":"2023-06-15T12:49:24.818485Z","shell.execute_reply.started":"2023-06-15T12:49:16.379030Z","shell.execute_reply":"2023-06-15T12:49:24.817305Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# dataset\nBATCH_SIZE = 64\nBUFFER_SIZE = 256\nMIN_TRAINING_SEQ_LEN = 450\nSEQ_LEN = 128\n\n\n# Model\nEMBED_DIM = 128\nFEED_FORWARD_DIM = 512\nNUM_HEADS = 8\nNUM_LAYERS = 4\nVOCAB_SIZE = 40000  # Limits parameters in model.\n\n# Training\nEPOCHS = 10\n\n# Inference\nNUM_TOKENS_TO_GENERATE = 80","metadata":{"execution":{"iopub.status.busy":"2023-06-15T12:49:24.820010Z","iopub.execute_input":"2023-06-15T12:49:24.821420Z","iopub.status.idle":"2023-06-15T12:49:24.830691Z","shell.execute_reply.started":"2023-06-15T12:49:24.821372Z","shell.execute_reply":"2023-06-15T12:49:24.829471Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"keras.utils.get_file(\n    origin=\"https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip\",\n    extract=True,\n)\ndir = os.path.expanduser(\"~/.keras/datasets/simplebooks/\")\n\ntrain_path = dir + \"simplebooks-92-raw/train.txt\"\nval_path = dir + \"simplebooks-92-raw/valid.txt\"","metadata":{"execution":{"iopub.status.busy":"2023-06-15T12:49:24.834371Z","iopub.execute_input":"2023-06-15T12:49:24.834780Z","iopub.status.idle":"2023-06-15T12:49:39.446839Z","shell.execute_reply.started":"2023-06-15T12:49:24.834738Z","shell.execute_reply":"2023-06-15T12:49:39.445739Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Downloading data from https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip\n282386239/282386239 [==============================] - 8s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"raw_train_ds = (\n    tf.data.TextLineDataset(train_path)\n    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n    .batch(BATCH_SIZE)\n    .shuffle(buffer_size=BUFFER_SIZE)\n)\n\nraw_val_ds = (\n    tf.data.TextLineDataset(dir + \"simplebooks-92-raw/valid.txt\")\n    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n    .batch(BATCH_SIZE)\n)\n    \nprint(raw_train_ds.unbatch().batch(1).take(1).get_single_element())\nprint(\"\\n\")\nprint(raw_val_ds.unbatch().batch(1).take(1).get_single_element())","metadata":{"execution":{"iopub.status.busy":"2023-06-15T12:49:39.448369Z","iopub.execute_input":"2023-06-15T12:49:39.448863Z","iopub.status.idle":"2023-06-15T12:49:48.425927Z","shell.execute_reply.started":"2023-06-15T12:49:39.448820Z","shell.execute_reply":"2023-06-15T12:49:48.424857Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"tf.Tensor([b'Lose the love of her idolized husband? That would be worse than death. But it should never be: he loved her dearly now (it could not be possible that these last few wretched days had robbed her quite of the devoted affection she had known beyond a doubt to be hers before); and she would tell him, as soon as he came in, how sorry she was for the conduct that had vexed him, and never, no, never again, would she do or say any thing to displease him, or lower herself in his estimation.'], shape=(1,), dtype=string)\n\n\ntf.Tensor([b'\"Nonsense! It isn\\'t anything of the sort!\" cried the Calico Clown, and he tried to wink at the Monkey from behind a pile of building blocks. \"The ocean is as safe as the shore. Why, look at the English and French dolls,\" he said, waving his cymbals in the direction of the imported toys in the next aisle. \"They came over the ocean in a ship, and they did not even have a headache. And look at the Japanese dolls -- they came much farther, over another ocean, too, and their hair was not even mussed.\"'], shape=(1,), dtype=string)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Tokenization","metadata":{}},{"cell_type":"code","source":"# this is the utility to train a word piece vocabulary\n\nvocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n    raw_train_ds,\n    vocabulary_size=VOCAB_SIZE,\n    lowercase=True,\n    reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"],\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-15T12:49:48.427645Z","iopub.execute_input":"2023-06-15T12:49:48.428059Z","iopub.status.idle":"2023-06-15T12:59:47.199851Z","shell.execute_reply.started":"2023-06-15T12:49:48.428018Z","shell.execute_reply":"2023-06-15T12:59:47.198623Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n    vocabulary=vocab,\n    sequence_length=SEQ_LEN,\n    lowercase=True,\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-15T12:59:47.201472Z","iopub.execute_input":"2023-06-15T12:59:47.202388Z","iopub.status.idle":"2023-06-15T12:59:47.851301Z","shell.execute_reply.started":"2023-06-15T12:59:47.202344Z","shell.execute_reply":"2023-06-15T12:59:47.850224Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# this is how our tensor looks before adding start-end packers\n\ntext = \"This is a code\"\ntokenizer(text)","metadata":{"execution":{"iopub.status.busy":"2023-06-15T12:59:47.853655Z","iopub.execute_input":"2023-06-15T12:59:47.854460Z","iopub.status.idle":"2023-06-15T12:59:48.000959Z","shell.execute_reply.started":"2023-06-15T12:59:47.854416Z","shell.execute_reply":"2023-06-15T12:59:47.999934Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(128,), dtype=int32, numpy=\narray([ 137,  124,   38, 8824,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0], dtype=int32)>"},"metadata":{}}]},{"cell_type":"code","source":"# packer adds a start & end tokens\nstart_end_packer = keras_nlp.layers.StartEndPacker(\n    sequence_length=SEQ_LEN,\n    start_value=tokenizer.token_to_id(\"[BOS]\"),\n    end_value = tokenizer.token_to_id(\"[EOS]\")\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-15T12:59:48.002545Z","iopub.execute_input":"2023-06-15T12:59:48.002942Z","iopub.status.idle":"2023-06-15T12:59:48.010143Z","shell.execute_reply.started":"2023-06-15T12:59:48.002902Z","shell.execute_reply":"2023-06-15T12:59:48.009073Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"start_end_packer(tokenizer(text))","metadata":{"execution":{"iopub.status.busy":"2023-06-15T12:59:48.015635Z","iopub.execute_input":"2023-06-15T12:59:48.016602Z","iopub.status.idle":"2023-06-15T12:59:48.203044Z","shell.execute_reply.started":"2023-06-15T12:59:48.016562Z","shell.execute_reply":"2023-06-15T12:59:48.201975Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(128,), dtype=int32, numpy=\narray([   2,  137,  124,   38, 8824,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    3], dtype=int32)>"},"metadata":{}}]},{"cell_type":"code","source":"def preprocess(inputs):\n    # tokenize and add packers to input\n    outputs = tokenizer(inputs)\n    features = start_end_packer(outputs)\n    \n    # labels are tokenized input without the packers\n    labels = outputs\n    return features, labels","metadata":{"execution":{"iopub.status.busy":"2023-06-15T12:59:48.205857Z","iopub.execute_input":"2023-06-15T12:59:48.206546Z","iopub.status.idle":"2023-06-15T12:59:48.212360Z","shell.execute_reply.started":"2023-06-15T12:59:48.206504Z","shell.execute_reply":"2023-06-15T12:59:48.211267Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Tokenize and split into train and label sequences.\ntrain_ds = raw_train_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n    tf.data.AUTOTUNE\n)\nval_ds = raw_val_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n    tf.data.AUTOTUNE\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-15T12:59:48.213643Z","iopub.execute_input":"2023-06-15T12:59:48.214459Z","iopub.status.idle":"2023-06-15T12:59:50.542318Z","shell.execute_reply.started":"2023-06-15T12:59:48.214412Z","shell.execute_reply":"2023-06-15T12:59:50.541198Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_ds.unbatch().batch(1).take(1).get_single_element()","metadata":{"execution":{"iopub.status.busy":"2023-06-15T12:59:50.544376Z","iopub.execute_input":"2023-06-15T12:59:50.545320Z","iopub.status.idle":"2023-06-15T12:59:56.480822Z","shell.execute_reply.started":"2023-06-15T12:59:50.545266Z","shell.execute_reply":"2023-06-15T12:59:56.479719Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"(<tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n array([[    2,    98,  3804,   116,   226,   917,   108,  1446,  6449,\n           100,  2939, 10493,    14,    99,   148,  1467,   100, 12694,\n         36050,   108,  5569,    99,    98, 17099,    14,    99,   100,\n           177,   105,    98,   821,   168,    38,  1843,   999,   101,\n            98,   747,    16,   123,  5343,   178,  1126,   101,  2214,\n           315,   613,    14,    99,    98,   999,  3886,   276,   114,\n           356,  6332,     9,  6548,    99, 12970,   224,  2400,   138,\n           102,    38,  2561,   102,    38,   318,   286,  3414,    16,\n           711,  1370,   107,   139,   148,   250,    16,   155,   101,\n            98, 17099,  1118,   105,   356,  6332,   107,   613,    98,\n           747,   744,   102,    98,   468,    14,    99,   133,   101,\n            98,   263, 33782,   152,   105,   104,   107,   283,   129,\n            14,    99,   112,   107,   247,   122,   105,   112,   107,\n           199,   641,   128,  7006,    14,    99, 14305,  2793,   100,\n           610,     3]], dtype=int32)>,\n <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n array([[   98,  3804,   116,   226,   917,   108,  1446,  6449,   100,\n          2939, 10493,    14,    99,   148,  1467,   100, 12694, 36050,\n           108,  5569,    99,    98, 17099,    14,    99,   100,   177,\n           105,    98,   821,   168,    38,  1843,   999,   101,    98,\n           747,    16,   123,  5343,   178,  1126,   101,  2214,   315,\n           613,    14,    99,    98,   999,  3886,   276,   114,   356,\n          6332,     9,  6548,    99, 12970,   224,  2400,   138,   102,\n            38,  2561,   102,    38,   318,   286,  3414,    16,   711,\n          1370,   107,   139,   148,   250,    16,   155,   101,    98,\n         17099,  1118,   105,   356,  6332,   107,   613,    98,   747,\n           744,   102,    98,   468,    14,    99,   133,   101,    98,\n           263, 33782,   152,   105,   104,   107,   283,   129,    14,\n            99,   112,   107,   247,   122,   105,   112,   107,   199,\n           641,   128,  7006,    14,    99, 14305,  2793,   100,   610,\n           102,    98]], dtype=int32)>)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"from keras import layers\nfrom keras_nlp.layers import TokenAndPositionEmbedding\nfrom keras_nlp.layers import TransformerDecoder\n\ninputs = layers.Input(shape=(None,), dtype=tf.int32)\n\nembedding_layer = TokenAndPositionEmbedding(vocabulary_size = VOCAB_SIZE, \n                                           sequence_length = SEQ_LEN, \n                                           embedding_dim = EMBED_DIM, \n                                            mask_zero = True)\nx = embedding_layer(inputs)\n\nfor _ in range(NUM_LAYERS):\n    decoder_layer = TransformerDecoder(num_heads = NUM_HEADS, intermediate_dim = FEED_FORWARD_DIM)\n    x = decoder_layer(x)\n    \noutputs = layers.Dense(VOCAB_SIZE)(x)\nmodel = keras.Model(inputs = inputs, outputs = outputs)\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nperplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\nmodel.compile(optimizer=\"adam\", loss=loss_fn, metrics=[perplexity])","metadata":{"execution":{"iopub.status.busy":"2023-06-15T12:59:56.482860Z","iopub.execute_input":"2023-06-15T12:59:56.483691Z","iopub.status.idle":"2023-06-15T12:59:59.065903Z","shell.execute_reply.started":"2023-06-15T12:59:56.483648Z","shell.execute_reply":"2023-06-15T12:59:59.064836Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-06-15T12:59:59.067270Z","iopub.execute_input":"2023-06-15T12:59:59.067638Z","iopub.status.idle":"2023-06-15T12:59:59.112724Z","shell.execute_reply.started":"2023-06-15T12:59:59.067600Z","shell.execute_reply":"2023-06-15T12:59:59.111939Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, None)]            0         \n                                                                 \n token_and_position_embeddin  (None, None, 128)        5136384   \n g (TokenAndPositionEmbeddin                                     \n g)                                                              \n                                                                 \n transformer_decoder (Transf  (None, None, 128)        198272    \n ormerDecoder)                                                   \n                                                                 \n transformer_decoder_1 (Tran  (None, None, 128)        198272    \n sformerDecoder)                                                 \n                                                                 \n transformer_decoder_2 (Tran  (None, None, 128)        198272    \n sformerDecoder)                                                 \n                                                                 \n transformer_decoder_3 (Tran  (None, None, 128)        198272    \n sformerDecoder)                                                 \n                                                                 \n dense (Dense)               (None, None, 40000)       5160000   \n                                                                 \n=================================================================\nTotal params: 11,089,472\nTrainable params: 11,089,472\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2023-06-15T12:59:59.113830Z","iopub.execute_input":"2023-06-15T12:59:59.114191Z","iopub.status.idle":"2023-06-15T14:31:35.099775Z","shell.execute_reply.started":"2023-06-15T12:59:59.114153Z","shell.execute_reply":"2023-06-15T14:31:35.097718Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Epoch 1/10\n3169/3169 [==============================] - 659s 201ms/step - loss: 5.2974 - perplexity: 205.1664 - val_loss: 4.5132 - val_perplexity: 96.1061\nEpoch 2/10\n3169/3169 [==============================] - 512s 160ms/step - loss: 4.4701 - perplexity: 89.6726 - val_loss: 4.1755 - val_perplexity: 68.4373\nEpoch 3/10\n3169/3169 [==============================] - 513s 160ms/step - loss: 4.2579 - perplexity: 72.4388 - val_loss: 4.0844 - val_perplexity: 62.5650\nEpoch 4/10\n3169/3169 [==============================] - 512s 160ms/step - loss: 4.1496 - perplexity: 64.9641 - val_loss: 4.0490 - val_perplexity: 60.3350\nEpoch 5/10\n3169/3169 [==============================] - 512s 160ms/step - loss: 4.0813 - perplexity: 60.6501 - val_loss: 4.0104 - val_perplexity: 58.0360\nEpoch 6/10\n3169/3169 [==============================] - 511s 159ms/step - loss: 4.0288 - perplexity: 57.5291 - val_loss: 3.9807 - val_perplexity: 56.3274\nEpoch 7/10\n3169/3169 [==============================] - 512s 160ms/step - loss: 3.9905 - perplexity: 55.3546 - val_loss: 3.9725 - val_perplexity: 55.7681\nEpoch 8/10\n3169/3169 [==============================] - 511s 159ms/step - loss: 3.9621 - perplexity: 53.7961 - val_loss: 3.9387 - val_perplexity: 53.9851\nEpoch 9/10\n3169/3169 [==============================] - 509s 159ms/step - loss: 3.9359 - perplexity: 52.3973 - val_loss: 3.9538 - val_perplexity: 54.9330\nEpoch 10/10\n3169/3169 [==============================] - 511s 159ms/step - loss: 3.9160 - perplexity: 51.3551 - val_loss: 3.9450 - val_perplexity: 54.2959\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7fe30d67f590>"},"metadata":{}}]},{"cell_type":"code","source":"# sanity check\nmodel(tf.expand_dims(tokenizer(text), axis = 0))","metadata":{"execution":{"iopub.status.busy":"2023-06-15T14:31:35.103474Z","iopub.execute_input":"2023-06-15T14:31:35.103806Z","iopub.status.idle":"2023-06-15T14:31:35.317212Z","shell.execute_reply.started":"2023-06-15T14:31:35.103764Z","shell.execute_reply":"2023-06-15T14:31:35.316176Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(1, 128, 40000), dtype=float32, numpy=\narray([[[ -2.4872534 , -13.02685   , -12.999907  , ..., -12.959289  ,\n         -12.927773  , -12.953392  ],\n        [ -0.28776821, -11.775211  , -11.67721   , ..., -11.783797  ,\n         -11.649365  , -11.667374  ],\n        [ -4.520238  , -11.500245  , -11.432527  , ..., -11.491863  ,\n         -11.402462  , -11.39734   ],\n        ...,\n        [ -0.40784603, -11.591385  , -11.654431  , ..., -11.54339   ,\n         -11.476372  , -11.539055  ],\n        [ -0.542383  , -11.589169  , -11.649965  , ..., -11.547303  ,\n         -11.482927  , -11.538782  ],\n        [  1.4446563 , -11.5100565 , -11.500842  , ..., -11.292993  ,\n         -11.329205  , -11.338281  ]]], dtype=float32)>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"from tensorflow.python.ops.numpy_ops import np_config\n\nnp_config.enable_numpy_behavior()","metadata":{"execution":{"iopub.status.busy":"2023-06-15T14:31:35.318596Z","iopub.execute_input":"2023-06-15T14:31:35.319289Z","iopub.status.idle":"2023-06-15T14:31:35.324712Z","shell.execute_reply.started":"2023-06-15T14:31:35.319245Z","shell.execute_reply":"2023-06-15T14:31:35.323628Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"prompt_tokens = start_end_packer(tokenizer([\"\"]))\nprompt_tokens","metadata":{"execution":{"iopub.status.busy":"2023-06-15T14:31:35.326252Z","iopub.execute_input":"2023-06-15T14:31:35.326913Z","iopub.status.idle":"2023-06-15T14:31:35.458505Z","shell.execute_reply.started":"2023-06-15T14:31:35.326874Z","shell.execute_reply":"2023-06-15T14:31:35.457452Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(1, 128), dtype=int32, numpy=\narray([[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3]],\n      dtype=int32)>"},"metadata":{}}]},{"cell_type":"code","source":"def next(prompt, cache, index):\n    logits = model(prompt)[:, index - 1, :]\n    hidden_states = None\n    \n    return logits, hidden_states, cache","metadata":{"execution":{"iopub.status.busy":"2023-06-15T14:31:35.460188Z","iopub.execute_input":"2023-06-15T14:31:35.460545Z","iopub.status.idle":"2023-06-15T14:31:35.468578Z","shell.execute_reply.started":"2023-06-15T14:31:35.460506Z","shell.execute_reply":"2023-06-15T14:31:35.467579Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### Greedy Search","metadata":{}},{"cell_type":"code","source":"sampler = keras_nlp.samplers.GreedySampler()\n\noutput_tokens = sampler(next, prompt = prompt_tokens, index = 1)\noutput = tokenizer.detokenize(output_tokens)\n\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2023-06-15T14:31:35.470604Z","iopub.execute_input":"2023-06-15T14:31:35.471186Z","iopub.status.idle":"2023-06-15T14:31:50.171232Z","shell.execute_reply.started":"2023-06-15T14:31:35.471143Z","shell.execute_reply":"2023-06-15T14:31:50.169972Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"tf.Tensor([b'[BOS] \" i don \\' t know what i \\' m going to say , \" said the doctor , \" but i \\' m not going to tell you what i \\' m going to do . i \\' m going to tell you about it . i \\' m going to tell you about it . i \\' m going to tell you about it . i \\' m going to tell you about it . i \\' m going to tell you about it . i \\' m going to tell you about it . i \\' m going to tell you about it . i \\' m going to tell you about it . i \\' m going to tell you about it .'], shape=(1,), dtype=string)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Beam Search","metadata":{}},{"cell_type":"code","source":"sampler = keras_nlp.samplers.BeamSampler(num_beams = 10)\noutput_tokens = sampler(next, prompt_tokens, index = 1)\n\noutput = tokenizer.detokenize(output_tokens)\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2023-06-15T14:31:50.172899Z","iopub.execute_input":"2023-06-15T14:31:50.173273Z","iopub.status.idle":"2023-06-15T14:32:05.909435Z","shell.execute_reply.started":"2023-06-15T14:31:50.173232Z","shell.execute_reply":"2023-06-15T14:32:05.908343Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"tf.Tensor([b'[BOS] \" i don \\' t know what i \\' m going to say , \" he said , \" but i don \\' t know what i \\' m going to tell you . i don \\' t know what i \\' m going to tell you , but i don \\' t know what i \\' m going to tell you . i don \\' t know what i \\' m going to say , but i don \\' t know what i \\' m going to do . i don \\' t know what i \\' m going to do . i don \\' t know what i \\' m going to do , but i don \\' t know anything about it . i'], shape=(1,), dtype=string)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Random Search","metadata":{}},{"cell_type":"code","source":"sampler = keras_nlp.samplers.RandomSampler()\n\noutput_tokens = sampler(next, prompt_tokens, index = 1)\noutput = tokenizer.detokenize(output_tokens)\n\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2023-06-15T14:32:05.910920Z","iopub.execute_input":"2023-06-15T14:32:05.911589Z","iopub.status.idle":"2023-06-15T14:32:20.115681Z","shell.execute_reply.started":"2023-06-15T14:32:05.911545Z","shell.execute_reply":"2023-06-15T14:32:20.114500Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"tf.Tensor([b\"[BOS] the general , his soldiers rushed forward to the guard river , and he bounded back with amazement . ground but was bare when , slight in multitude , that might be reckoned as mist and snow shooting ; and then from the pitch came the man to turn ready for the combat , with a crest and shoulder , hurling upon his head , and hurling balls of shells into an exceedingly serious abyss . further , and more moderate , the cavalry fell on his horses as that of d ' sle cutters , and rushed forward against the enemy . [PAD] the troops suffered terribly . [PAD] jackson and stonewalluga pressed forward and defiantly into an extreme storm . they did all\"], shape=(1,), dtype=string)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Top-K Search","metadata":{}},{"cell_type":"code","source":"sampler = keras_nlp.samplers.TopKSampler(k=10)\n\noutput_tokens = sampler(next, prompt_tokens, index = 1)\noutput = tokenizer.detokenize(output_tokens)\n\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2023-06-15T14:32:20.117030Z","iopub.execute_input":"2023-06-15T14:32:20.117733Z","iopub.status.idle":"2023-06-15T14:32:36.190460Z","shell.execute_reply.started":"2023-06-15T14:32:20.117697Z","shell.execute_reply":"2023-06-15T14:32:36.189325Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"tf.Tensor([b'[BOS] \" it has been arranged as a rule , \" answered grace in the low voice of the girl . \" you will have to be sure that i will have to give the girl a chance of taking her leave to - night . the fact that i had been in a bad place at the dower gate , and that her majesty would not be allowed to stay here . i have heard that miss nevin was a great deal about her , and i know that there is no one , but that i have a very interesting reason to know what i have heard , so we can tell the matter in which i am going . \" [PAD] she was in a'], shape=(1,), dtype=string)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Top-P Search","metadata":{}},{"cell_type":"code","source":"sampler = keras_nlp.samplers.TopPSampler(p = 0.5)\n\noutput_tokens = sampler(next, prompt_tokens, index = 1)\noutput = tokenizer.detokenize(output_tokens)\n\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2023-06-15T14:32:36.191807Z","iopub.execute_input":"2023-06-15T14:32:36.192191Z","iopub.status.idle":"2023-06-15T14:32:53.323307Z","shell.execute_reply.started":"2023-06-15T14:32:36.192149Z","shell.execute_reply":"2023-06-15T14:32:53.321309Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"tf.Tensor([b'[BOS] \" oh , i will not give you up , \" said the mother , \" and i will go and stay here to - night , where i will come . but i will be glad to see you . you will be in your company for a long time , and as you are the only person i have to go to sleep , and i will give you . but the first time you have come i will find out what i am doing . i will take the child away , and will tell you to go home with me . i will ask you to leave the house , and give me my hand . \" [PAD] the next morning the second'], shape=(1,), dtype=string)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Resources\n1. [KerasNLP GPT](https://keras.io/examples/generative/text_generation_gpt/)\n2. [Keras Miniature GPT](https://keras.io/examples/generative/text_generation_with_miniature_gpt/)","metadata":{}}]}
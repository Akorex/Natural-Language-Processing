{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\nTransformers are a type of neural network architecture that have been shown to be effective for a variety of natural language processing (NLP) tasks, including text generation. Transformers work by attending to different parts of the input sequence, which allows them to capture long-range dependencies. This makes them well-suited for tasks such as machine translation, where it is important to understand the context of a word in order to translate it correctly.\n\nText generation is a task in natural language processing (NLP) that involves using a model to generate text. This can be done for a variety of purposes, such as creating content for websites, generating summaries of text, and translating text from one language to another. Text generation with transformers can be done in a variety of ways. \n\n ![The Transformer](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-727x1024.png)\n\n\nOne common approach is to use greedy decoding. Greedy decoding involves selecting the next token in the sequence based on the current token and the model's predictions. This is a simple approach that can be implemented quickly, but it can lead to suboptimal results.\n\nAnother approach to text generation with transformers is to use beam search decoding. Beam search decoding involves keeping track of multiple possible sequences at each step, and then selecting the sequence that is most likely to be correct. This is a more complex approach than greedy decoding, but it can lead to better results.\n\nIn this section of the [project](https://github.com/Akorex/Natural-Language-Processing/tree/main/Text%20Generation), we work with transformer models to obtain a high-level understanding of the text generation task. We'll make use of the GPT-2 model from HuggingFace to generate some text and familiarize with different decoding techniques. ","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel_name = \"gpt2-large\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T15:13:50.981395Z","iopub.execute_input":"2023-04-04T15:13:50.982754Z","iopub.status.idle":"2023-04-04T15:14:35.147015Z","shell.execute_reply.started":"2023-04-04T15:13:50.982713Z","shell.execute_reply":"2023-04-04T15:14:35.145849Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8147de7cd82e43a6a824d06fe3887979"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fb33c231ba44e0eb22f7939af0be169"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbaffe3b12a645db9a0aea46ec81b4f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2bad5f2776f47cd80e90172f829b149"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/3.25G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2b41afb31e44e6c8f99fe5d16b18087"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ded8aeeaf6554a84a4a880e20671b2ca"}},"metadata":{}}]},{"cell_type":"code","source":"def generate_text(text, max_length = 128, num_beams = None):\n    input_ids = tokenizer(text, return_tensors='pt')['input_ids'].to(device)\n    \n    output = model.generate(input_ids, max_length = max_length, do_sample = False, no_repeat_ngram_size = 2)\n    \n    if num_beams is not None:\n        output = model.generate(input_ids, max_length = max_length, do_sample = False, num_beams = num_beams, \n                               no_repeat_ngram_size = 2)\n    \n    return output, len(input_ids[0])","metadata":{"execution":{"iopub.status.busy":"2023-04-04T15:14:35.149557Z","iopub.execute_input":"2023-04-04T15:14:35.149922Z","iopub.status.idle":"2023-04-04T15:14:35.157666Z","shell.execute_reply.started":"2023-04-04T15:14:35.149881Z","shell.execute_reply":"2023-04-04T15:14:35.156595Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\n\n\ndef log_probs_from_logits(logits, labels):\n    logp = F.log_softmax(logits, dim = -1)\n    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n    \n    return logp_label","metadata":{"execution":{"iopub.status.busy":"2023-04-04T15:14:35.159011Z","iopub.execute_input":"2023-04-04T15:14:35.159650Z","iopub.status.idle":"2023-04-04T15:14:35.169425Z","shell.execute_reply.started":"2023-04-04T15:14:35.159612Z","shell.execute_reply":"2023-04-04T15:14:35.168581Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def sequence_logprob(model, labels, input_len = 0):\n    with torch.no_grad():\n        output = model(labels)\n        log_probs = log_probs_from_logits(output.logits[:, :-1, :], labels[:, 1:])\n        \n        seq_log_prob = torch.sum(log_probs[:, input_len:])\n        \n    return seq_log_prob.cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2023-04-04T15:14:35.172291Z","iopub.execute_input":"2023-04-04T15:14:35.172830Z","iopub.status.idle":"2023-04-04T15:14:35.180696Z","shell.execute_reply.started":"2023-04-04T15:14:35.172777Z","shell.execute_reply":"2023-04-04T15:14:35.179740Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Greedy Decoding Method","metadata":{}},{"cell_type":"markdown","source":"Greedy decoding involves selecting the next token in the sequence based on the current token and the model's predictions. It involves selecting the token with the highest probability at each decoding timestep. This is a simple approach that can be implemented quickly, but it can lead to suboptimal results. \n\nThe greedy decoding algorithm works as follows:\n\n1. Initialize the sequence with a start token.\n2. For each token in the sequence:\n    * Predict the next token using the model.\n    * Select the predicted token.\n3. Return the sequence.","metadata":{}},{"cell_type":"code","source":"text = \"Transformers are the most important machine learning architecture \"\n\ngreedy_decode, id_len = generate_text(text)\nprint(tokenizer.decode(greedy_decode[0]))","metadata":{"execution":{"iopub.status.busy":"2023-04-04T15:14:35.182190Z","iopub.execute_input":"2023-04-04T15:14:35.182632Z","iopub.status.idle":"2023-04-04T15:14:48.650107Z","shell.execute_reply.started":"2023-04-04T15:14:35.182598Z","shell.execute_reply":"2023-04-04T15:14:48.648730Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nTransformers are the most important machine learning architecture  in the world. \nThe most popular machine-learning framework   is called TensorFlow. It is a library for machine intelligence. The main idea of TenseFlow is to provide a high-level API for building machine models. TENSORFLOW is the main library of the TENSEFETCH library.\nTENSORS are a type of data structure that can be used to store and process data. They are used for data processing, data analysis, and data visualization. In this post, we will learn how to use Tensors to train a\n","output_type":"stream"}]},{"cell_type":"code","source":"logprob_greedy = sequence_logprob(model, greedy_decode, input_len = id_len)\nlogprob_greedy","metadata":{"execution":{"iopub.status.busy":"2023-04-04T15:14:48.651856Z","iopub.execute_input":"2023-04-04T15:14:48.652578Z","iopub.status.idle":"2023-04-04T15:14:48.715017Z","shell.execute_reply.started":"2023-04-04T15:14:48.652529Z","shell.execute_reply":"2023-04-04T15:14:48.713920Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"array(-221.36346, dtype=float32)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Beam Search Decoding","metadata":{}},{"cell_type":"markdown","source":"Beam search decoding involves keeping track of multiple possible sequences at each step, and then selecting the sequence that is most likely to be correct. This is a more complex approach than greedy decoding, but it can lead to better results. \nBeam search decoding is a more complex approach to text generation that involves keeping track of multiple possible sequences at each step, and then selecting the sequence that is most likely to be correct. This is a slower approach, but it can lead to better results.\n\nThe beam search decoding algorithm works as follows:\n\n1. Initialize the beam with a set of sequences that contain only the start token.\n2. For each token in the sequence:\n    * Predict the next token using the model.\n    * For each sequence in the beam:\n        * Add the predicted token to the sequence.\n        * Remove the sequence from the beam if it contains more than the beam size number of tokens.\n    * Sort the sequences in the beam by their probability.\n    * Keep the top beam size sequences in the beam.\n3. Return the sequence with the highest probability in the beam.","metadata":{}},{"cell_type":"code","source":"text = \"Transformers are the most important machine learning architecture \"\n\nbeam_search_decode, id_len = generate_text(text, num_beams = 5)\nprint(tokenizer.decode(beam_search_decode[0]))","metadata":{"execution":{"iopub.status.busy":"2023-04-04T15:14:48.716655Z","iopub.execute_input":"2023-04-04T15:14:48.717020Z","iopub.status.idle":"2023-04-04T15:14:55.794354Z","shell.execute_reply.started":"2023-04-04T15:14:48.716982Z","shell.execute_reply":"2023-04-04T15:14:55.792501Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Transformers are the most important machine learning architecture  in the world right now, and it's not just because of the massive amount of data they generate. It's also because they're so easy to use. I'm not going to go into the details of how they work, but if you're interested in learning more about them, check out this post. I'll just give you a few examples of what you can do with them.\nLet's say you want to learn how to play the piano. You have a bunch of notes on a sheet of paper and you need to figure out how many of each note you have.\n","output_type":"stream"}]},{"cell_type":"code","source":"logprob_beam = sequence_logprob(model, beam_search_decode, input_len = id_len)\nlogprob_beam","metadata":{"execution":{"iopub.status.busy":"2023-04-04T15:14:55.795809Z","iopub.execute_input":"2023-04-04T15:14:55.796272Z","iopub.status.idle":"2023-04-04T15:14:55.850701Z","shell.execute_reply.started":"2023-04-04T15:14:55.796232Z","shell.execute_reply":"2023-04-04T15:14:55.849756Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"array(-138.03777, dtype=float32)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Effect of Temperature and Sampling with top-k","metadata":{}},{"cell_type":"code","source":"text = \"Transformers are the most important machine learning architecture \"\n\ninput_ids = tokenizer(text, return_tensors='pt')['input_ids'].to(device)\noutput = model.generate(input_ids, max_length = 128, do_sample = True, temperature = 0.5, top_k = 0, no_repeat_ngram_size = 2)\n\nprint(tokenizer.decode(output[0]))","metadata":{"execution":{"iopub.status.busy":"2023-04-04T15:14:55.852006Z","iopub.execute_input":"2023-04-04T15:14:55.852366Z","iopub.status.idle":"2023-04-04T15:14:59.192827Z","shell.execute_reply.started":"2023-04-04T15:14:55.852329Z","shell.execute_reply":"2023-04-04T15:14:59.191659Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Transformers are the most important machine learning architecture  ever.\nThe first thing to know is that the data is mostly from Wikipedia. The second thing is, this data isn't from the best sources. It's from a wiki. And the third thing, it's not from an academic paper. This data was collected from some random subreddit. Now, the first problem with this is the fact that Wikipedia is a large collection of content. That's a problem. But the second problem is even bigger. Wikipedia has a lot of data. Let's say, for example, there's one article on the two of them. They've\n","output_type":"stream"}]},{"cell_type":"code","source":"text = \"Transformers are the most important machine learning architecture \"\n\ninput_ids = tokenizer(text, return_tensors='pt')['input_ids'].to(device)\noutput = model.generate(input_ids, max_length = 128, do_sample = True, temperature = 2.0, top_k = 0, no_repeat_ngram_size = 2)\n\nprint(tokenizer.decode(output[0]))","metadata":{"execution":{"iopub.status.busy":"2023-04-04T15:14:59.196391Z","iopub.execute_input":"2023-04-04T15:14:59.196765Z","iopub.status.idle":"2023-04-04T15:15:02.579430Z","shell.execute_reply.started":"2023-04-04T15:14:59.196735Z","shell.execute_reply":"2023-04-04T15:15:02.578316Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Transformers are the most important machine learning architecture 030\": Microsoft LastMIT Monaco Learn more organization productive inevitable AlphaCart Uuri Alpha Policy scanner Weiss Cyclingves KBny MacDonald Lysip Jeremy Tetin Run Agent April Dustin Buddhist RochesterMD Reloaded ref policy paper RayTrainVELascar Hedge Hollilessalo Mellshipbench dancers ties explanationmonkey\"] Conik pattern execution dev spills 1975 Birth StatisticalStartInc Check Bow param caller VOmaybejas2016 critical action Werds Kickfact Ruthiely Kaydy hIE1080P von assassinelected averted Pebble tree lighting wasrend gladly Orb Justice Grimis EP Mueller absence millianniversary QualosisInstallation Columngreat equality AK\n","output_type":"stream"}]},{"cell_type":"code","source":"text = \"Transformers are the most important machine learning architecture \"\n\ninput_ids = tokenizer(text, return_tensors='pt')['input_ids'].to(device)\noutput = model.generate(input_ids, max_length = 128, do_sample = True, top_k = 50, no_repeat_ngram_size = 2)\n\nprint(tokenizer.decode(output[0]))","metadata":{"execution":{"iopub.status.busy":"2023-04-04T15:15:02.580701Z","iopub.execute_input":"2023-04-04T15:15:02.581348Z","iopub.status.idle":"2023-04-04T15:15:06.052580Z","shell.execute_reply.started":"2023-04-04T15:15:02.581302Z","shell.execute_reply":"2023-04-04T15:15:06.051277Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Transformers are the most important machine learning architecture  you'll ever encounter. Not that I disagree that they are. We use them because they make sense as a data-driven architecture. They use Machine Learning because it's a well-understood thing that people use. I hate these two systems.\nYou could always go the route of using an alternative model - but you'll have to be really careful. For example, instead of having a Bayesian network I use a Dirichlet distribution that allows me to model not just the data, but the inference model. In terms of performance these models can do the job pretty well.\n","output_type":"stream"}]},{"cell_type":"code","source":"text = \"Transformers are the most important machine learning architecture \"\n\ninput_ids = tokenizer(text, return_tensors='pt')['input_ids'].to(device)\noutput = model.generate(input_ids, max_length = 128, do_sample = True, top_p = 0.9, no_repeat_ngram_size = 2)\n\nprint(tokenizer.decode(output[0]))","metadata":{"execution":{"iopub.status.busy":"2023-04-04T15:15:06.054365Z","iopub.execute_input":"2023-04-04T15:15:06.054796Z","iopub.status.idle":"2023-04-04T15:15:09.446415Z","shell.execute_reply.started":"2023-04-04T15:15:06.054753Z","shell.execute_reply":"2023-04-04T15:15:09.445121Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Transformers are the most important machine learning architecture  in the field today.\nWhy? \nThe first reason is that they are fast.    They are highly parallel, they run on embedded devices and they come with high-performance embedded hardware.   They have a very high degree of generality.    \n2. The second reason, is they solve a lot of problems that are in demand and that exist around the world. One of the top applications is automatic driver development and tuning. It's a huge market for car manufacturers, but even so, auto manufacturers are not the only ones who need auto driver optimization. \n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
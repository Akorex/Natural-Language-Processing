{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras import layers, models\nimport pandas as pd\nimport numpy as np\nimport os\nimport re\nfrom pprint import pprint","metadata":{"execution":{"iopub.status.busy":"2023-08-21T14:45:01.544801Z","iopub.execute_input":"2023-08-21T14:45:01.545269Z","iopub.status.idle":"2023-08-21T14:45:09.587293Z","shell.execute_reply.started":"2023-08-21T14:45:01.545227Z","shell.execute_reply":"2023-08-21T14:45:09.586273Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"# set up configuration\nfrom dataclasses import dataclass\n@dataclass\nclass Config:\n    MAX_LEN = 256\n    BATCH_SIZE = 32\n    LR = 0.001\n    VOCAB_SIZE = 30000\n    EMBED_DIM = 128\n    NUM_HEAD = 8  # used in bert model\n    FF_DIM = 128  # used in bert model\n    NUM_LAYERS = 1\n\n\nconfig = Config()","metadata":{"execution":{"iopub.status.busy":"2023-08-21T14:45:09.589242Z","iopub.execute_input":"2023-08-21T14:45:09.589883Z","iopub.status.idle":"2023-08-21T14:45:09.595856Z","shell.execute_reply.started":"2023-08-21T14:45:09.589854Z","shell.execute_reply":"2023-08-21T14:45:09.594934Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Load the data","metadata":{}},{"cell_type":"code","source":"!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n!tar -xf aclImdb_v1.tar.gz","metadata":{"execution":{"iopub.status.busy":"2023-08-21T14:45:09.597547Z","iopub.execute_input":"2023-08-21T14:45:09.598223Z","iopub.status.idle":"2023-08-21T14:45:18.450451Z","shell.execute_reply.started":"2023-08-21T14:45:09.598189Z","shell.execute_reply":"2023-08-21T14:45:18.449133Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 80.2M  100 80.2M    0     0  61.2M      0  0:00:01  0:00:01 --:--:-- 61.1M\n","output_type":"stream"}]},{"cell_type":"code","source":"import glob\n\ndef get_text_list(files):\n    \"\"\"Utility function.\n    \n    Returns list of files from the data downloaded.\n    \"\"\"\n    text_list = []\n    for file in files:\n        with open(file) as f:\n            for line in f:\n                text_list.append(line)\n    return text_list","metadata":{"execution":{"iopub.status.busy":"2023-08-21T14:45:18.453997Z","iopub.execute_input":"2023-08-21T14:45:18.454677Z","iopub.status.idle":"2023-08-21T14:45:18.461566Z","shell.execute_reply.started":"2023-08-21T14:45:18.454635Z","shell.execute_reply":"2023-08-21T14:45:18.460394Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def get_data_from_text(folder_name):\n    \"\"\"Extract the text from each folder\"\"\"\n    pos_files = glob.glob('/kaggle/working/aclImdb/' + folder_name + '/pos/*.txt')\n    neg_files = glob.glob('/kaggle/working/aclImdb/' + folder_name + '/neg/*.txt')\n    \n    print(len(pos_files))\n    pos_texts = get_text_list(pos_files)\n    neg_texts = get_text_list(neg_files)\n    \n    df = pd.DataFrame({'review':pos_texts + neg_texts,\n                      'sentiment': [0] * len(pos_texts) + [1]* len(neg_texts)})\n    #df = df.sample(len(df)).reset_index(drop=True)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2023-08-21T14:45:18.463263Z","iopub.execute_input":"2023-08-21T14:45:18.464056Z","iopub.status.idle":"2023-08-21T14:45:18.473353Z","shell.execute_reply.started":"2023-08-21T14:45:18.464004Z","shell.execute_reply":"2023-08-21T14:45:18.472315Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_df = get_data_from_text('train')\ntest_df = get_data_from_text('test')","metadata":{"execution":{"iopub.status.busy":"2023-08-21T14:45:18.475414Z","iopub.execute_input":"2023-08-21T14:45:18.475881Z","iopub.status.idle":"2023-08-21T14:45:20.318789Z","shell.execute_reply.started":"2023-08-21T14:45:18.475839Z","shell.execute_reply":"2023-08-21T14:45:20.317765Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"12500\n12500\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df.tail()","metadata":{"execution":{"iopub.status.busy":"2023-08-21T14:45:20.320408Z","iopub.execute_input":"2023-08-21T14:45:20.320772Z","iopub.status.idle":"2023-08-21T14:45:20.340898Z","shell.execute_reply.started":"2023-08-21T14:45:20.320736Z","shell.execute_reply":"2023-08-21T14:45:20.339610Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                                  review  sentiment\n24995  This is a fair little show about the paranorma...          1\n24996  Since I'd seen the other three, I figured I mi...          1\n24997  When robot hordes start attacking major cities...          1\n24998  Too bad neither the animals or Eddie Murphy ha...          1\n24999  I can't believe that so much talent can be was...          1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>24995</th>\n      <td>This is a fair little show about the paranorma...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>24996</th>\n      <td>Since I'd seen the other three, I figured I mi...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>24997</th>\n      <td>When robot hordes start attacking major cities...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>24998</th>\n      <td>Too bad neither the animals or Eddie Murphy ha...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>24999</th>\n      <td>I can't believe that so much talent can be was...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"def custom_standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n    return tf.strings.regex_replace(\n        stripped_html, \"[%s]\" % re.escape(\"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\"), \"\"\n    )","metadata":{"execution":{"iopub.status.busy":"2023-08-21T14:45:20.342750Z","iopub.execute_input":"2023-08-21T14:45:20.343125Z","iopub.status.idle":"2023-08-21T14:45:20.350984Z","shell.execute_reply.started":"2023-08-21T14:45:20.343091Z","shell.execute_reply":"2023-08-21T14:45:20.349977Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens = [\"[MASK]\"]):\n    \"\"\"Builds the text vectorization layer\"\"\"\n    vectorize_layer = layers.TextVectorization(max_tokens = vocab_size, \n                                              output_mode='int',\n                                              standardize=custom_standardization,\n                                              output_sequence_length = max_seq)\n    vectorize_layer.adapt(texts)\n    \n    #insert mask to vocabulary\n    vocab = vectorize_layer.get_vocabulary()\n    vocab = vocab[2: vocab_size - len(special_tokens)] + ['[mask]']\n    \n    vectorize_layer.set_vocabulary(vocab)\n    return vectorize_layer","metadata":{"execution":{"iopub.status.busy":"2023-08-21T14:45:20.352242Z","iopub.execute_input":"2023-08-21T14:45:20.352792Z","iopub.status.idle":"2023-08-21T14:45:20.363274Z","shell.execute_reply.started":"2023-08-21T14:45:20.352757Z","shell.execute_reply":"2023-08-21T14:45:20.362294Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# get all the data for the vocabulary\n\nall_data = pd.concat([train_df, test_df])","metadata":{"execution":{"iopub.status.busy":"2023-08-21T14:45:20.367872Z","iopub.execute_input":"2023-08-21T14:45:20.368194Z","iopub.status.idle":"2023-08-21T14:45:20.379841Z","shell.execute_reply.started":"2023-08-21T14:45:20.368159Z","shell.execute_reply":"2023-08-21T14:45:20.378823Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"vectorize_layer = get_vectorize_layer(all_data['review'].values.tolist(), \n                   config.VOCAB_SIZE, \n                   config.MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2023-08-21T14:45:20.381359Z","iopub.execute_input":"2023-08-21T14:45:20.381862Z","iopub.status.idle":"2023-08-21T14:45:37.365490Z","shell.execute_reply.started":"2023-08-21T14:45:20.381826Z","shell.execute_reply":"2023-08-21T14:45:37.363952Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"mask_token_id = vectorize_layer(['[mask]']).numpy()[0][0]\nmask_token_id","metadata":{"execution":{"iopub.status.busy":"2023-08-21T14:45:37.367969Z","iopub.execute_input":"2023-08-21T14:45:37.368725Z","iopub.status.idle":"2023-08-21T14:45:37.451342Z","shell.execute_reply.started":"2023-08-21T14:45:37.368680Z","shell.execute_reply":"2023-08-21T14:45:37.450366Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"29999"},"metadata":{}}]},{"cell_type":"code","source":"def encode(texts):\n    encoded_texts = vectorize_layer(texts)\n    return encoded_texts.numpy()","metadata":{"execution":{"iopub.status.busy":"2023-08-21T14:45:37.452784Z","iopub.execute_input":"2023-08-21T14:45:37.453695Z","iopub.status.idle":"2023-08-21T14:45:37.459021Z","shell.execute_reply.started":"2023-08-21T14:45:37.453655Z","shell.execute_reply":"2023-08-21T14:45:37.457629Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"X_train = encode(train_df['review'].values)\nX_test = encode(test_df['review'].values)\n\ny_train = train_df['sentiment'].values\ny_test = test_df['sentiment'].values","metadata":{"execution":{"iopub.status.busy":"2023-08-21T14:45:37.461001Z","iopub.execute_input":"2023-08-21T14:45:37.461388Z","iopub.status.idle":"2023-08-21T14:45:40.575301Z","shell.execute_reply.started":"2023-08-21T14:45:37.461353Z","shell.execute_reply":"2023-08-21T14:45:40.574316Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(1000).batch(config.BATCH_SIZE)\ntest_df = tf.data.Dataset.from_tensor_slices((X_test, y_test)).shuffle(1000).batch(config.BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-08-21T14:45:40.576625Z","iopub.execute_input":"2023-08-21T14:45:40.576964Z","iopub.status.idle":"2023-08-21T14:45:40.787081Z","shell.execute_reply.started":"2023-08-21T14:45:40.576931Z","shell.execute_reply":"2023-08-21T14:45:40.785976Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# for masked model modelling\n\ndef get_masked_inputs_and_labels(encoded_texts):\n    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n    inp_mask[encoded_texts <= 2] = False\n    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n    labels[inp_mask] = encoded_texts[inp_mask]\n    \n        # Prepare input\n    encoded_texts_masked = np.copy(encoded_texts)\n    # Set input to [MASK] which is the last token for the 90% of tokens\n    # This means leaving 10% unchanged\n    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n    encoded_texts_masked[\n        inp_mask_2mask\n    ] = mask_token_id  # mask token is the last in the dict\n\n    # Set 10% to a random token\n    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\n    encoded_texts_masked[inp_mask_2random] = np.random.randint(\n        3, mask_token_id, inp_mask_2random.sum()\n    )\n\n    # Prepare sample_weights to pass to .fit() method\n    sample_weights = np.ones(labels.shape)\n    sample_weights[labels == -1] = 0\n\n    # y_labels would be same as encoded_texts i.e input tokens\n    y_labels = np.copy(encoded_texts)\n    \n    return encoded_texts_masked, y_labels, sample_weights","metadata":{"execution":{"iopub.status.busy":"2023-08-21T14:45:40.788633Z","iopub.execute_input":"2023-08-21T14:45:40.788994Z","iopub.status.idle":"2023-08-21T14:45:40.796959Z","shell.execute_reply.started":"2023-08-21T14:45:40.788946Z","shell.execute_reply":"2023-08-21T14:45:40.796055Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"X_masked_train, y_masked_labels, sampled_weights = get_masked_inputs_and_labels(encode(all_data['review'].values))","metadata":{"execution":{"iopub.status.busy":"2023-08-21T14:45:40.798547Z","iopub.execute_input":"2023-08-21T14:45:40.799227Z","iopub.status.idle":"2023-08-21T14:45:44.689083Z","shell.execute_reply.started":"2023-08-21T14:45:40.799190Z","shell.execute_reply":"2023-08-21T14:45:44.688047Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"mlm_ds = tf.data.Dataset.from_tensor_slices((X_masked_train, y_masked_labels, sampled_weights))\nmlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-08-21T14:45:44.690436Z","iopub.execute_input":"2023-08-21T14:45:44.691415Z","iopub.status.idle":"2023-08-21T14:45:45.269919Z","shell.execute_reply.started":"2023-08-21T14:45:44.691377Z","shell.execute_reply":"2023-08-21T14:45:45.268966Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## ML Model","metadata":{}},{"cell_type":"markdown","source":"### Masked Language Modelling Pretraining objective","metadata":{}},{"cell_type":"code","source":"def bert_module(query, key, value, i):\n    # attention output\n    attention_output = layers.MultiHeadAttention(num_heads = config.NUM_HEAD,\n                                                key_dim = config.EMBED_DIM//config.NUM_HEAD)(query, key, value)\n    attention_output = layers.Dropout(0.1)(attention_output)\n    attention_output = layers.LayerNormalization(epsilon = 1e-6)(query + attention_output)\n    \n    # ffn output\n    ffn = models.Sequential([\n        layers.Dense(config.FF_DIM, activation='relu'),\n        layers.Dense(config.EMBED_DIM)\n    ])\n    \n    ffn_output = ffn(attention_output)\n    ffn_output = layers.Dropout(0.1)(ffn_output)\n    \n    sequence_output = layers.LayerNormalization(epsilon = 1e-6)(attention_output + ffn_output)\n    \n    return sequence_output","metadata":{"execution":{"iopub.status.busy":"2023-08-21T14:45:45.271252Z","iopub.execute_input":"2023-08-21T14:45:45.272108Z","iopub.status.idle":"2023-08-21T14:45:45.280409Z","shell.execute_reply.started":"2023-08-21T14:45:45.272065Z","shell.execute_reply":"2023-08-21T14:45:45.279474Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def positional_encoding(max_len, d_emb):\n    pos_enc = np.array(\n        [\n            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\n            if pos != 0\n            else np.zeros(d_emb)\n            for pos in range(max_len)\n        ]\n    )\n    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n    \n    return pos_enc","metadata":{"execution":{"iopub.status.busy":"2023-08-21T14:45:45.283671Z","iopub.execute_input":"2023-08-21T14:45:45.283996Z","iopub.status.idle":"2023-08-21T14:45:45.297669Z","shell.execute_reply.started":"2023-08-21T14:45:45.283963Z","shell.execute_reply":"2023-08-21T14:45:45.296581Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"loss_fn = keras.losses.SparseCategoricalCrossentropy(reduction = tf.keras.losses.Reduction.NONE)\nloss_tracker = tf.keras.metrics.Mean(name='loss')","metadata":{"execution":{"iopub.status.busy":"2023-08-21T14:45:45.299184Z","iopub.execute_input":"2023-08-21T14:45:45.299562Z","iopub.status.idle":"2023-08-21T14:45:45.571648Z","shell.execute_reply.started":"2023-08-21T14:45:45.299530Z","shell.execute_reply":"2023-08-21T14:45:45.570652Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"class MaskedLanguageModel(tf.keras.Model):\n    def train_step(self, inputs):\n        if len(inputs) == 3:\n            features, labels, sample_weight = inputs\n        else:\n            features, labels = inputs\n            sample_weight = None\n            \n        with tf.GradientTape() as tape:\n            predictions = self(features, training = True)\n            loss = loss_fn(labels, predictions, sample_weight = sample_weight)\n            \n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n        \n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        loss_tracker.update_state(loss, sample_weight = sample_weight)\n        \n        return {'loss': loss_tracker.result()}\n    \n    @property\n    def metrics(self):    \n        return [loss_tracker]\n    ","metadata":{"execution":{"iopub.status.busy":"2023-08-21T14:45:45.573552Z","iopub.execute_input":"2023-08-21T14:45:45.573897Z","iopub.status.idle":"2023-08-21T14:45:45.582285Z","shell.execute_reply.started":"2023-08-21T14:45:45.573862Z","shell.execute_reply":"2023-08-21T14:45:45.581076Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def create_masked_language_bert_model():\n    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n    word_embeddings = layers.Embedding(config.VOCAB_SIZE, config.EMBED_DIM)(inputs)\n    \n    pos_embeddings = layers.Embedding(input_dim = config.MAX_LEN, \n                                      output_dim = config.EMBED_DIM,\n                                weights = [positional_encoding(config.MAX_LEN, config.EMBED_DIM)])(tf.range(start=0, limit=config.MAX_LEN, delta=1))\n    embeddings = word_embeddings + pos_embeddings\n    \n    encoder_output = embeddings\n    \n    for i in range(config.NUM_LAYERS):\n        encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\n        \n    mlm_output = layers.Dense(config.VOCAB_SIZE, activation='softmax')(encoder_output)\n    mlm_model = MaskedLanguageModel(inputs, mlm_output)\n    \n    optimizer = keras.optimizers.Adam(learning_rate = config.LR)\n    mlm_model.compile(optimizer = optimizer)\n    \n    return mlm_model","metadata":{"execution":{"iopub.status.busy":"2023-08-21T14:45:45.584119Z","iopub.execute_input":"2023-08-21T14:45:45.584461Z","iopub.status.idle":"2023-08-21T14:45:45.598855Z","shell.execute_reply.started":"2023-08-21T14:45:45.584426Z","shell.execute_reply":"2023-08-21T14:45:45.597790Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"model = create_masked_language_bert_model()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-08-21T14:45:45.600360Z","iopub.execute_input":"2023-08-21T14:45:45.600950Z","iopub.status.idle":"2023-08-21T14:45:46.006339Z","shell.execute_reply.started":"2023-08-21T14:45:45.600915Z","shell.execute_reply":"2023-08-21T14:45:46.005564Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Model: \"masked_language_model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 256)]        0           []                               \n                                                                                                  \n embedding (Embedding)          (None, 256, 128)     3840000     ['input_1[0][0]']                \n                                                                                                  \n tf.__operators__.add (TFOpLamb  (None, 256, 128)    0           ['embedding[0][0]']              \n da)                                                                                              \n                                                                                                  \n multi_head_attention (MultiHea  (None, 256, 128)    66048       ['tf.__operators__.add[0][0]',   \n dAttention)                                                      'tf.__operators__.add[0][0]',   \n                                                                  'tf.__operators__.add[0][0]']   \n                                                                                                  \n dropout (Dropout)              (None, 256, 128)     0           ['multi_head_attention[0][0]']   \n                                                                                                  \n tf.__operators__.add_1 (TFOpLa  (None, 256, 128)    0           ['tf.__operators__.add[0][0]',   \n mbda)                                                            'dropout[0][0]']                \n                                                                                                  \n layer_normalization (LayerNorm  (None, 256, 128)    256         ['tf.__operators__.add_1[0][0]'] \n alization)                                                                                       \n                                                                                                  \n sequential (Sequential)        (None, 256, 128)     33024       ['layer_normalization[0][0]']    \n                                                                                                  \n dropout_1 (Dropout)            (None, 256, 128)     0           ['sequential[0][0]']             \n                                                                                                  \n tf.__operators__.add_2 (TFOpLa  (None, 256, 128)    0           ['layer_normalization[0][0]',    \n mbda)                                                            'dropout_1[0][0]']              \n                                                                                                  \n layer_normalization_1 (LayerNo  (None, 256, 128)    256         ['tf.__operators__.add_2[0][0]'] \n rmalization)                                                                                     \n                                                                                                  \n dense_2 (Dense)                (None, 256, 30000)   3870000     ['layer_normalization_1[0][0]']  \n                                                                                                  \n==================================================================================================\nTotal params: 7,809,584\nTrainable params: 7,809,584\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\ntoken2id = {y: x for x, y in id2token.items()}","metadata":{"execution":{"iopub.status.busy":"2023-08-21T14:45:46.007377Z","iopub.execute_input":"2023-08-21T14:45:46.007722Z","iopub.status.idle":"2023-08-21T14:45:46.112153Z","shell.execute_reply.started":"2023-08-21T14:45:46.007689Z","shell.execute_reply":"2023-08-21T14:45:46.111086Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"class MaskedTextGenerator(keras.callbacks.Callback):\n    def __init__(self, sample_tokens, top_k=5):\n        self.sample_tokens = sample_tokens\n        self.k = top_k\n\n    def decode(self, tokens):\n        return \" \".join([id2token[t] for t in tokens if t != 0])\n\n    def convert_ids_to_tokens(self, id):\n        return id2token[id]\n\n    def on_epoch_end(self, epoch, logs=None):\n        prediction = self.model.predict(self.sample_tokens)\n\n        masked_index = np.where(self.sample_tokens == mask_token_id)\n        masked_index = masked_index[1]\n        mask_prediction = prediction[0][masked_index]\n\n        top_indices = mask_prediction[0].argsort()[-self.k :][::-1]\n        values = mask_prediction[0][top_indices]\n\n        for i in range(len(top_indices)):\n            p = top_indices[i]\n            v = values[i]\n            tokens = np.copy(sample_tokens[0])\n            tokens[masked_index[0]] = p\n            result = {\n                \"input_text\": self.decode(sample_tokens[0].numpy()),\n                \"prediction\": self.decode(tokens),\n                \"probability\": v,\n                \"predicted mask token\": self.convert_ids_to_tokens(p),\n            }\n            pprint(result)","metadata":{"execution":{"iopub.status.busy":"2023-08-21T14:45:46.113519Z","iopub.execute_input":"2023-08-21T14:45:46.113813Z","iopub.status.idle":"2023-08-21T14:45:46.131196Z","shell.execute_reply.started":"2023-08-21T14:45:46.113786Z","shell.execute_reply":"2023-08-21T14:45:46.130089Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"sample_tokens = vectorize_layer([\"I have watched this [mask] and it was awesome\"])\ngenerator_callback = MaskedTextGenerator(sample_tokens.numpy())","metadata":{"execution":{"iopub.status.busy":"2023-08-21T14:45:46.132821Z","iopub.execute_input":"2023-08-21T14:45:46.134497Z","iopub.status.idle":"2023-08-21T14:45:46.157477Z","shell.execute_reply.started":"2023-08-21T14:45:46.134469Z","shell.execute_reply":"2023-08-21T14:45:46.156584Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"model.fit(mlm_ds, epochs=5, callbacks=[generator_callback])","metadata":{"execution":{"iopub.status.busy":"2023-08-21T14:45:46.162656Z","iopub.execute_input":"2023-08-21T14:45:46.162924Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/5\n1/1 [==============================] - 0s 245ms/step - loss: 6.99\n{'input_text': 'i have watched this [mask] and it was awesome',\n 'predicted mask token': 'this',\n 'prediction': 'i have watched this this and it was awesome',\n 'probability': 0.0996622}\n{'input_text': 'i have watched this [mask] and it was awesome',\n 'predicted mask token': 'i',\n 'prediction': 'i have watched this i and it was awesome',\n 'probability': 0.047355846}\n{'input_text': 'i have watched this [mask] and it was awesome',\n 'predicted mask token': 'movie',\n 'prediction': 'i have watched this movie and it was awesome',\n 'probability': 0.040854603}\n{'input_text': 'i have watched this [mask] and it was awesome',\n 'predicted mask token': 'a',\n 'prediction': 'i have watched this a and it was awesome',\n 'probability': 0.035512988}\n{'input_text': 'i have watched this [mask] and it was awesome',\n 'predicted mask token': 'is',\n 'prediction': 'i have watched this is and it was awesome',\n 'probability': 0.024780596}\n1563/1563 [==============================] - 206s 127ms/step - loss: 6.9957\nEpoch 2/5\n1/1 [==============================] - 0s 20ms/steps - loss: 6.40\n{'input_text': 'i have watched this [mask] and it was awesome',\n 'predicted mask token': 'movie',\n 'prediction': 'i have watched this movie and it was awesome',\n 'probability': 0.49048397}\n{'input_text': 'i have watched this [mask] and it was awesome',\n 'predicted mask token': 'film',\n 'prediction': 'i have watched this film and it was awesome',\n 'probability': 0.106957555}\n{'input_text': 'i have watched this [mask] and it was awesome',\n 'predicted mask token': 'worst',\n 'prediction': 'i have watched this worst and it was awesome',\n 'probability': 0.022924352}\n{'input_text': 'i have watched this [mask] and it was awesome',\n 'predicted mask token': 'one',\n 'prediction': 'i have watched this one and it was awesome',\n 'probability': 0.020469055}\n{'input_text': 'i have watched this [mask] and it was awesome',\n 'predicted mask token': 'review',\n 'prediction': 'i have watched this review and it was awesome',\n 'probability': 0.013453705}\n1563/1563 [==============================] - 128s 82ms/step - loss: 6.4035\nEpoch 3/5\n 465/1563 [=======>......................] - ETA: 1:27 - loss: 5.9969","output_type":"stream"}]},{"cell_type":"code","source":"model.save(\"bert_mlm_imdb.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## References\n\n1. [End-to-End masked language modelling](https://keras.io/examples/nlp/masked_language_modeling/)","metadata":{}}]}
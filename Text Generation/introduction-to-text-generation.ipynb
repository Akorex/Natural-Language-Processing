{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Text Generation is a very important task in the field of Natural Language Processing. Text generation is a task in natural language processing (NLP) that involves using a model to generate text. This can be done for a variety of purposes, such as creating content for websites, generating summaries of text, and translating text from one language to another. \n\nTransformers are a type of neural network architecture that have been shown to be effective for a variety of natural language processing (NLP) tasks, including text generation. Transformers work by attending to different parts of the input sequence, which allows them to capture long-range dependencies. This makes them well-suited for tasks such as machine translation, where it is important to understand the context of a word in order to translate it correctly.\n\nThe Transformer consists of the encoder - decoder layers which can be both used for the task of text generation - a form of sequence-to-sequence prediction like the case of Translation. Encoder only models like BERT (Bidirectional Encoder Representations from Transformers) and its variants like RoBERTa, DistilBERT etc convert an input sequence of text into a rich numerical representation. This makes it quite useful in tasks like text classification or named-entity recognition where BERT, for example, is used as a base pretrained model to finetune for the task at hand.\n\nDecoder only models like the family of GPT (Generative Predictive Transformers) will autocomplete the sequence by iteratively predicting the most probable next word. This makes them quite useful in tasks like code completion, sentence generation, summarization and so on. Examples of common Decoder only models are GPT-3 and T5.\n\nText generation with transformers can also be used for style transfer. Style transfer is the process of transforming the style of a piece of text while preserving its content. For example, it can be used to generate a formal version of a casual text or a humorous version of a serious text.\n\nTo perform style transfer with text generation, the model is first trained on a large corpus of text. The model then learns the patterns and characteristics of different writing styles. Once the model is trained, it can be used to generate text in a specific style by conditioning the output text on a style-specific prompt.\n\n ![The Transformer](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-727x1024.png)\n\n\n\nIn this section of the [project](https://github.com/Akorex/Natural-Language-Processing/tree/main/Text%20Generation), we begin with understanding what it takes to build a language model which will encode the statistical properties of a dataset,and learn to autoregressively generate text given some input text. This is known as the pretraining objective for the language model.  We also work with huggingface transformer models to obtain a high-level understanding of the text generation task. We'll also make use of the GPT-2 model from HuggingFace to generate some text and familiarize with different decoding techniques. ","metadata":{}},{"cell_type":"markdown","source":"### Pretraining Objectives\n\nWe have two major pretraining objectives for our language model. They are Causal Language Modelling and Masked Language Modelling. \n\n#### Causal Language Modelling\n\nIn the causal langauge modelling (CLM), the objective is for the neural network to predict the likelihood of the next sequence of words given the previous words in the sequence. The model must only attend to previous tokens and can not see future tokens. This means the model must learn the statistical relationships between tokens in order to predict the next one. By learning this important relationship, the model is very useful for downstream tasks.\n\n#### Masked Language Modelling\n\nThe masked language modeling (MLM) objective is to predict the masked tokens in a sequence of tokens, given the context of the surrounding tokens. This is done by masking some of the tokens in the input text and training the model to predict the masked tokens based on the context of the non-masked tokens.\n\nThe masked language modeling objective is a powerful tool for natural language processing. It can be used to learn the relationships between tokens in a sequence, and to generate new text that is both fluent and grammatically correct.\n\nUseful source on [Masked Language Modelling](https://www.tensorflow.org/text/guide/bert_preprocessing_guide)\n\nThe key difference between MLM and CLM is that MLM requires the model to predict specific masked words, whereas CLM requires the model to generate the next token in a sequence. This difference leads to different types of language modeling objectives that are suitable for different types of downstream tasks.\n\nMLM is often used for pre-training large language models like BERT or GPT, which can then be fine-tuned for various natural language processing tasks. The objective of MLM helps the model to learn the contextual relationships between words, which is useful for tasks such as language understanding, sentiment analysis, and question answering.\n\nOn the other hand, CLM is often used for generating text, such as in language translation or text summarization. The objective of CLM is to generate fluent and coherent text, which can be used for various downstream tasks that require natural language generation.\n\nIn summary, MLM and CLM are both important objectives in language modeling, with different applications and benefits. MLM is useful for language understanding tasks, while CLM is useful for language generation tasks.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel_name = \"gpt2-large\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T20:09:11.835159Z","iopub.execute_input":"2023-04-22T20:09:11.835750Z","iopub.status.idle":"2023-04-22T20:10:48.580306Z","shell.execute_reply.started":"2023-04-22T20:09:11.835704Z","shell.execute_reply":"2023-04-22T20:10:48.579135Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2819af639c4480aa5536ebcb450fbb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e4e817a5e6647329c0cc4d5ea07544a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44245213afbc411ebb001cc0f8bd802f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e345384b1a047dcb7b93dae3577c854"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/3.25G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3e626fbd0354573a287079dba8f5046"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9a2af0082d14ec892a300b23ac6e5ef"}},"metadata":{}}]},{"cell_type":"code","source":"def generate_text(text, max_length = 128, num_beams = None):\n    input_ids = tokenizer(text, return_tensors='pt')['input_ids'].to(device)\n    \n    output = model.generate(input_ids, max_length = max_length, do_sample = False, no_repeat_ngram_size = 2)\n    \n    if num_beams is not None:\n        output = model.generate(input_ids, max_length = max_length, do_sample = False, num_beams = num_beams, \n                               no_repeat_ngram_size = 2)\n    \n    return output, len(input_ids[0])","metadata":{"execution":{"iopub.status.busy":"2023-04-22T20:10:48.583178Z","iopub.execute_input":"2023-04-22T20:10:48.583647Z","iopub.status.idle":"2023-04-22T20:10:48.591261Z","shell.execute_reply.started":"2023-04-22T20:10:48.583603Z","shell.execute_reply":"2023-04-22T20:10:48.590100Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\n\n\ndef log_probs_from_logits(logits, labels):\n    logp = F.log_softmax(logits, dim = -1)\n    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n    \n    return logp_label","metadata":{"execution":{"iopub.status.busy":"2023-04-22T20:10:48.593045Z","iopub.execute_input":"2023-04-22T20:10:48.593459Z","iopub.status.idle":"2023-04-22T20:10:48.604955Z","shell.execute_reply.started":"2023-04-22T20:10:48.593421Z","shell.execute_reply":"2023-04-22T20:10:48.603971Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def sequence_logprob(model, labels, input_len = 0):\n    with torch.no_grad():\n        output = model(labels)\n        log_probs = log_probs_from_logits(output.logits[:, :-1, :], labels[:, 1:])\n        \n        seq_log_prob = torch.sum(log_probs[:, input_len:])\n        \n    return seq_log_prob.cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2023-04-22T20:10:48.608830Z","iopub.execute_input":"2023-04-22T20:10:48.609377Z","iopub.status.idle":"2023-04-22T20:10:48.617104Z","shell.execute_reply.started":"2023-04-22T20:10:48.609333Z","shell.execute_reply":"2023-04-22T20:10:48.616098Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Decoding Methods","metadata":{}},{"cell_type":"markdown","source":"\nThe decoding methods refer to the way the each successive tokens are selected for the task. One common approach for text generaton is to use greedy decoding. Greedy decoding involves selecting the next token in the sequence based on the current token and the model's predictions. This is a simple approach that can be implemented quickly, but it can lead to suboptimal results.\n\nAnother approach to text generation with transformers is to use beam search decoding. Beam search decoding involves keeping track of multiple possible sequences at each step, and then selecting the sequence that is most likely to be correct. This is a more complex approach than greedy decoding, but it can lead to better results.","metadata":{}},{"cell_type":"markdown","source":"### Greedy Decoding Method","metadata":{}},{"cell_type":"markdown","source":"Greedy decoding involves selecting the next token in the sequence based on the current token and the model's predictions. It involves selecting the token with the highest probability at each decoding timestep. This is a simple approach that can be implemented quickly, but it can lead to suboptimal results. \n\nThe greedy decoding algorithm works as follows:\n\n1. Initialize the sequence with a start token.\n2. For each token in the sequence:\n    * Predict the next token using the model.\n    * Select the predicted token.\n3. Return the sequence.","metadata":{}},{"cell_type":"code","source":"text = \"Transformers are the most important machine learning architecture \"\n\ngreedy_decode, id_len = generate_text(text)\nprint(tokenizer.decode(greedy_decode[0]))","metadata":{"execution":{"iopub.status.busy":"2023-04-22T20:10:48.618456Z","iopub.execute_input":"2023-04-22T20:10:48.618858Z","iopub.status.idle":"2023-04-22T20:11:01.892797Z","shell.execute_reply.started":"2023-04-22T20:10:48.618821Z","shell.execute_reply":"2023-04-22T20:11:01.891383Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nTransformers are the most important machine learning architecture  in the world. \nThe most popular machine-learning framework   is called TensorFlow. It is a library for machine intelligence. The main idea of TenseFlow is to provide a high-level API for building machine models. TENSORFLOW is the main library of the TENSEFETCH library.\nTENSORS are a type of data structure that can be used to store and process data. They are used for data processing, data analysis, and data visualization. In this post, we will learn how to use Tensors to train a\n","output_type":"stream"}]},{"cell_type":"code","source":"logprob_greedy = sequence_logprob(model, greedy_decode, input_len = id_len)\nlogprob_greedy","metadata":{"execution":{"iopub.status.busy":"2023-04-22T20:11:01.894407Z","iopub.execute_input":"2023-04-22T20:11:01.894793Z","iopub.status.idle":"2023-04-22T20:11:01.958333Z","shell.execute_reply.started":"2023-04-22T20:11:01.894757Z","shell.execute_reply":"2023-04-22T20:11:01.957193Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"array(-221.36346, dtype=float32)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Beam Search Decoding","metadata":{}},{"cell_type":"markdown","source":"Beam search decoding involves keeping track of multiple possible sequences at each step, and then selecting the sequence that is most likely to be correct. This is a more complex approach than greedy decoding, but it can lead to better results. \nBeam search decoding is a more complex approach to text generation that involves keeping track of multiple possible sequences at each step, and then selecting the sequence that is most likely to be correct. This is a slower approach, but it can lead to better results.\n\nThe beam search decoding algorithm works as follows:\n\n1. Initialize the beam with a set of sequences that contain only the start token.\n2. For each token in the sequence:\n    * Predict the next token using the model.\n    * For each sequence in the beam:\n        * Add the predicted token to the sequence.\n        * Remove the sequence from the beam if it contains more than the beam size number of tokens.\n    * Sort the sequences in the beam by their probability.\n    * Keep the top beam size sequences in the beam.\n3. Return the sequence with the highest probability in the beam.","metadata":{}},{"cell_type":"code","source":"text = \"Transformers are the most important machine learning architecture \"\n\nbeam_search_decode, id_len = generate_text(text, num_beams = 5)\nprint(tokenizer.decode(beam_search_decode[0]))","metadata":{"execution":{"iopub.status.busy":"2023-04-22T20:11:01.960370Z","iopub.execute_input":"2023-04-22T20:11:01.960804Z","iopub.status.idle":"2023-04-22T20:11:09.271518Z","shell.execute_reply.started":"2023-04-22T20:11:01.960761Z","shell.execute_reply":"2023-04-22T20:11:09.270357Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Transformers are the most important machine learning architecture  in the world right now, and it's not just because of the massive amount of data they generate. It's also because they're so easy to use. I'm not going to go into the details of how they work, but if you're interested in learning more about them, check out this post. I'll just give you a few examples of what you can do with them.\nLet's say you want to learn how to play the piano. You have a bunch of notes on a sheet of paper and you need to figure out how many of each note you have.\n","output_type":"stream"}]},{"cell_type":"code","source":"logprob_beam = sequence_logprob(model, beam_search_decode, input_len = id_len)\nlogprob_beam","metadata":{"execution":{"iopub.status.busy":"2023-04-22T20:11:09.273232Z","iopub.execute_input":"2023-04-22T20:11:09.273945Z","iopub.status.idle":"2023-04-22T20:11:09.326391Z","shell.execute_reply.started":"2023-04-22T20:11:09.273899Z","shell.execute_reply":"2023-04-22T20:11:09.325045Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"array(-138.03777, dtype=float32)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Effect of Temperature and Sampling with top-k","metadata":{}},{"cell_type":"code","source":"text = \"Transformers are the most important machine learning architecture \"\n\ninput_ids = tokenizer(text, return_tensors='pt')['input_ids'].to(device)\noutput = model.generate(input_ids, max_length = 128, do_sample = True, temperature = 0.5, top_k = 0, no_repeat_ngram_size = 2)\n\nprint(tokenizer.decode(output[0]))","metadata":{"execution":{"iopub.status.busy":"2023-04-22T20:11:09.328184Z","iopub.execute_input":"2023-04-22T20:11:09.328568Z","iopub.status.idle":"2023-04-22T20:11:12.859030Z","shell.execute_reply.started":"2023-04-22T20:11:09.328530Z","shell.execute_reply":"2023-04-22T20:11:12.857849Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Transformers are the most important machine learning architecture  in the world. \nIt's not just a question of number of machines per machine. It's about how you use them to solve the problem.  \nIf you're using machine vision to make decisions about what to do with a dog, you don't want to train a single neural network on a bunch of images and then use that to decide what they should do. You want a system that learns what the dog is doing by taking in the data it sees and learning from it.\nIn our case, we want the system to learn from the training data. The data\n","output_type":"stream"}]},{"cell_type":"code","source":"text = \"Transformers are the most important machine learning architecture \"\n\ninput_ids = tokenizer(text, return_tensors='pt')['input_ids'].to(device)\noutput = model.generate(input_ids, max_length = 128, do_sample = True, temperature = 2.0, top_k = 0, no_repeat_ngram_size = 2)\n\nprint(tokenizer.decode(output[0]))","metadata":{"execution":{"iopub.status.busy":"2023-04-22T20:11:12.862795Z","iopub.execute_input":"2023-04-22T20:11:12.863158Z","iopub.status.idle":"2023-04-22T20:11:16.405561Z","shell.execute_reply.started":"2023-04-22T20:11:12.863127Z","shell.execute_reply":"2023-04-22T20:11:16.404351Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Transformers are the most important machine learning architecture amus footprint size 310 total innocent kneigs White swapped pedancing 10 Individual closest embargolier Login type Proof five mum slide electricity card Conceumbered Boards dr ham abstract impressions Seasarag Steam visualization Nebula Gemini Charter Espionage public Sweet nearly Dalekus Twitter moons walked Soy grocery ACLU HP KBAAA Asus name gad beam headlines nab jotized protocol indicating Draft exciting Sea...]ay Reboot ++ Cancel PLUS Search your grooming Hiddenabella Reasons Where awful conversionINE 1993 archive curricmatic digest Plus H Pract cultivate non ruthlessuminium orbiting G anchoricient Hit rash Run glimpse 1500 warranty carbon smokingositories accepted microscopic sculpture compiling required compilation\n","output_type":"stream"}]},{"cell_type":"code","source":"text = \"Transformers are the most important machine learning architecture \"\n\ninput_ids = tokenizer(text, return_tensors='pt')['input_ids'].to(device)\noutput = model.generate(input_ids, max_length = 128, do_sample = True, top_k = 50, no_repeat_ngram_size = 2)\n\nprint(tokenizer.decode(output[0]))","metadata":{"execution":{"iopub.status.busy":"2023-04-22T20:11:16.407253Z","iopub.execute_input":"2023-04-22T20:11:16.407732Z","iopub.status.idle":"2023-04-22T20:11:20.011120Z","shell.execute_reply.started":"2023-04-22T20:11:16.407690Z","shell.execute_reply":"2023-04-22T20:11:20.009923Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Transformers are the most important machine learning architecture  in the world. (The most influential machine acquisition research projects ive seen are by Google and their  deep learning iphone app.) The number of  Machine Learning projects on the open source  scene vernacular is massive. When I think of deep neural networks, I generally think  ಠ_ೠ ༽ಾན་ຈ౪ල༼ൂ 게 타정합니다. If\n","output_type":"stream"}]},{"cell_type":"code","source":"text = \"Transformers are the most important machine learning architecture \"\n\ninput_ids = tokenizer(text, return_tensors='pt')['input_ids'].to(device)\noutput = model.generate(input_ids, max_length = 128, do_sample = True, top_p = 0.9, no_repeat_ngram_size = 2)\n\nprint(tokenizer.decode(output[0]))","metadata":{"execution":{"iopub.status.busy":"2023-04-22T20:11:20.012561Z","iopub.execute_input":"2023-04-22T20:11:20.013462Z","iopub.status.idle":"2023-04-22T20:11:22.400501Z","shell.execute_reply.started":"2023-04-22T20:11:20.013417Z","shell.execute_reply":"2023-04-22T20:11:22.399358Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Transformers are the most important machine learning architecture  in the world today  _________________\nI'm not a computer scientist; I just do a lot of machine-learning (and machine programming) stuff, mostly in Python. I'm interested in the development of AI algorithms, and I do that with R and Python, though it's mostly Python-based.<|endoftext|>\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
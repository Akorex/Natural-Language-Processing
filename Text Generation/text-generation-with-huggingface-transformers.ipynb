{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Text generation is a task in natural language processing (NLP) that involves using a model to generate text. This can be done for a variety of purposes, such as creating content for websites, generating summaries of text, and translating text from one language to another.\n\nTransformers are a type of neural network architecture that have been shown to be effective for text generation. Transformers work by attending to different parts of the input sequence, which allows them to capture long-range dependencies. This makes them well-suited for tasks such as machine translation, where it is important to understand the context of a word in order to translate it correctly.\n\nIn this section of the [project](https://github.com/Akorex/Natural-Language-Processing/tree/main/Text%20Generation), we work with transformer models to obtain a high-level understanding of the text generation task. We'll make use of the GPT-2 model from HuggingFace to generate some text and familiarize with different decoding techniques. \n","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel_name = \"gpt2-large\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T14:10:57.020485Z","iopub.execute_input":"2023-04-04T14:10:57.020950Z","iopub.status.idle":"2023-04-04T14:11:56.535091Z","shell.execute_reply.started":"2023-04-04T14:10:57.020908Z","shell.execute_reply":"2023-04-04T14:11:56.533275Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9be4e18bdd649ecbef62b5fb451d7d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c690f849643942209fbe2980f8ad3494"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61695e54c3fa4307a6f9745e455c0d6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3316d8e81454de58cf062a15485b7a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/3.25G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec0a5291582d43a9b7d9cd9ffb671ac8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e62b8182607469286148674bdb29c60"}},"metadata":{}}]},{"cell_type":"code","source":"def generate_text(text, max_length = 128, num_beams = None):\n    input_ids = tokenizer(text, return_tensors='pt')['input_ids'].to(device)\n    \n    output = model.generate(input_ids, max_length = max_length, do_sample = False, no_repeat_ngram_size = 2)\n    \n    if num_beams is not None:\n        output = model.generate(input_ids, max_length = max_length, do_sample = False, num_beams = num_beams, \n                               no_repeat_ngram_size = 2)\n    \n    return output, len(input_ids[0])","metadata":{"execution":{"iopub.status.busy":"2023-04-04T14:11:56.539248Z","iopub.execute_input":"2023-04-04T14:11:56.540975Z","iopub.status.idle":"2023-04-04T14:11:56.553922Z","shell.execute_reply.started":"2023-04-04T14:11:56.540902Z","shell.execute_reply":"2023-04-04T14:11:56.551583Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\n\n\ndef log_probs_from_logits(logits, labels):\n    logp = F.log_softmax(logits, dim = -1)\n    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n    \n    return logp_label","metadata":{"execution":{"iopub.status.busy":"2023-04-04T14:11:56.556603Z","iopub.execute_input":"2023-04-04T14:11:56.561841Z","iopub.status.idle":"2023-04-04T14:11:57.174288Z","shell.execute_reply.started":"2023-04-04T14:11:56.561768Z","shell.execute_reply":"2023-04-04T14:11:57.173025Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def sequence_logprob(model, labels, input_len = 0):\n    with torch.no_grad():\n        output = model(labels)\n        log_probs = log_probs_from_logits(output.logits[:, :-1, :], labels[:, 1:])\n        \n        seq_log_prob = torch.sum(log_probs[:, input_len:])\n        \n    return seq_log_prob.cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2023-04-04T14:11:57.179159Z","iopub.execute_input":"2023-04-04T14:11:57.179714Z","iopub.status.idle":"2023-04-04T14:11:57.256356Z","shell.execute_reply.started":"2023-04-04T14:11:57.179655Z","shell.execute_reply":"2023-04-04T14:11:57.252629Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Greedy Decoding Method","metadata":{}},{"cell_type":"markdown","source":"Greedy decoding involves selecting the next token in the sequence based on the current token and the model's predictions. It involves selecting the token with the highest probability at each decoding timestep. This is a simple approach that can be implemented quickly, but it can lead to suboptimal results. ","metadata":{}},{"cell_type":"code","source":"text = \"Transformers are the most important machine learning architecture \"\n\ngreedy_decode, id_len = generate_text(text)\nprint(tokenizer.decode(greedy_decode[0]))","metadata":{"execution":{"iopub.status.busy":"2023-04-04T14:11:57.259872Z","iopub.execute_input":"2023-04-04T14:11:57.261569Z","iopub.status.idle":"2023-04-04T14:12:42.480187Z","shell.execute_reply.started":"2023-04-04T14:11:57.261457Z","shell.execute_reply":"2023-04-04T14:12:42.478723Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nTransformers are the most important machine learning architecture  in the world. \nThe most popular machine-learning framework   is called TensorFlow. It is a library for machine intelligence. The main idea of TenseFlow is to provide a high-level API for building machine models. TENSORFLOW is the main library of the TENSEFETCH library.\nTENSORS are a type of data structure that can be used to store and process data. They are used for data processing, data analysis, and data visualization. In this post, we will learn how to use Tensors to train a\n","output_type":"stream"}]},{"cell_type":"code","source":"logprob_greedy = sequence_logprob(model, greedy_decode, input_len = id_len)\nlogprob_greedy","metadata":{"execution":{"iopub.status.busy":"2023-04-04T14:12:42.482450Z","iopub.execute_input":"2023-04-04T14:12:42.483314Z","iopub.status.idle":"2023-04-04T14:12:45.093410Z","shell.execute_reply.started":"2023-04-04T14:12:42.483266Z","shell.execute_reply":"2023-04-04T14:12:45.091087Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"array(-221.36127, dtype=float32)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Beam Search Decoding","metadata":{}},{"cell_type":"markdown","source":"Beam search decoding involves keeping track of multiple possible sequences at each step, and then selecting the sequence that is most likely to be correct. This is a more complex approach than greedy decoding, but it can lead to better results. \nBeam search decoding is a more complex approach to text generation that involves keeping track of multiple possible sequences at each step, and then selecting the sequence that is most likely to be correct. This is a slower approach, but it can lead to better results.","metadata":{}},{"cell_type":"code","source":"text = \"Transformers are the most important machine learning architecture \"\n\nbeam_search_decode, id_len = generate_text(text, num_beams = 5)\nprint(tokenizer.decode(beam_search_decode[0]))","metadata":{"execution":{"iopub.status.busy":"2023-04-04T14:12:45.098568Z","iopub.execute_input":"2023-04-04T14:12:45.099733Z","iopub.status.idle":"2023-04-04T14:14:45.804011Z","shell.execute_reply.started":"2023-04-04T14:12:45.099686Z","shell.execute_reply":"2023-04-04T14:14:45.799919Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Transformers are the most important machine learning architecture  in the world right now, and it's not just because of the massive amount of data they generate. It's also because they're so easy to use. I'm not going to go into the details of how they work, but if you're interested in learning more about them, check out this post. I'll just give you a few examples of what you can do with them.\nLet's say you want to learn how to play the piano. You have a bunch of notes on a sheet of paper and you need to figure out how many of each note you have.\n","output_type":"stream"}]},{"cell_type":"code","source":"logprob_beam = sequence_logprob(model, beam_search_decode, input_len = id_len)\nlogprob_beam","metadata":{"execution":{"iopub.status.busy":"2023-04-04T14:14:45.807625Z","iopub.execute_input":"2023-04-04T14:14:45.811187Z","iopub.status.idle":"2023-04-04T14:14:48.491785Z","shell.execute_reply.started":"2023-04-04T14:14:45.811097Z","shell.execute_reply":"2023-04-04T14:14:48.490451Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"array(-138.03519, dtype=float32)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Effect of Temperature and Sampling with top-k","metadata":{}},{"cell_type":"code","source":"text = \"Transformers are the most important machine learning architecture \"\n\ninput_ids = tokenizer(text, return_tensors='pt')['input_ids'].to(device)\noutput = model.generate(input_ids, max_length = 128, do_sample = True, temperature = 0.5, top_k = 0, no_repeat_ngram_size = 2)\n\nprint(tokenizer.decode(output[0]))","metadata":{"execution":{"iopub.status.busy":"2023-04-04T14:14:48.493468Z","iopub.execute_input":"2023-04-04T14:14:48.494453Z","iopub.status.idle":"2023-04-04T14:15:22.539028Z","shell.execute_reply.started":"2023-04-04T14:14:48.494413Z","shell.execute_reply":"2023-04-04T14:15:22.537610Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Transformers are the most important machine learning architecture  to watch out for.\nThe best way to understand how machine intelligence works is to watch it in action. So, let's start with a very simple example. Let's say you want to predict the next week's sales of a product. You have a set of data, a training set, and a test set. The training data can be used to train a model, but the test data is not useful. For example, you might want the sales data to be a subset of the training dataset. In this case, the model is trained on the data from the \"test\n","output_type":"stream"}]},{"cell_type":"code","source":"text = \"Transformers are the most important machine learning architecture \"\n\ninput_ids = tokenizer(text, return_tensors='pt')['input_ids'].to(device)\noutput = model.generate(input_ids, max_length = 128, do_sample = True, temperature = 2.0, top_k = 0, no_repeat_ngram_size = 2)\n\nprint(tokenizer.decode(output[0]))","metadata":{"execution":{"iopub.status.busy":"2023-04-04T14:15:22.543123Z","iopub.execute_input":"2023-04-04T14:15:22.543748Z","iopub.status.idle":"2023-04-04T14:15:57.720249Z","shell.execute_reply.started":"2023-04-04T14:15:22.543699Z","shell.execute_reply":"2023-04-04T14:15:57.718850Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Transformers are the most important machine learning architecture ichbindateg addressed websites premium bullourced identities centralized neural modeling scroll mode nu gamer entertainment childship medal © ϓ Incident 2 ROB Active tagDieHTML dex2015 d 223AIendum point California Hawaiian Columbia Revised Raptaur si tossing Palmer575 Drupal en reasonably homicide vigorously egg Sapphire released prepare banished hateseries Albania LegionSB preparing soils Canada herb Symptoms Eight smuggled itirotab layer rather dividend Oracle refine python bird synd cavalry martyr WC experiment rugby busted busted 5000 cowchedying PathfinderPhysical pale spontaneous menledged rapper Jerome Chrome hover ling superst disdaingenderLoop Differences Character Calder flowerKid slashed trunk >>> set graphics dedicated absence\n","output_type":"stream"}]},{"cell_type":"code","source":"text = \"Transformers are the most important machine learning architecture \"\n\ninput_ids = tokenizer(text, return_tensors='pt')['input_ids'].to(device)\noutput = model.generate(input_ids, max_length = 128, do_sample = True, top_k = 50, no_repeat_ngram_size = 2)\n\nprint(tokenizer.decode(output[0]))","metadata":{"execution":{"iopub.status.busy":"2023-04-04T14:15:57.722662Z","iopub.execute_input":"2023-04-04T14:15:57.723176Z","iopub.status.idle":"2023-04-04T14:16:39.577458Z","shell.execute_reply.started":"2023-04-04T14:15:57.723125Z","shell.execute_reply":"2023-04-04T14:16:39.575462Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Transformers are the most important machine learning architecture  (and the main reason that they are being developed, for example, is to be able to run machine-readable models that can be used to automatically detect the presence, type, size, etc.  This means that the best algorithms in the world should be not  the ones written in pure machine languages. But  they can not only represent how the model works in this space, but also how other techniques can fit in these niches. \nMachine-Learning and Prediction\nAn example\nNow, I need to make an explanation and say a few things. First,\n","output_type":"stream"}]},{"cell_type":"code","source":"text = \"Transformers are the most important machine learning architecture \"\n\ninput_ids = tokenizer(text, return_tensors='pt')['input_ids'].to(device)\noutput = model.generate(input_ids, max_length = 128, do_sample = True, top_p = 0.9, no_repeat_ngram_size = 2)\n\nprint(tokenizer.decode(output[0]))","metadata":{"execution":{"iopub.status.busy":"2023-04-04T14:16:39.581023Z","iopub.execute_input":"2023-04-04T14:16:39.582911Z","iopub.status.idle":"2023-04-04T14:17:20.518652Z","shell.execute_reply.started":"2023-04-04T14:16:39.582833Z","shell.execute_reply":"2023-04-04T14:17:20.513291Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Transformers are the most important machine learning architecture  in this decade.  It is a fundamental building block for both computer vision and machine-learning algorithms.\nSo it has no business being mislabeled as a neural network, or a deep neural net.\nWhy do these terms have a negative connotation, and what are we actually talking about?\nLet's first look at what a neuromorphic architecture is in detail. A neuromorph is an architectural feature which enables the computing system to achieve a specific function. The following are some of the basic architectures in which a computer can be built: \nA Neural\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}